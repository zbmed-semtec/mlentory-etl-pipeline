<!DOCTYPE html>
<!-- saved from url=(0072)https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata -->
<html class=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
		<meta name="description" content="We’re on a journey to advance and democratize artificial intelligence through open source and open science.">
		<meta property="fb:app_id" content="1321688464574422">
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@huggingface">
		<meta property="og:title" content="librarian-bots/model_cards_with_metadata · Datasets at Hugging Face">
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata">
		<meta property="og:image" content="https://cdn-thumbnails.huggingface.co/social-thumbnails/datasets/librarian-bots/model_cards_with_metadata.png">

		<link rel="stylesheet" href="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/style.css">

		<link rel="preconnect" href="https://fonts.gstatic.com/">
		<link href="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/css2" rel="stylesheet">
		<link href="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/css2(1)" rel="stylesheet">

		<link rel="stylesheet" href="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/katex.min.css" as="style" onload="this.onload=null;this.rel=&#39;stylesheet&#39;">
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<link rel="canonical" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata"> <script type="application/ld+json">{
  "@context": {
    "@language": "en",
    "@vocab": "https://schema.org/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http://mlcommons.org/croissant/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataBiases": "cr:dataBiases",
    "dataCollection": "cr:dataCollection",
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http://purl.org/dc/terms/",
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "personalSensitiveInformation": "cr:personalSensitiveInformation",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https://schema.org/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "distribution": [
    {
      "@type": "cr:FileObject",
      "@id": "repo",
      "name": "repo",
      "description": "The Hugging Face git repository.",
      "contentUrl": "https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/tree/refs%2Fconvert%2Fparquet",
      "encodingFormat": "git+https",
      "sha256": "https://github.com/mlcommons/croissant/issues/80"
    },
    {
      "@type": "cr:FileSet",
      "@id": "parquet-files-for-config-default",
      "name": "parquet-files-for-config-default",
      "description": "The underlying Parquet files as converted by Hugging Face (see: https://huggingface.co/docs/datasets-server/parquet).",
      "containedIn": {
        "@id": "repo"
      },
      "encodingFormat": "application/x-parquet",
      "includes": "default/*/*.parquet"
    }
  ],
  "recordSet": [
    {
      "@type": "cr:RecordSet",
      "@id": "default",
      "name": "default",
      "description": "librarian-bots/model_cards_with_metadata - 'default' subset\n\nAdditional information:\n- 3 skipped columns: last_modified, tags, createdAt",
      "field": [
        {
          "@type": "cr:Field",
          "@id": "default/modelId",
          "name": "default/modelId",
          "description": "Column 'modelId' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "modelId"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default/author",
          "name": "default/author",
          "description": "Column 'author' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "author"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default/downloads",
          "name": "default/downloads",
          "description": "Column 'downloads' from the Hugging Face parquet file.",
          "dataType": "sc:Integer",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "downloads"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default/likes",
          "name": "default/likes",
          "description": "Column 'likes' from the Hugging Face parquet file.",
          "dataType": "sc:Integer",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "likes"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default/library_name",
          "name": "default/library_name",
          "description": "Column 'library_name' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "library_name"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default/pipeline_tag",
          "name": "default/pipeline_tag",
          "description": "Column 'pipeline_tag' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "pipeline_tag"
            }
          }
        },
        {
          "@type": "cr:Field",
          "@id": "default/card",
          "name": "default/card",
          "description": "Column 'card' from the Hugging Face parquet file.",
          "dataType": "sc:Text",
          "source": {
            "fileSet": {
              "@id": "parquet-files-for-config-default"
            },
            "extract": {
              "column": "card"
            }
          }
        }
      ]
    }
  ],
  "conformsTo": "http://mlcommons.org/croissant/1.0",
  "name": "model_cards_with_metadata",
  "description": "\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Hugging Face Hub Model Cards\n\t\n\nThis datasets consists of model cards for models hosted on the Hugging Face Hub. The model cards are created by the community and provide information about the model, its performance, its intended uses, and more. \nThis dataset is updated on a daily basis and includes publicly available models on the Hugging Face Hub.\nThis dataset is made available to help support users wanting to work with a large number of Model Cards from the Hub.… See the full description on the dataset page: https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata.",
  "alternateName": [
    "librarian-bots/model_cards_with_metadata",
    "Hugging Face Hub Model Cards"
  ],
  "creator": {
    "@type": "Organization",
    "name": "Librarian Bots",
    "url": "https://huggingface.co/librarian-bots"
  },
  "keywords": [
    "text-retrieval",
    "100K&lt;n&lt;1M",
    "ethics",
    "Croissant",
    "🇺🇸 Region: US"
  ],
  "url": "https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata"
}</script> 

		<title>librarian-bots/model_cards_with_metadata · Datasets at Hugging Face</title>

		<script defer="" data-domain="huggingface.co" event-loggedin="true" src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/script.pageview-props.js.descarga"></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script type="text/javascript" src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/challenge.js.descarga" defer=""></script>
	<script src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/saved_resource" async=""></script><script src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/js" async=""></script><style data-styled="active" data-styled-version="5.0.1"></style><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/DatasetHeader-7a086f23.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/RepoHeaderTag-88203f34.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconDownload-7f3e71ae.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/Tag-52e0ad45.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/index-7a2c5289.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/index-2e076a9d.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconGguf-fc429495.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconJs-04bac839.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconLicense-600f61a6.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconNotForAllEyes-78c6630b.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/Tags-9b7b88ca.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconQuote-f8bdb2cf.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/Tooltip-205f806d.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/index-320b7deb.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconCogFlat-30976135.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconCommunity-6b5dbaf4.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconList-47116277.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/ResourceGroupTag-dafe9944.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconHeart-92558aec.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconHeartFilled-17d41629.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/UsersListModal-402a51b9.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/ModalBody-bef2be68.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/urlWatcher-f8616968.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/index-79e4fb58.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/FollowButton-931b70e7.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconBellWatching-7369df29.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconCheckmarkFilled-fb4a5e74.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/PrivateTag-cd2b0cb3.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/CopyButton-db38f987.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconDuplicate-3384457e.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/RepoTabs-afb82bfa.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconDataTable-3001f63a.js"><style>@font-face{font-family:"ProximaVara-Roman";src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/proxima-vara-roman-black.f99ab2034eea98d11ef4.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/proxima-vara-roman-black.fe9b55be9cb49a68107a.woff) format("woff");font-weight:100 1000;font-display:swap;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.scispace-plugin-maximized{width:calc(100% - 420px) !important}@media print{#scispace-extension-root{visibility:hidden}.scispace-plugin-maximized{width:initial !important}}</style><style>@font-face{font-family:KaTeX_AMS;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_AMS-Regular.73ea273a72f4aca30ca5.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_AMS-Regular.d562e886c52f12660a41.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_AMS-Regular.853be92419a6c3766b9a.ttf) format("truetype")}@font-face{font-family:KaTeX_Caligraphic;font-style:normal;font-weight:700;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Caligraphic-Bold.a1abf90dfd72792a577a.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Caligraphic-Bold.d757c535a2e5902f1325.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Caligraphic-Bold.7489a2fbfb9bfe704420.ttf) format("truetype")}@font-face{font-family:KaTeX_Caligraphic;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Caligraphic-Regular.d6484fce1ef428d5bd94.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Caligraphic-Regular.db074fa22cf224af93d7.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Caligraphic-Regular.7e873d3833eb108a0758.ttf) format("truetype")}@font-face{font-family:KaTeX_Fraktur;font-style:normal;font-weight:700;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Fraktur-Bold.931d67ea207ab37ee693.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Fraktur-Bold.354501bac435c3264834.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Fraktur-Bold.4c761b3711973ab04edf.ttf) format("truetype")}@font-face{font-family:KaTeX_Fraktur;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Fraktur-Regular.172d3529b26f8cedef6b.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Fraktur-Regular.6fdf0ac577be0ba82a4c.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Fraktur-Regular.ed305b5434865e06ffde.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:700;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Bold.39890742bc957b368704.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Bold.0c3b8929d377c0e9b2f3.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Bold.8169508bf58f8bd92ad8.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:700;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-BoldItalic.20f389c4120be058d80a.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-BoldItalic.428978dc7837d46de091.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-BoldItalic.828abcb200061cffbaae.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:italic;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Italic.fe2176f79edaa716e621.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Italic.fd947498bc16392e76c2.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Italic.fa675e5e4bec9eb250b6.ttf) format("truetype")}@font-face{font-family:KaTeX_Main;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Regular.f650f111a3b890d116f1.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Regular.4f35fbcc9ee8614c2bcc.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Main-Regular.9eba1d77abcf2aa6e94e.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:700;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Math-BoldItalic.dcbcbd93bac0470b462d.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Math-BoldItalic.3f07ed67f06c720120ce.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Math-BoldItalic.bf2d440b3a42ea78a998.ttf) format("truetype")}@font-face{font-family:KaTeX_Math;font-style:italic;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Math-Italic.6d3d25f4820d0da8f01f.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Math-Italic.96759856b4e70f3a8338.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Math-Italic.8a5f936332e8028c7278.ttf) format("truetype")}@font-face{font-family:"KaTeX_SansSerif";font-style:normal;font-weight:700;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Bold.95591a929f0d32aa282a.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Bold.b9cd458ac6d5889ff9c3.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Bold.5b49f4993ae22d7975b4.ttf) format("truetype")}@font-face{font-family:"KaTeX_SansSerif";font-style:italic;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Italic.7d393d382f3e7fb1c637.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Italic.8d593cfaa96238d5e2f8.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Italic.b257a18c016f37ee4543.ttf) format("truetype")}@font-face{font-family:"KaTeX_SansSerif";font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Regular.cd5e231e0cc53b2cb2c0.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Regular.02271ec5cb9f5b4588ac.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_SansSerif-Regular.2f7bc363fc5424ebda59.ttf) format("truetype")}@font-face{font-family:KaTeX_Script;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Script-Regular.c81d1b2a4b75d3eded60.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Script-Regular.073b3402d036714b4370.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Script-Regular.fc9ba5249878cd8f8d88.ttf) format("truetype")}@font-face{font-family:KaTeX_Size1;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size1-Regular.6eec866c69313624be60.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size1-Regular.0108e89c9003e8c14ea3.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size1-Regular.6de7d4b539221a49e9e2.ttf) format("truetype")}@font-face{font-family:KaTeX_Size2;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size2-Regular.2960900c4f271311eb36.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size2-Regular.3a99e70aee4076660d38.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size2-Regular.57f5c1837853986ea1db.ttf) format("truetype")}@font-face{font-family:KaTeX_Size3;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size3-Regular.e1951519f6f0596f7356.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size3-Regular.7947224e8a9914fa332b.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size3-Regular.8d6b6822586eea3d3b20.ttf) format("truetype")}@font-face{font-family:KaTeX_Size4;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size4-Regular.e418bf257af1052628d8.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size4-Regular.aeffd8025cba3647f1a6.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Size4-Regular.4ad7c7e8bb8d10a34bb7.ttf) format("truetype")}@font-face{font-family:KaTeX_Typewriter;font-style:normal;font-weight:400;src:url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Typewriter-Regular.c295e7f71970f03c0549.woff2) format("woff2"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Typewriter-Regular.4c6b94fd1d07f8beff7c.woff) format("woff"),url(chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/fonts/KaTeX_Typewriter-Regular.c5c02d763c89380dcb4e.ttf) format("truetype")}.katex{text-rendering:auto;font:normal 1.21em KaTeX_Main,Times New Roman,serif;line-height:1.2;text-indent:0}.katex *{-ms-high-contrast-adjust:none!important;border-color:currentColor}.katex .katex-version:after{content:"0.16.7"}.katex .katex-mathml{clip:rect(1px,1px,1px,1px);border:0;height:1px;overflow:hidden;padding:0;position:absolute;width:1px}.katex .katex-html>.newline{display:block}.katex .base{position:relative;white-space:nowrap;width:-webkit-min-content;width:-moz-min-content;width:min-content}.katex .base,.katex .strut{display:inline-block}.katex .textbf{font-weight:700}.katex .textit{font-style:italic}.katex .textrm{font-family:KaTeX_Main}.katex .textsf{font-family:KaTeX_SansSerif}.katex .texttt{font-family:KaTeX_Typewriter}.katex .mathnormal{font-family:KaTeX_Math;font-style:italic}.katex .mathit{font-family:KaTeX_Main;font-style:italic}.katex .mathrm{font-style:normal}.katex .mathbf{font-family:KaTeX_Main;font-weight:700}.katex .boldsymbol{font-family:KaTeX_Math;font-style:italic;font-weight:700}.katex .amsrm,.katex .mathbb,.katex .textbb{font-family:KaTeX_AMS}.katex .mathcal{font-family:KaTeX_Caligraphic}.katex .mathfrak,.katex .textfrak{font-family:KaTeX_Fraktur}.katex .mathtt{font-family:KaTeX_Typewriter}.katex .mathscr,.katex .textscr{font-family:KaTeX_Script}.katex .mathsf,.katex .textsf{font-family:KaTeX_SansSerif}.katex .mathboldsf,.katex .textboldsf{font-family:KaTeX_SansSerif;font-weight:700}.katex .mathitsf,.katex .textitsf{font-family:KaTeX_SansSerif;font-style:italic}.katex .mainrm{font-family:KaTeX_Main;font-style:normal}.katex .vlist-t{border-collapse:collapse;display:inline-table;table-layout:fixed}.katex .vlist-r{display:table-row}.katex .vlist{display:table-cell;position:relative;vertical-align:bottom}.katex .vlist>span{display:block;height:0;position:relative}.katex .vlist>span>span{display:inline-block}.katex .vlist>span>.pstrut{overflow:hidden;width:0}.katex .vlist-t2{margin-right:-2px}.katex .vlist-s{display:table-cell;font-size:1px;min-width:2px;vertical-align:bottom;width:2px}.katex .vbox{align-items:baseline;display:inline-flex;flex-direction:column}.katex .hbox{width:100%}.katex .hbox,.katex .thinbox{display:inline-flex;flex-direction:row}.katex .thinbox{max-width:0;width:0}.katex .msupsub{text-align:left}.katex .mfrac>span>span{text-align:center}.katex .mfrac .frac-line{border-bottom-style:solid;display:inline-block;width:100%}.katex .hdashline,.katex .hline,.katex .mfrac .frac-line,.katex .overline .overline-line,.katex .rule,.katex .underline .underline-line{min-height:1px}.katex .mspace{display:inline-block}.katex .clap,.katex .llap,.katex .rlap{position:relative;width:0}.katex .clap>.inner,.katex .llap>.inner,.katex .rlap>.inner{position:absolute}.katex .clap>.fix,.katex .llap>.fix,.katex .rlap>.fix{display:inline-block}.katex .llap>.inner{right:0}.katex .clap>.inner,.katex .rlap>.inner{left:0}.katex .clap>.inner>span{margin-left:-50%;margin-right:50%}.katex .rule{border:0 solid;display:inline-block;position:relative}.katex .hline,.katex .overline .overline-line,.katex .underline .underline-line{border-bottom-style:solid;display:inline-block;width:100%}.katex .hdashline{border-bottom-style:dashed;display:inline-block;width:100%}.katex .sqrt>.root{margin-left:.27777778em;margin-right:-.55555556em}.katex .fontsize-ensurer.reset-size1.size1,.katex .sizing.reset-size1.size1{font-size:1em}.katex .fontsize-ensurer.reset-size1.size2,.katex .sizing.reset-size1.size2{font-size:1.2em}.katex .fontsize-ensurer.reset-size1.size3,.katex .sizing.reset-size1.size3{font-size:1.4em}.katex .fontsize-ensurer.reset-size1.size4,.katex .sizing.reset-size1.size4{font-size:1.6em}.katex .fontsize-ensurer.reset-size1.size5,.katex .sizing.reset-size1.size5{font-size:1.8em}.katex .fontsize-ensurer.reset-size1.size6,.katex .sizing.reset-size1.size6{font-size:2em}.katex .fontsize-ensurer.reset-size1.size7,.katex .sizing.reset-size1.size7{font-size:2.4em}.katex .fontsize-ensurer.reset-size1.size8,.katex .sizing.reset-size1.size8{font-size:2.88em}.katex .fontsize-ensurer.reset-size1.size9,.katex .sizing.reset-size1.size9{font-size:3.456em}.katex .fontsize-ensurer.reset-size1.size10,.katex .sizing.reset-size1.size10{font-size:4.148em}.katex .fontsize-ensurer.reset-size1.size11,.katex .sizing.reset-size1.size11{font-size:4.976em}.katex .fontsize-ensurer.reset-size2.size1,.katex .sizing.reset-size2.size1{font-size:.83333333em}.katex .fontsize-ensurer.reset-size2.size2,.katex .sizing.reset-size2.size2{font-size:1em}.katex .fontsize-ensurer.reset-size2.size3,.katex .sizing.reset-size2.size3{font-size:1.16666667em}.katex .fontsize-ensurer.reset-size2.size4,.katex .sizing.reset-size2.size4{font-size:1.33333333em}.katex .fontsize-ensurer.reset-size2.size5,.katex .sizing.reset-size2.size5{font-size:1.5em}.katex .fontsize-ensurer.reset-size2.size6,.katex .sizing.reset-size2.size6{font-size:1.66666667em}.katex .fontsize-ensurer.reset-size2.size7,.katex .sizing.reset-size2.size7{font-size:2em}.katex .fontsize-ensurer.reset-size2.size8,.katex .sizing.reset-size2.size8{font-size:2.4em}.katex .fontsize-ensurer.reset-size2.size9,.katex .sizing.reset-size2.size9{font-size:2.88em}.katex .fontsize-ensurer.reset-size2.size10,.katex .sizing.reset-size2.size10{font-size:3.45666667em}.katex .fontsize-ensurer.reset-size2.size11,.katex .sizing.reset-size2.size11{font-size:4.14666667em}.katex .fontsize-ensurer.reset-size3.size1,.katex .sizing.reset-size3.size1{font-size:.71428571em}.katex .fontsize-ensurer.reset-size3.size2,.katex .sizing.reset-size3.size2{font-size:.85714286em}.katex .fontsize-ensurer.reset-size3.size3,.katex .sizing.reset-size3.size3{font-size:1em}.katex .fontsize-ensurer.reset-size3.size4,.katex .sizing.reset-size3.size4{font-size:1.14285714em}.katex .fontsize-ensurer.reset-size3.size5,.katex .sizing.reset-size3.size5{font-size:1.28571429em}.katex .fontsize-ensurer.reset-size3.size6,.katex .sizing.reset-size3.size6{font-size:1.42857143em}.katex .fontsize-ensurer.reset-size3.size7,.katex .sizing.reset-size3.size7{font-size:1.71428571em}.katex .fontsize-ensurer.reset-size3.size8,.katex .sizing.reset-size3.size8{font-size:2.05714286em}.katex .fontsize-ensurer.reset-size3.size9,.katex .sizing.reset-size3.size9{font-size:2.46857143em}.katex .fontsize-ensurer.reset-size3.size10,.katex .sizing.reset-size3.size10{font-size:2.96285714em}.katex .fontsize-ensurer.reset-size3.size11,.katex .sizing.reset-size3.size11{font-size:3.55428571em}.katex .fontsize-ensurer.reset-size4.size1,.katex .sizing.reset-size4.size1{font-size:.625em}.katex .fontsize-ensurer.reset-size4.size2,.katex .sizing.reset-size4.size2{font-size:.75em}.katex .fontsize-ensurer.reset-size4.size3,.katex .sizing.reset-size4.size3{font-size:.875em}.katex .fontsize-ensurer.reset-size4.size4,.katex .sizing.reset-size4.size4{font-size:1em}.katex .fontsize-ensurer.reset-size4.size5,.katex .sizing.reset-size4.size5{font-size:1.125em}.katex .fontsize-ensurer.reset-size4.size6,.katex .sizing.reset-size4.size6{font-size:1.25em}.katex .fontsize-ensurer.reset-size4.size7,.katex .sizing.reset-size4.size7{font-size:1.5em}.katex .fontsize-ensurer.reset-size4.size8,.katex .sizing.reset-size4.size8{font-size:1.8em}.katex .fontsize-ensurer.reset-size4.size9,.katex .sizing.reset-size4.size9{font-size:2.16em}.katex .fontsize-ensurer.reset-size4.size10,.katex .sizing.reset-size4.size10{font-size:2.5925em}.katex .fontsize-ensurer.reset-size4.size11,.katex .sizing.reset-size4.size11{font-size:3.11em}.katex .fontsize-ensurer.reset-size5.size1,.katex .sizing.reset-size5.size1{font-size:.55555556em}.katex .fontsize-ensurer.reset-size5.size2,.katex .sizing.reset-size5.size2{font-size:.66666667em}.katex .fontsize-ensurer.reset-size5.size3,.katex .sizing.reset-size5.size3{font-size:.77777778em}.katex .fontsize-ensurer.reset-size5.size4,.katex .sizing.reset-size5.size4{font-size:.88888889em}.katex .fontsize-ensurer.reset-size5.size5,.katex .sizing.reset-size5.size5{font-size:1em}.katex .fontsize-ensurer.reset-size5.size6,.katex .sizing.reset-size5.size6{font-size:1.11111111em}.katex .fontsize-ensurer.reset-size5.size7,.katex .sizing.reset-size5.size7{font-size:1.33333333em}.katex .fontsize-ensurer.reset-size5.size8,.katex .sizing.reset-size5.size8{font-size:1.6em}.katex .fontsize-ensurer.reset-size5.size9,.katex .sizing.reset-size5.size9{font-size:1.92em}.katex .fontsize-ensurer.reset-size5.size10,.katex .sizing.reset-size5.size10{font-size:2.30444444em}.katex .fontsize-ensurer.reset-size5.size11,.katex .sizing.reset-size5.size11{font-size:2.76444444em}.katex .fontsize-ensurer.reset-size6.size1,.katex .sizing.reset-size6.size1{font-size:.5em}.katex .fontsize-ensurer.reset-size6.size2,.katex .sizing.reset-size6.size2{font-size:.6em}.katex .fontsize-ensurer.reset-size6.size3,.katex .sizing.reset-size6.size3{font-size:.7em}.katex .fontsize-ensurer.reset-size6.size4,.katex .sizing.reset-size6.size4{font-size:.8em}.katex .fontsize-ensurer.reset-size6.size5,.katex .sizing.reset-size6.size5{font-size:.9em}.katex .fontsize-ensurer.reset-size6.size6,.katex .sizing.reset-size6.size6{font-size:1em}.katex .fontsize-ensurer.reset-size6.size7,.katex .sizing.reset-size6.size7{font-size:1.2em}.katex .fontsize-ensurer.reset-size6.size8,.katex .sizing.reset-size6.size8{font-size:1.44em}.katex .fontsize-ensurer.reset-size6.size9,.katex .sizing.reset-size6.size9{font-size:1.728em}.katex .fontsize-ensurer.reset-size6.size10,.katex .sizing.reset-size6.size10{font-size:2.074em}.katex .fontsize-ensurer.reset-size6.size11,.katex .sizing.reset-size6.size11{font-size:2.488em}.katex .fontsize-ensurer.reset-size7.size1,.katex .sizing.reset-size7.size1{font-size:.41666667em}.katex .fontsize-ensurer.reset-size7.size2,.katex .sizing.reset-size7.size2{font-size:.5em}.katex .fontsize-ensurer.reset-size7.size3,.katex .sizing.reset-size7.size3{font-size:.58333333em}.katex .fontsize-ensurer.reset-size7.size4,.katex .sizing.reset-size7.size4{font-size:.66666667em}.katex .fontsize-ensurer.reset-size7.size5,.katex .sizing.reset-size7.size5{font-size:.75em}.katex .fontsize-ensurer.reset-size7.size6,.katex .sizing.reset-size7.size6{font-size:.83333333em}.katex .fontsize-ensurer.reset-size7.size7,.katex .sizing.reset-size7.size7{font-size:1em}.katex .fontsize-ensurer.reset-size7.size8,.katex .sizing.reset-size7.size8{font-size:1.2em}.katex .fontsize-ensurer.reset-size7.size9,.katex .sizing.reset-size7.size9{font-size:1.44em}.katex .fontsize-ensurer.reset-size7.size10,.katex .sizing.reset-size7.size10{font-size:1.72833333em}.katex .fontsize-ensurer.reset-size7.size11,.katex .sizing.reset-size7.size11{font-size:2.07333333em}.katex .fontsize-ensurer.reset-size8.size1,.katex .sizing.reset-size8.size1{font-size:.34722222em}.katex .fontsize-ensurer.reset-size8.size2,.katex .sizing.reset-size8.size2{font-size:.41666667em}.katex .fontsize-ensurer.reset-size8.size3,.katex .sizing.reset-size8.size3{font-size:.48611111em}.katex .fontsize-ensurer.reset-size8.size4,.katex .sizing.reset-size8.size4{font-size:.55555556em}.katex .fontsize-ensurer.reset-size8.size5,.katex .sizing.reset-size8.size5{font-size:.625em}.katex .fontsize-ensurer.reset-size8.size6,.katex .sizing.reset-size8.size6{font-size:.69444444em}.katex .fontsize-ensurer.reset-size8.size7,.katex .sizing.reset-size8.size7{font-size:.83333333em}.katex .fontsize-ensurer.reset-size8.size8,.katex .sizing.reset-size8.size8{font-size:1em}.katex .fontsize-ensurer.reset-size8.size9,.katex .sizing.reset-size8.size9{font-size:1.2em}.katex .fontsize-ensurer.reset-size8.size10,.katex .sizing.reset-size8.size10{font-size:1.44027778em}.katex .fontsize-ensurer.reset-size8.size11,.katex .sizing.reset-size8.size11{font-size:1.72777778em}.katex .fontsize-ensurer.reset-size9.size1,.katex .sizing.reset-size9.size1{font-size:.28935185em}.katex .fontsize-ensurer.reset-size9.size2,.katex .sizing.reset-size9.size2{font-size:.34722222em}.katex .fontsize-ensurer.reset-size9.size3,.katex .sizing.reset-size9.size3{font-size:.40509259em}.katex .fontsize-ensurer.reset-size9.size4,.katex .sizing.reset-size9.size4{font-size:.46296296em}.katex .fontsize-ensurer.reset-size9.size5,.katex .sizing.reset-size9.size5{font-size:.52083333em}.katex .fontsize-ensurer.reset-size9.size6,.katex .sizing.reset-size9.size6{font-size:.5787037em}.katex .fontsize-ensurer.reset-size9.size7,.katex .sizing.reset-size9.size7{font-size:.69444444em}.katex .fontsize-ensurer.reset-size9.size8,.katex .sizing.reset-size9.size8{font-size:.83333333em}.katex .fontsize-ensurer.reset-size9.size9,.katex .sizing.reset-size9.size9{font-size:1em}.katex .fontsize-ensurer.reset-size9.size10,.katex .sizing.reset-size9.size10{font-size:1.20023148em}.katex .fontsize-ensurer.reset-size9.size11,.katex .sizing.reset-size9.size11{font-size:1.43981481em}.katex .fontsize-ensurer.reset-size10.size1,.katex .sizing.reset-size10.size1{font-size:.24108004em}.katex .fontsize-ensurer.reset-size10.size2,.katex .sizing.reset-size10.size2{font-size:.28929605em}.katex .fontsize-ensurer.reset-size10.size3,.katex .sizing.reset-size10.size3{font-size:.33751205em}.katex .fontsize-ensurer.reset-size10.size4,.katex .sizing.reset-size10.size4{font-size:.38572806em}.katex .fontsize-ensurer.reset-size10.size5,.katex .sizing.reset-size10.size5{font-size:.43394407em}.katex .fontsize-ensurer.reset-size10.size6,.katex .sizing.reset-size10.size6{font-size:.48216008em}.katex .fontsize-ensurer.reset-size10.size7,.katex .sizing.reset-size10.size7{font-size:.57859209em}.katex .fontsize-ensurer.reset-size10.size8,.katex .sizing.reset-size10.size8{font-size:.69431051em}.katex .fontsize-ensurer.reset-size10.size9,.katex .sizing.reset-size10.size9{font-size:.83317261em}.katex .fontsize-ensurer.reset-size10.size10,.katex .sizing.reset-size10.size10{font-size:1em}.katex .fontsize-ensurer.reset-size10.size11,.katex .sizing.reset-size10.size11{font-size:1.19961427em}.katex .fontsize-ensurer.reset-size11.size1,.katex .sizing.reset-size11.size1{font-size:.20096463em}.katex .fontsize-ensurer.reset-size11.size2,.katex .sizing.reset-size11.size2{font-size:.24115756em}.katex .fontsize-ensurer.reset-size11.size3,.katex .sizing.reset-size11.size3{font-size:.28135048em}.katex .fontsize-ensurer.reset-size11.size4,.katex .sizing.reset-size11.size4{font-size:.32154341em}.katex .fontsize-ensurer.reset-size11.size5,.katex .sizing.reset-size11.size5{font-size:.36173633em}.katex .fontsize-ensurer.reset-size11.size6,.katex .sizing.reset-size11.size6{font-size:.40192926em}.katex .fontsize-ensurer.reset-size11.size7,.katex .sizing.reset-size11.size7{font-size:.48231511em}.katex .fontsize-ensurer.reset-size11.size8,.katex .sizing.reset-size11.size8{font-size:.57877814em}.katex .fontsize-ensurer.reset-size11.size9,.katex .sizing.reset-size11.size9{font-size:.69453376em}.katex .fontsize-ensurer.reset-size11.size10,.katex .sizing.reset-size11.size10{font-size:.83360129em}.katex .fontsize-ensurer.reset-size11.size11,.katex .sizing.reset-size11.size11{font-size:1em}.katex .delimsizing.size1{font-family:KaTeX_Size1}.katex .delimsizing.size2{font-family:KaTeX_Size2}.katex .delimsizing.size3{font-family:KaTeX_Size3}.katex .delimsizing.size4{font-family:KaTeX_Size4}.katex .delimsizing.mult .delim-size1>span{font-family:KaTeX_Size1}.katex .delimsizing.mult .delim-size4>span{font-family:KaTeX_Size4}.katex .nulldelimiter{display:inline-block;width:.12em}.katex .delimcenter,.katex .op-symbol{position:relative}.katex .op-symbol.small-op{font-family:KaTeX_Size1}.katex .op-symbol.large-op{font-family:KaTeX_Size2}.katex .accent>.vlist-t,.katex .op-limits>.vlist-t{text-align:center}.katex .accent .accent-body{position:relative}.katex .accent .accent-body:not(.accent-full){width:0}.katex .overlay{display:block}.katex .mtable .vertical-separator{display:inline-block;min-width:1px}.katex .mtable .arraycolsep{display:inline-block}.katex .mtable .col-align-c>.vlist-t{text-align:center}.katex .mtable .col-align-l>.vlist-t{text-align:left}.katex .mtable .col-align-r>.vlist-t{text-align:right}.katex .svg-align{text-align:left}.katex svg{fill:currentColor;stroke:currentColor;fill-rule:nonzero;fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;display:block;height:inherit;position:absolute;width:100%}.katex svg path{stroke:none}.katex img{border-style:none;max-height:none;max-width:none;min-height:0;min-width:0}.katex .stretchy{display:block;overflow:hidden;position:relative;width:100%}.katex .stretchy:after,.katex .stretchy:before{content:""}.katex .hide-tail{overflow:hidden;position:relative;width:100%}.katex .halfarrow-left{left:0;overflow:hidden;position:absolute;width:50.2%}.katex .halfarrow-right{overflow:hidden;position:absolute;right:0;width:50.2%}.katex .brace-left{left:0;overflow:hidden;position:absolute;width:25.1%}.katex .brace-center{left:25%;overflow:hidden;position:absolute;width:50%}.katex .brace-right{overflow:hidden;position:absolute;right:0;width:25.1%}.katex .x-arrow-pad{padding:0 .5em}.katex .cd-arrow-pad{padding:0 .55556em 0 .27778em}.katex .mover,.katex .munder,.katex .x-arrow{text-align:center}.katex .boxpad{padding:0 .3em}.katex .fbox,.katex .fcolorbox{border:.04em solid;box-sizing:border-box}.katex .cancel-pad{padding:0 .2em}.katex .cancel-lap{margin-left:-.2em;margin-right:-.2em}.katex .sout{border-bottom-style:solid;border-bottom-width:.08em}.katex .angl{border-right:.049em solid;border-top:.049em solid;box-sizing:border-box;margin-right:.03889em}.katex .anglpad{padding:0 .03889em}.katex .eqn-num:before{content:"(" counter(katexEqnNo) ")";counter-increment:katexEqnNo}.katex .mml-eqn-num:before{content:"(" counter(mmlEqnNo) ")";counter-increment:mmlEqnNo}.katex .mtr-glue{width:50%}.katex .cd-vert-arrow{display:inline-block;position:relative}.katex .cd-label-left{display:inline-block;position:absolute;right:calc(50% + .3em);text-align:left}.katex .cd-label-right{display:inline-block;left:calc(50% + .3em);position:absolute;text-align:right}.katex-display{display:block;margin:1em 0;text-align:center}.katex-display>.katex{display:block;text-align:center;white-space:nowrap}.katex-display>.katex>.katex-html{display:block;position:relative}.katex-display>.katex>.katex-html>.tag{position:absolute;right:0}.katex-display.leqno>.katex>.katex-html>.tag{left:0;right:auto}.katex-display.fleqn>.katex{padding-left:2em;text-align:left}body{counter-reset:katexEqnNo mmlEqnNo}
</style><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/UnsafeBanner-ddce7340.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconInfected-6643da07.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/DatasetViewer-fec977c6.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconEnterprise-4e524547.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconRefresh-ea672c5c.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconCaretV2-a334b7cf.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconEye-5a189e62.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/DescriptionTooltip-f9a7470f.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconInfo-c26a12f6.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/Modal-56a38eee.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconCode-409e0b19.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/MediaViewer-072f0cc9.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconCaretLeft-2a3f58f0.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconCaretRight-1fadc85a.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconArrowTopRight-d0a97687.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconFilterFilled-2eedfec8.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconSpinner-fb9a1ad2.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/Pagination-7318db15.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/DatasetAndModelActionsDropdown-76223f94.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/handlers-47275c6a.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/MarkdownEditor-789d218c.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconImageFilled-47951a5f.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/UserAutocompleteMenu-948a4267.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/MarkdownContent-e7721491.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/interceptImages-a3800527.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/Media-66bac898.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/CloneInstructions-0994e3f9.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/CitationModal-b87f7580.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconWarning-8d76faac.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconKebabMenu-f485ffcd.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/IconMute-79c57296.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/CollectionModal-a10aff74.js"><link rel="modulepreload" as="script" crossorigin="" href="https://huggingface.co/front/build/kube-02525e9/DatasetLibraryModal-6cbc8006.js"></head>
	<body class="flex flex-col min-h-screen bg-white dark:bg-gray-950 text-black DatasetPage vsc-initialized" data-new-gr-c-s-check-loaded="14.1166.0" data-gr-ext-installed="">
		<div class="flex min-h-screen flex-col">
	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670933955682-63986cc798234ca22f870e7d.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;user&quot;:&quot;nelson2424&quot;,&quot;unreadNotifications&quot;:0,&quot;csrf&quot;:&quot;eyJkYXRhIjp7ImV4cGlyYXRpb24iOjE3MTI3NDE3Mjc1MzEsInVzZXJJZCI6IjYzOTg2Y2M3OTgyMzRjYTIyZjg3MGU3ZCJ9LCJzaWduYXR1cmUiOiIwNzNiYTczMGU2ZDkwOGUxM2VhMjFlOTdmNmZmMjNkNGYxNWUxNGI3ZDgwOTZjZDhiZGRiMmNhYTUyOTZjMDQ0In0=&quot;}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="https://huggingface.co/"><img alt="Hugging Face&#39;s logo" class="w-7 md:mr-2" src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/huggingface_logo-noborder.svg"> <span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a> <div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..." spellcheck="false" type="text"> <svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg> </div> <div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg> </button> </div></div> <nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 xl:space-x-2"><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-indigo-700" href="https://huggingface.co/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg> Models</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-red-700" href="https://huggingface.co/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg> Datasets</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-blue-700" href="https://huggingface.co/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg> Spaces</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="https://huggingface.co/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 !text-yellow-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg> Posts</a></li><li><a class="group flex items-center px-2 py-0.5 dark:hover:text-gray-400 hover:text-yellow-700" href="https://huggingface.co/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path opacity="0.5" d="M20.9022 5.10334L10.8012 10.8791L7.76318 9.11193C8.07741 8.56791 8.5256 8.11332 9.06512 7.7914L15.9336 3.73907C17.0868 3.08811 18.5002 3.26422 19.6534 3.91519L19.3859 3.73911C19.9253 4.06087 20.5879 4.56025 20.9022 5.10334Z" fill="currentColor"></path><path d="M10.7999 10.8792V28.5483C10.2136 28.5475 9.63494 28.4139 9.10745 28.1578C8.5429 27.8312 8.074 27.3621 7.74761 26.7975C7.42122 26.2327 7.24878 25.5923 7.24756 24.9402V10.9908C7.25062 10.3319 7.42358 9.68487 7.74973 9.1123L10.7999 10.8792Z" fill="currentColor" fill-opacity="0.75"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M21.3368 10.8499V6.918C21.3331 6.25959 21.16 5.61234 20.8346 5.03949L10.7971 10.8727L10.8046 10.874L21.3368 10.8499Z" fill="currentColor"></path><path opacity="0.5" d="M21.7937 10.8488L10.7825 10.8741V28.5486L21.7937 28.5234C23.3344 28.5234 24.5835 27.2743 24.5835 25.7335V13.6387C24.5835 12.0979 23.4365 11.1233 21.7937 10.8488Z" fill="currentColor"></path></svg> Docs</a></li> <li class="max-2xl:hidden"><div class="relative "><button class="px-2 py-0.5 group hover:text-green-700 dark:hover:text-gray-400 flex items-center " type="button"><svg class="mr-1.5 text-gray-400 group-hover:text-green-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M19 6H5a3 3 0 0 0-3 3v2.72L8.837 14h6.326L22 11.72V9a3 3 0 0 0-3-3z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M10 6V5h4v1h2V5a2.002 2.002 0 0 0-2-2h-4a2.002 2.002 0 0 0-2 2v1h2zm-1.163 8L2 11.72V18a3.003 3.003 0 0 0 3 3h14a3.003 3.003 0 0 0 3-3v-6.28L15.163 14H8.837z" fill="currentColor"></path></svg> Solutions </button> </div></li> <li><a class="group flex items-center px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-400" href="https://huggingface.co/pricing">Pricing</a></li> <li><div class="relative group"><button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button"><svg class="mr-1.5 text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>  </button> </div></li> <li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li> <li><form action="https://huggingface.co/logout" method="POST" class="hidden"><input type="hidden" name="csrf" value="eyJkYXRhIjp7ImV4cGlyYXRpb24iOjE3MTI3NDE3Mjc1MzEsInVzZXJJZCI6IjYzOTg2Y2M3OTgyMzRjYTIyZjg3MGU3ZCJ9LCJzaWduYXR1cmUiOiIwNzNiYTczMGU2ZDkwOGUxM2VhMjFlOTdmNmZmMjNkNGYxNWUxNGI3ZDgwOTZjZDhiZGRiMmNhYTUyOTZjMDQ0In0="></form> <div class="relative ml-2 w-[1.38rem] h-[1.38rem] "><button class="ml-auto rounded-full ring-2 group ring-indigo-400 focus:ring-blue-500 hover:ring-offset-1 focus:ring-offset-1 focus:outline-none outline-none dark:ring-offset-gray-950 " type="button"><div class="relative"><img alt="" class="h-[1.38rem] w-[1.38rem] overflow-hidden rounded-full" src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/1670933955682-63986cc798234ca22f870e7d.jpeg" crossorigin="anonymous"> </div> </button> </div></li></ul></nav></div></header></div>
	
	<div class="SVELTE_HYDRATER contents" data-target="GoogleAnalyticsTracker" data-props="{}"></div>
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{&quot;organizations&quot;:[]}"></div>
	

	<main class="flex flex-1 flex-col"><div class="SVELTE_HYDRATER contents" data-target="DatasetHeader" data-props="{&quot;activeTab&quot;:&quot;datasetCard&quot;,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d3e0e8ff1384ce6c5dd17d/h26nKAVX5MubfQiXItvCZ.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Librarian Bots&quot;,&quot;name&quot;:&quot;librarian-bots&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isEnterprise&quot;:false},&quot;canReadRepoSettings&quot;:false,&quot;dataset&quot;:{&quot;author&quot;:&quot;librarian-bots&quot;,&quot;cardData&quot;:{&quot;size_categories&quot;:[&quot;100K&lt;n&lt;1M&quot;],&quot;task_categories&quot;:[&quot;text-retrieval&quot;],&quot;pretty_name&quot;:&quot;Hugging Face Hub Model Cards&quot;,&quot;dataset_info&quot;:{&quot;features&quot;:[{&quot;name&quot;:&quot;modelId&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;author&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;last_modified&quot;,&quot;dtype&quot;:&quot;timestamp[us, tz=UTC]&quot;},{&quot;name&quot;:&quot;downloads&quot;,&quot;dtype&quot;:&quot;int64&quot;},{&quot;name&quot;:&quot;likes&quot;,&quot;dtype&quot;:&quot;int64&quot;},{&quot;name&quot;:&quot;library_name&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;tags&quot;,&quot;sequence&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;pipeline_tag&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;createdAt&quot;,&quot;dtype&quot;:&quot;timestamp[us, tz=UTC]&quot;},{&quot;name&quot;:&quot;card&quot;,&quot;dtype&quot;:&quot;string&quot;}],&quot;splits&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;num_bytes&quot;:1070235362,&quot;num_examples&quot;:581390}],&quot;download_size&quot;:323296504,&quot;dataset_size&quot;:1070235362},&quot;configs&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;train&quot;,&quot;path&quot;:&quot;data/train-*&quot;}]}],&quot;tags&quot;:[&quot;ethics&quot;]},&quot;cardExists&quot;:true,&quot;description&quot;:&quot;\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Hugging Face Hub Model Cards\n\t\n\nThis datasets consists of model cards for models hosted on the Hugging Face Hub. The model cards are created by the community and provide information about the model, its performance, its intended uses, and more. \nThis dataset is updated on a daily basis and includes publicly available models on the Hugging Face Hub.\nThis dataset is made available to help support users wanting to work with a large number of Model Cards from the Hub.… See the full description on the dataset page: https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata.&quot;,&quot;downloads&quot;:104,&quot;downloadsAllTime&quot;:435,&quot;id&quot;:&quot;librarian-bots/model_cards_with_metadata&quot;,&quot;isLikedByUser&quot;:true,&quot;isMutedByUser&quot;:false,&quot;isWatchedByUser&quot;:false,&quot;lastModified&quot;:&quot;2024-04-05T07:02:39.000Z&quot;,&quot;likes&quot;:7,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:581390,&quot;tags&quot;:[&quot;croissant&quot;],&quot;libraries&quot;:[&quot;datasets&quot;,&quot;dask&quot;,&quot;mlcroissant&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;task_categories:text-retrieval&quot;,&quot;size_categories:100K&lt;n&lt;1M&quot;,&quot;ethics&quot;,&quot;croissant&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;task_categories:text-retrieval&quot;,&quot;label&quot;:&quot;text-retrieval&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;nlp&quot;},{&quot;id&quot;:&quot;size_categories:100K&lt;n&lt;1M&quot;,&quot;label&quot;:&quot;100K&lt;n&lt;1M&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;ethics&quot;,&quot;label&quot;:&quot;ethics&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;croissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}]},&quot;discussionsStats&quot;:{&quot;closed&quot;:1,&quot;open&quot;:1,&quot;total&quot;:2},&quot;loggedInUser&quot;:&quot;nelson2424&quot;}"><header class="from-gray-50-to-white border-b border-gray-100 bg-gradient-to-t via-white dark:via-gray-950 pt-6 sm:pt-9"><div class="container relative "><h1 class="flex flex-wrap items-center leading-tight mb-3 text-lg md:text-xl"><a href="https://huggingface.co/datasets" class="group flex items-center"><svg class="mr-1.5 text-gray-400" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg> <span class="mr-2.5 font-semibold text-gray-400 group-hover:text-gray-500">Datasets:</span></a> <div class="group flex flex-none items-center"><div class="relative mr-1.5 flex items-center"> <img alt="" class="w-3.5 h-3.5 rounded  flex-none" src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/h26nKAVX5MubfQiXItvCZ.jpeg" crossorigin="anonymous"></div> <a href="https://huggingface.co/librarian-bots" class="text-gray-400 hover:text-blue-600">librarian-bots</a> <div class="mx-0.5 text-gray-300">/</div></div> <div class="max-w-full "><a class="break-words font-mono font-semibold hover:text-blue-600 " href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata">model_cards_with_metadata</a> <button class="relative text-sm mr-4 inline-flex cursor-pointer items-center text-sm focus:outline-none  mx-0.5   text-gray-600 " title="Copy dataset name to clipboard" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg>  </button></div> <div class="inline-flex items-center overflow-hidden whitespace-nowrap rounded-md border bg-white text-sm leading-none text-gray-500  mr-2"><button class="relative flex items-center overflow-hidden from-red-50 to-transparent dark:from-red-900 px-1.5 py-1 hover:bg-gradient-to-t focus:outline-none" title="Unlike"><svg class="left-1.5 absolute" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg> <svg class="absolute text-red-500 origin-center transform transition-transform ease-in
						
						left-1.5 " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.5,4c-2,0-3.9,0.8-5.3,2.2L16,7.4l-1.1-1.1C12,3.3,7.2,3.3,4.3,6.2c0,0-0.1,0.1-0.1,0.1c-3,3-3,7.8,0,10.8L16,29l11.8-11.9c3-3,3-7.8,0-10.8C26.4,4.8,24.5,4,22.5,4z"></path></svg> <span class="ml-4 pl-0.5 ">like</span></button> <button class="flex items-center border-l px-1.5 py-1 text-gray-400 hover:bg-gray-50 focus:bg-gray-100 focus:outline-none dark:hover:bg-gray-900 dark:focus:bg-gray-800" title="See users who liked this repository">7</button></div>  </h1> <div class="mb-3 flex flex-wrap md:mb-4"><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tasks:</span> <a class="tag  tag-white" href="https://huggingface.co/datasets?task_categories=task_categories%3Atext-retrieval"><div class="tag-ico tag-ico-indigo"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 18 19"><path d="M12.3625 13.85H10.1875V12.7625H12.3625V10.5875H13.45V12.7625C13.4497 13.0508 13.335 13.3272 13.1312 13.5311C12.9273 13.735 12.6508 13.8497 12.3625 13.85V13.85Z"></path><path d="M5.8375 8.41246H4.75V6.23746C4.75029 5.94913 4.86496 5.67269 5.06884 5.4688C5.27272 5.26492 5.54917 5.15025 5.8375 5.14996H8.0125V6.23746H5.8375V8.41246Z"></path><path d="M15.625 5.14998H13.45V2.97498C13.4497 2.68665 13.335 2.4102 13.1312 2.20632C12.9273 2.00244 12.6508 1.88777 12.3625 1.88748H2.575C2.28666 1.88777 2.01022 2.00244 1.80633 2.20632C1.60245 2.4102 1.48778 2.68665 1.4875 2.97498V12.7625C1.48778 13.0508 1.60245 13.3273 1.80633 13.5311C2.01022 13.735 2.28666 13.8497 2.575 13.85H4.75V16.025C4.75028 16.3133 4.86495 16.5898 5.06883 16.7936C5.27272 16.9975 5.54916 17.1122 5.8375 17.1125H15.625C15.9133 17.1122 16.1898 16.9975 16.3937 16.7936C16.5975 16.5898 16.7122 16.3133 16.7125 16.025V6.23748C16.7122 5.94915 16.5975 5.6727 16.3937 5.46882C16.1898 5.26494 15.9133 5.15027 15.625 5.14998V5.14998ZM15.625 16.025H5.8375V13.85H8.0125V12.7625H5.8375V10.5875H4.75V12.7625H2.575V2.97498H12.3625V5.14998H10.1875V6.23748H12.3625V8.41248H13.45V6.23748H15.625V16.025Z"></path></svg></div> <span>Text Retrieval</span> </a> </div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Size Categories:</span> <a class="tag  tag-orange" href="https://huggingface.co/datasets?size_categories=size_categories%3A100K%3Cn%3C1M"> <span>100K&lt;n&lt;1M</span> </a> </div><div class="mr-1 flex flex-wrap items-center"><span class="mb-1 mr-1 p-1 text-sm leading-tight text-gray-400 md:mb-1.5">Tags:</span> <a class="tag  tag-white" href="https://huggingface.co/datasets?other=ethics"> <span>ethics</span> </a><div class="relative inline-block mr-1 mb-1 md:mr-1.5 md:mb-1.5"><button class=" " type="button"><a class="tag mr-0 mb-0 md:mr-0 md:mb-0 tag-white" href="https://huggingface.co/datasets?other=croissant"><svg class="ml-2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="none" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="#F5AB6A"></path><path d="M22.2812 12.2656L25.9532 15.7187C26.8932 16.6587 28.0913 17.3931 29.0313 16.4531C29.8594 15.7812 29.9332 14.1 29.9532 13.5C29.9532 11.5625 28.9219 8.375 27.25 6.71875C25.5604 5.04493 23.3782 3.91692 22.7032 3.78125L22.2812 12.2656Z" fill="url(#paint0_radial_18_31665)"></path><g filter="url(#filter0_f_18_31665)"><path d="M22.2849 12.1817L23.4375 13.2656L24.4375 4.70312C23.5121 4.1242 23.0198 3.96369 22.6563 3.89062L22.2849 12.1817Z" fill="url(#paint1_linear_18_31665)"></path></g><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint2_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint3_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint4_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint5_radial_18_31665)"></path><path d="M22.7969 3.05741L21.8437 2.65269C19.6421 1.96765 17.2344 1.81208 14.6719 2.23236C12.1094 2.65264 11.5156 3.2442 11.5156 3.2442C10.896 3.3674 10.8671 3.88898 11.1718 4.45363C13.0797 6.72576 15.176 13.4043 18.0469 14.2506C18.89 14.4662 21.3791 14.4776 22.2019 14.1593L22.3238 14.108C22.6692 13.9745 23.1672 13.2814 23.1875 12.9118L23.4219 4.03043C23.4523 3.59924 23.1484 3.25204 22.7969 3.05741Z" fill="url(#paint6_linear_18_31665)"></path><g filter="url(#filter1_f_18_31665)"><path d="M13.1016 2.72656C11.862 3.06924 11.5298 3.40016 11.5298 3.40016C10.9102 3.52335 10.936 4.11525 11.2408 4.6799C13.1487 6.95202 15.1361 13.2496 18.007 14.0958C18.2707 14.1633 18.6953 14.2107 19.1797 14.2344L13.1016 2.72656Z" fill="url(#paint7_linear_18_31665)"></path></g><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint8_radial_18_31665)"></path><path d="M12.2187 22.7187L15.7656 26.2031C16.7332 27.1171 17.3334 28.2487 16.4219 29.2188C15.7749 30.0687 14.2241 29.9933 13.625 30.0313C11.6883 30.0891 9.09014 29.5622 6.84373 27.5313C5.07737 25.9343 4.09321 23.688 3.93751 23.0156L12.2187 22.7187Z" fill="url(#paint9_radial_18_31665)"></path><g filter="url(#filter2_f_18_31665)"><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint10_linear_18_31665)"></path><path d="M12.0523 22.7916L13.2187 23.9375L4.81018 24.5721C4.4328 23.8671 4.20835 23.2768 4.14062 22.9844L12.0523 22.7916Z" fill="url(#paint11_radial_18_31665)"></path></g><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="#EC9F6A"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint12_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint13_linear_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint14_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint15_radial_18_31665)"></path><path d="M2.99219 22.6484C2.07068 20.538 1.78913 17.4452 2.21096 15.0703C2.63279 12.6953 3.20759 11.951 3.20759 11.951C3.31231 11.3264 3.83281 11.2818 4.40626 11.5703C6.73409 13.4144 13.3119 15.2264 14.2432 18.0781C14.5947 19.3034 14.6279 21.3125 14.1641 22.5156C14.0409 22.8657 13.5 23.3516 13.0625 23.4141C12.625 23.4765 9.47656 23.3516 8.61719 23.3516C7.75781 23.3516 4.64844 23.6719 4.14062 23.6172C3.63281 23.5625 3.23437 23.2031 2.99219 22.6484Z" fill="url(#paint16_radial_18_31665)"></path><g filter="url(#filter3_f_18_31665)"><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint17_linear_18_31665)"></path><path d="M2.70313 13.6719C3.04135 12.4711 3.36224 12.0555 3.36224 12.0555C3.46697 11.4309 3.98746 11.3864 4.56092 11.6749C6.88874 13.5189 13.0809 15.1104 14.0121 17.9622C14.1731 18.5231 14.2766 19.0394 14.3287 19.5128L2.70313 13.6719Z" fill="url(#paint18_linear_18_31665)"></path></g><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="#D79453"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint19_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint20_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint21_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint22_linear_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint23_radial_18_31665)"></path><path d="M9.83184 2.82184C6.62184 4.07184 4.07184 6.62184 2.82184 9.83184C2.48184 10.7118 2.82184 11.7018 3.62184 12.1918L14.0418 18.5418C14.6618 18.9218 15.4418 18.9218 16.0618 18.5418C17.0718 17.9218 17.9318 17.0718 18.5418 16.0618C18.9218 15.4418 18.9218 14.6618 18.5418 14.0418L12.1918 3.62184C11.7018 2.82184 10.7118 2.48184 9.83184 2.82184Z" fill="url(#paint24_radial_18_31665)"></path><defs><filter id="filter0_f_18_31665" x="22.0349" y="3.64062" width="2.65265" height="9.875" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feflood flood-opacity="0" result="BackgroundImageFix"></feflood><feblend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feblend><fegaussianblur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></fegaussianblur></filter><filter id="filter1_f_18_31665" x="10.7815" y="2.47656" width="8.64819" height="12.0078" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feflood flood-opacity="0" result="BackgroundImageFix"></feflood><feblend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feblend><fegaussianblur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></fegaussianblur></filter><filter id="filter2_f_18_31665" x="3.89062" y="22.5416" width="9.57812" height="2.2804" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feflood flood-opacity="0" result="BackgroundImageFix"></feflood><feblend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feblend><fegaussianblur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></fegaussianblur></filter><filter id="filter3_f_18_31665" x="2.45312" y="11.2538" width="12.1255" height="8.50903" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feflood flood-opacity="0" result="BackgroundImageFix"></feflood><feblend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"></feblend><fegaussianblur stdDeviation="0.125" result="effect1_foregroundBlur_18_31665"></fegaussianblur></filter><radialgradient id="paint0_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.8125 12.9375) rotate(42.7741) scale(12.5164 7.08839)"><stop offset="0.0937591" stop-color="#C05159"></stop><stop offset="0.553697" stop-color="#F6AC6A"></stop><stop offset="0.832916" stop-color="#FFD186"></stop><stop offset="0.916927" stop-color="#FFDC87"></stop></radialgradient><lineargradient id="paint1_linear_18_31665" x1="24.7344" y1="4.67187" x2="20.8594" y2="12.8906" gradientUnits="userSpaceOnUse"><stop stop-color="#EBD67C"></stop><stop offset="0.0655686" stop-color="#FFFFA6"></stop><stop offset="0.530552" stop-color="#F8C281"></stop><stop offset="0.937338" stop-color="#E99E6B"></stop></lineargradient><radialgradient id="paint2_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.9009 13.1847) rotate(-127.648) scale(14.3438 11.7966)"><stop stop-color="#FFBE66"></stop><stop offset="1" stop-color="#E2AE5B"></stop></radialgradient><radialgradient id="paint3_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(18 11.4375) rotate(53.9726) scale(11.9013 4.84018)"><stop stop-color="#D67C63"></stop><stop offset="1" stop-color="#D97D67" stop-opacity="0"></stop></radialgradient><radialgradient id="paint4_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(23 4.1875) rotate(45.7639) scale(3.31486 5.75622)"><stop stop-color="#FFE4A6"></stop><stop offset="0.711285" stop-color="#F8B76F"></stop><stop offset="1" stop-color="#F9B870" stop-opacity="0"></stop></radialgradient><radialgradient id="paint5_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(22.875 12.4375) rotate(88.9391) scale(3.37558 1.29066)"><stop stop-color="#FFBC67"></stop><stop offset="1" stop-color="#FFBC67" stop-opacity="0"></stop></radialgradient><lineargradient id="paint6_linear_18_31665" x1="20.375" y1="15.6875" x2="20.125" y2="12.7813" gradientUnits="userSpaceOnUse"><stop offset="0.461609" stop-color="#B45077"></stop><stop offset="0.855389" stop-color="#B75077" stop-opacity="0"></stop></lineargradient><lineargradient id="paint7_linear_18_31665" x1="12.9375" y1="2.57056" x2="18.5625" y2="14.3891" gradientUnits="userSpaceOnUse"><stop stop-color="#DDC173"></stop><stop offset="0.485173" stop-color="#D59F65"></stop><stop offset="1" stop-color="#E49966"></stop></lineargradient><radialgradient id="paint8_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.5625 23.5) rotate(109.113) scale(6.68078 10.2578)"><stop offset="0.165756" stop-color="#FFBF7E"></stop><stop offset="0.827674" stop-color="#DF8C6D"></stop><stop offset="1" stop-color="#B05A66"></stop></radialgradient><radialgradient id="paint9_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.1875 26) rotate(41.0652) scale(8.37243 2.03649)"><stop stop-color="#FFD483"></stop><stop offset="1" stop-color="#FFD688" stop-opacity="0"></stop></radialgradient><lineargradient id="paint10_linear_18_31665" x1="3.96063" y1="23.794" x2="13.3748" y2="23.5143" gradientUnits="userSpaceOnUse"><stop stop-color="#A8716F"></stop><stop offset="0.103615" stop-color="#B37173"></stop><stop offset="0.225484" stop-color="#DB9F84"></stop><stop offset="0.799889" stop-color="#F1BB8A"></stop><stop offset="1" stop-color="#FFD780"></stop></lineargradient><radialgradient id="paint11_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4219 23.1719) rotate(-178.616) scale(3.23532 0.569081)"><stop offset="0.621498" stop-color="#AF5A3E"></stop><stop offset="1" stop-color="#B35445" stop-opacity="0"></stop></radialgradient><radialgradient id="paint12_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(13.625 19.125) rotate(-171.737) scale(15.2205 15.0767)"><stop offset="0.138435" stop-color="#FFB974"></stop><stop offset="0.403618" stop-color="#F2A56D"></stop><stop offset="0.925938" stop-color="#A16948"></stop></radialgradient><lineargradient id="paint13_linear_18_31665" x1="8.22184" y1="13.125" x2="6.81191" y2="15.4996" gradientUnits="userSpaceOnUse"><stop offset="0.610751" stop-color="#984847"></stop><stop offset="0.850075" stop-color="#9A4947" stop-opacity="0"></stop></lineargradient><radialgradient id="paint14_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.25 23.7461) scale(11.25 5.68361)"><stop stop-color="#C66364"></stop><stop offset="1" stop-color="#D4766B" stop-opacity="0"></stop></radialgradient><radialgradient id="paint15_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(7.875 23.5313) scale(10.0937 1.29657)"><stop stop-color="#B64B4B"></stop><stop offset="1" stop-color="#C56158" stop-opacity="0"></stop></radialgradient><radialgradient id="paint16_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(11.4375 19.875) rotate(-46.8882) scale(4.02385 7.51767)"><stop stop-color="#FFC083"></stop><stop offset="0.620218" stop-color="#FFBD7D" stop-opacity="0"></stop></radialgradient><lineargradient id="paint17_linear_18_31665" x1="2.8125" y1="13.0312" x2="14.5582" y2="18.9404" gradientUnits="userSpaceOnUse"><stop stop-color="#B89367"></stop><stop offset="1" stop-color="#C5835E"></stop></lineargradient><lineargradient id="paint18_linear_18_31665" x1="8.21875" y1="14.6406" x2="7.59349" y2="15.6717" gradientUnits="userSpaceOnUse"><stop offset="0.351552" stop-color="#A74746"></stop><stop offset="0.845198" stop-color="#A04346" stop-opacity="0"></stop></lineargradient><radialgradient id="paint19_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.5625 14.5625) rotate(140.244) scale(18.3733 13.7403)"><stop stop-color="#FDAE69"></stop><stop offset="0.729021" stop-color="#CE8C4F"></stop><stop offset="0.921546" stop-color="#AD7B45"></stop><stop offset="1" stop-color="#8B6B4A"></stop></radialgradient><radialgradient id="paint20_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.0625 7) rotate(65.3152) scale(11.0745 3.16547)"><stop offset="0.233237" stop-color="#FFD47C"></stop><stop offset="0.853648" stop-color="#FFD98B" stop-opacity="0"></stop></radialgradient><radialgradient id="paint21_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15.3125 8.875) rotate(100.886) scale(6.6191 5.57808)"><stop offset="0.128419" stop-color="#FFD88C"></stop><stop offset="0.924134" stop-color="#FFBE7B" stop-opacity="0"></stop></radialgradient><lineargradient id="paint22_linear_18_31665" x1="7.25" y1="15.1875" x2="10.7588" y2="10.3142" gradientUnits="userSpaceOnUse"><stop offset="0.142353" stop-color="#C15F4D"></stop><stop offset="1" stop-color="#D58366" stop-opacity="0"></stop></lineargradient><radialgradient id="paint23_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(8.15625 15.7813) rotate(28.5422) scale(12.5574 1.96589)"><stop offset="0.149989" stop-color="#E4745D"></stop><stop offset="0.453292" stop-color="#C8604C"></stop><stop offset="0.632597" stop-color="#C0605F"></stop><stop offset="1" stop-color="#C0605F" stop-opacity="0"></stop></radialgradient><radialgradient id="paint24_radial_18_31665" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(1.40625 2.69067) rotate(46.0943) scale(22.3963)"><stop offset="0.935802" stop-color="#C17C61" stop-opacity="0"></stop><stop offset="0.982109" stop-color="#C17C61"></stop></radialgradient></defs></svg> <span>Croissant</span> </a> </button> </div> </div></div> <div class="flex flex-col-reverse lg:flex-row lg:items-center lg:justify-between"><div class="-mb-px flex h-12 items-center overflow-x-auto overflow-y-hidden "><a class="tab-alternate active" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata"><svg class="mr-1.5 text-gray-400 flex-none" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg> Dataset card  </a><a class="tab-alternate " href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg> Viewer  </a><a class="tab-alternate " href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/tree/main"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-tertiary" d="M21 19h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0-8h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2zm0 4h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M9 19a1 1 0 0 1-1-1V6a1 1 0 0 1 2 0v12a1 1 0 0 1-1 1zm-6-4.333a1 1 0 0 1-.64-1.769L3.438 12l-1.078-.898a1 1 0 0 1 1.28-1.538l2 1.667a1 1 0 0 1 0 1.538l-2 1.667a.999.999 0 0 1-.64.231z" fill="currentColor"></path></svg> <span class="xl:hidden">Files</span> <span class="hidden xl:inline">Files and versions</span>  </a><a class="tab-alternate " href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/discussions"><svg class="mr-1.5 text-gray-400 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z" fill="#FF9D00"></path><path d="M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z" fill="#FFD21E"></path></svg> Community <div class="ml-1.5 flex h-4 min-w-[1rem] items-center justify-center rounded px-1 text-xs leading-none shadow-sm bg-black text-white dark:bg-gray-800 dark:text-gray-200">2</div> </a> </div> </div></div></header></div>
	
<div class="container relative flex flex-col md:grid md:space-y-0 w-full md:grid-cols-12 md:flex-1 md:grid-rows-full space-y-4 md:gap-6 ">
		<section class="pt-6 border-gray-100 md:col-span-8 pb-24 relative break-words copiable-code-container"><div class="SVELTE_HYDRATER contents" data-target="UnsafeBanner" data-props="{&quot;classNames&quot;:&quot;mb-4&quot;,&quot;repoId&quot;:&quot;librarian-bots/model_cards_with_metadata&quot;,&quot;repoType&quot;:&quot;dataset&quot;}"></div>

				<div class="SVELTE_HYDRATER contents" data-target="DatasetViewer" data-props="{&quot;data&quot;:{&quot;kind&quot;:&quot;DatasetAndSampleData&quot;,&quot;datasetInfo&quot;:[{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Size of downloaded dataset files:&quot;,&quot;value&quot;:&quot;323 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;/datasets/librarian-bots/model_cards_with_metadata/tree/refs%2Fconvert%2Fparquet/&quot;,&quot;label&quot;:&quot;Size of the auto-converted Parquet files:&quot;,&quot;value&quot;:&quot;323 MB&quot;},{&quot;isValid&quot;:true,&quot;href&quot;:&quot;&quot;,&quot;label&quot;:&quot;Number of rows:&quot;,&quot;value&quot;:&quot;581,390&quot;}],&quot;partial&quot;:false,&quot;configsData&quot;:{&quot;configInfos&quot;:[{&quot;name&quot;:&quot;default&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;numRows&quot;:581390}],&quot;selectedConfig&quot;:&quot;default&quot;,&quot;hasSelectedConfigParquet&quot;:true},&quot;splitsData&quot;:{&quot;splitInfos&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;numRows&quot;:581390}],&quot;selectedSplit&quot;:&quot;train&quot;},&quot;sampleData&quot;:{&quot;dataset&quot;:&quot;librarian-bots/model_cards_with_metadata&quot;,&quot;config&quot;:&quot;default&quot;,&quot;split&quot;:&quot;train&quot;,&quot;capabilities&quot;:{&quot;rows&quot;:true,&quot;search&quot;:true,&quot;filter&quot;:true,&quot;statistics&quot;:true},&quot;navigation&quot;:{&quot;p&quot;:0},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sIm9uQmVoYWxmT2YiOnsia2luZCI6InVzZXIiLCJfaWQiOiI2Mzk4NmNjNzk4MjM0Y2EyMmY4NzBlN2QiLCJ1c2VyIjoibmVsc29uMjQyNCJ9LCJpYXQiOjE3MTI2NTUzMjcsInN1YiI6Ii9kYXRhc2V0cy9saWJyYXJpYW4tYm90cy9tb2RlbF9jYXJkc193aXRoX21ldGFkYXRhIiwiZXhwIjoxNzEyNjU4OTI3LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.RZ5FKYVwHG-baF8G1xk_7I3o4X8De0UxPtc9tqhXdFe8rDVBMgqwzXzJIH1OMt2FDgpznA2kXf9ih7fDNjmrCA&quot;,&quot;sampleData&quot;:{&quot;columns&quot;:[{&quot;name&quot;:&quot;modelId&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;modelId&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:5,&quot;max&quot;:122,&quot;mean&quot;:33.24396,&quot;median&quot;:30,&quot;std&quot;:16.33862,&quot;histogram&quot;:{&quot;hist&quot;:[65941,202883,161519,84908,33195,19476,7505,4746,1193,24],&quot;bin_edges&quot;:[5,17,29,41,53,65,77,89,101,113,122]}}}},{&quot;name&quot;:&quot;author&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;author&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:2,&quot;max&quot;:42,&quot;mean&quot;:9.08137,&quot;median&quot;:9,&quot;std&quot;:3.24894,&quot;histogram&quot;:{&quot;hist&quot;:[121917,344426,98365,14940,1443,261,26,8,4],&quot;bin_edges&quot;:[2,7,12,17,22,27,32,37,42,42]}}}},{&quot;name&quot;:&quot;last_modified&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;unknown&quot;},{&quot;name&quot;:&quot;downloads&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;int64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;downloads&quot;,&quot;column_type&quot;:&quot;int&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:0,&quot;max&quot;:100903751,&quot;mean&quot;:1937.75453,&quot;median&quot;:0,&quot;std&quot;:235459.2543,&quot;histogram&quot;:{&quot;hist&quot;:[581375,7,1,1,2,2,0,0,1,1],&quot;bin_edges&quot;:[0,10090376,20180752,30271128,40361504,50451880,60542256,70632632,80723008,90813384,100903751]}}}},{&quot;name&quot;:&quot;likes&quot;,&quot;align&quot;:&quot;right&quot;,&quot;type&quot;:&quot;int64&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;likes&quot;,&quot;column_type&quot;:&quot;int&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:0,&quot;max&quot;:10567,&quot;mean&quot;:1.16928,&quot;median&quot;:0,&quot;std&quot;:30.65946,&quot;histogram&quot;:{&quot;hist&quot;:[581328,40,11,7,2,1,0,0,0,1],&quot;bin_edges&quot;:[0,1057,2114,3171,4228,5285,6342,7399,8456,9513,10567]}}}},{&quot;name&quot;:&quot;library_name&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;library_name&quot;,&quot;column_type&quot;:&quot;string_label&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:236432,&quot;nan_proportion&quot;:0.40667,&quot;no_label_count&quot;:0,&quot;no_label_proportion&quot;:0,&quot;n_unique&quot;:182,&quot;frequencies&quot;:{&quot;RAGatouille&quot;:4,&quot;PyTorch&quot;:6,&quot;clinicadl&quot;:8,&quot;seamless_communication&quot;:3,&quot;UniFormer&quot;:1,&quot;https://github.com/monet-joe/Piano-Classification&quot;:1,&quot;whisperkit&quot;:2,&quot;BERT&quot;:1,&quot;Flax&quot;:1,&quot;awesome-library&quot;:1,&quot;pcdet&quot;:2,&quot;Spacy Explosion&quot;:2,&quot;colbert-ai&quot;:1,&quot;pythae&quot;:27,&quot;scikit-learn&quot;:1,&quot;nemo&quot;:288,&quot;tensorflow, keras&quot;:1,&quot;sentence-transformers&quot;:4921,&quot;torch&quot;:3,&quot;EveryDream&quot;:2,&quot;safe-rlhf&quot;:5,&quot;sklearn&quot;:191,&quot;paddle_nlp&quot;:1,&quot;v-jepa&quot;:1,&quot;Spacy&quot;:3,&quot;zeroshot_classifier&quot;:9,&quot;monai&quot;:25,&quot;tf&quot;:11,&quot;KerasCV Stable Diffusion in Diffusers&quot;:1,&quot;gemma_torch&quot;:6,&quot;onnx&quot;:1,&quot;transformers&quot;:253283,&quot;lucidrains/gated-state-spaces-pytorch&quot;:1,&quot;Transformers&quot;:6,&quot;easydel&quot;:1,&quot;peft - PEFT 0.5.0&quot;:1,&quot;Transformers PHP&quot;:4,&quot;parking-env&quot;:1,&quot;ml-agents&quot;:7127,&quot;yolor&quot;:2,&quot;transformers, peft, torch&quot;:1,&quot;clot&quot;:1,&quot;peft&quot;:22509,&quot;CTranslate2&quot;:1,&quot;based&quot;:6,&quot;superb&quot;:18,&quot;coqui&quot;:3,&quot;opennmt&quot;:5,&quot;FastAI&quot;:1,&quot;PaddleNLP&quot;:1,&quot;keras3&quot;:1,&quot;stanza&quot;:89,&quot;yolo&quot;:1,&quot;pysentimiento&quot;:10,&quot;pruna-engine&quot;:1162,&quot;gpt-neox&quot;:1,&quot;fla&quot;:5,&quot;ultralytics&quot;:106,&quot;trl&quot;:66,&quot;tfhub&quot;:4,&quot;nanotron&quot;:1,&quot;pytorch_geometric&quot;:5,&quot;wildlife-datasets&quot;:5,&quot;output-small&quot;:1,&quot;sample-factory&quot;:1572,&quot;Bunkatopics&quot;:1,&quot;adapter-transformers&quot;:1256,&quot;fastpitch&quot;:1,&quot;k2&quot;:17,&quot;mlx-image&quot;:22,&quot;asteroid1111111111111111111111111111111&quot;:1,&quot;mlx-llm&quot;:4,&quot;rl-algo-impls&quot;:81,&quot;grok&quot;:2,&quot;sglang&quot;:2,&quot;yolov6detect&quot;:1,&quot;fasttext, bert&quot;:1,&quot;transformers, pe&quot;:1,&quot;xtuner&quot;:5,&quot;Core ML&quot;:1,&quot;whisper&quot;:1,&quot;jax&quot;:5,&quot;audiocraft&quot;:11,&quot;bitsandbytes, transformers, peft, accelerate, bitsandbytes, datasets, deepspeed, trl&quot;:1,&quot;archai&quot;:1,&quot;peft,sfttrainer&quot;:1,&quot;mlx&quot;:283,&quot;stable-diffusion&quot;:7,&quot;tensorflowtts&quot;:34,&quot;mlc&quot;:1,&quot;TraceBERT&quot;:1,&quot;scvi-tools&quot;:103,&quot;braindecode&quot;:1,&quot;GGUF&quot;:1,&quot;fairseq&quot;:153,&quot;open_clip&quot;:198,&quot;fairseq2&quot;:4,&quot;paddlenlp&quot;:71,&quot;yasep&quot;:1,&quot;fasttext&quot;:202,&quot;pytorch&quot;:415,&quot;keras-nlp&quot;:1,&quot;deep-rl-course&quot;:2,&quot;timm&quot;:1633,&quot;bertopic&quot;:231,&quot;mobileclip&quot;:6,&quot;fastai&quot;:459,&quot;mindspore&quot;:3,&quot;PyTorch Lightning&quot;:2,&quot;textgen&quot;:1,&quot;txtai&quot;:9,&quot;tokenizers&quot;:1,&quot;https://github.com/ICE-PIXIU&quot;:1,&quot;span-marker&quot;:69,&quot;TTS&quot;:3,&quot;xmen&quot;:5,&quot;htrflow_core&quot;:1,&quot;pytorch-lightning&quot;:1,&quot;hierarchy-transformers&quot;:8,&quot;stable-baselines3&quot;:14293,&quot;tensorflow&quot;:4,&quot;cleanrl&quot;:1715,&quot;safetensors&quot;:7,&quot;transformers(OpenNMT)&quot;:1,&quot;ml-aim&quot;:4,&quot;xpmir&quot;:4,&quot;femr&quot;:1,&quot;flair&quot;:1593,&quot;Megatron-LM&quot;:4,&quot;Keras&quot;:2,&quot;espnet&quot;:558,&quot;KerasNLP&quot;:1,&quot;Pytorch&quot;:1,&quot;minetest-baselines&quot;:5,&quot;OpenVINO&quot;:1,&quot;hezar&quot;:29,&quot;aster&quot;:1,&quot;kenlm&quot;:2,&quot;asteroid&quot;:109,&quot;allennlp&quot;:87,&quot;clmbr&quot;:1,&quot;ppdiffusers&quot;:2,&quot;tdc&quot;:9,&quot;ExLlamaV2&quot;:2,&quot;PyLaia&quot;:13,&quot;llama.cpp&quot;:9,&quot;doctr&quot;:4,&quot;UniFormerV2&quot;:1,&quot;skrl&quot;:27,&quot;transformers, peft&quot;:3,&quot;spacy&quot;:631,&quot;diffusers&quot;:23049,&quot;gliner&quot;:13,&quot;yolov5&quot;:45,&quot;k2-sherpa&quot;:1,&quot;ctranslate2&quot;:79,&quot;speechbrain&quot;:190,&quot;transformers.js&quot;:764,&quot;Doc-UFCN&quot;:3,&quot;pytorch-ie&quot;:1,&quot;non&quot;:1,&quot;JoeyNMT&quot;:2,&quot;gguf&quot;:48,&quot;transformers, Unsloth, Peft, trl, accelerate, bitsandbytes&quot;:1,&quot;ggml&quot;:6,&quot;setfit&quot;:324,&quot;keras&quot;:4072,&quot;muzero-general&quot;:1,&quot;generic&quot;:155,&quot;optimum_neuronx&quot;:1,&quot;fastMONAI&quot;:2,&quot;pyannote-audio&quot;:34,&quot;mlconsole&quot;:38,&quot;pufferlib&quot;:1,&quot;ctransformers&quot;:1,&quot;mbrl-lib&quot;:2,&quot;metavoice&quot;:2,&quot;aim&quot;:2,&quot;gemma.cpp&quot;:8,&quot;unity-sentis&quot;:22,&quot;atommic&quot;:61,&quot;DanishFungi&quot;:47}}}},{&quot;name&quot;:&quot;tags&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;sequence&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;tags&quot;,&quot;column_type&quot;:&quot;list&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1,&quot;max&quot;:1840,&quot;mean&quot;:6.00977,&quot;median&quot;:6,&quot;std&quot;:6.29342,&quot;histogram&quot;:{&quot;hist&quot;:[581323,51,15,0,0,0,0,0,0,1],&quot;bin_edges&quot;:[1,185,369,553,737,921,1105,1289,1473,1657,1840]}}}},{&quot;name&quot;:&quot;pipeline_tag&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;pipeline_tag&quot;,&quot;column_type&quot;:&quot;string_label&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:271438,&quot;nan_proportion&quot;:0.46688,&quot;no_label_count&quot;:0,&quot;no_label_proportion&quot;:0,&quot;n_unique&quot;:48,&quot;frequencies&quot;:{&quot;zero-shot-classification&quot;:240,&quot;other&quot;:20,&quot;token-classification&quot;:16255,&quot;fill-mask&quot;:11118,&quot;image-feature-extraction&quot;:195,&quot;image-to-image&quot;:291,&quot;text-to-3d&quot;:21,&quot;document-question-answering&quot;:163,&quot;text-retrieval&quot;:1,&quot;text-generation&quot;:73187,&quot;text-to-video&quot;:91,&quot;image-to-3d&quot;:31,&quot;feature-extraction&quot;:7672,&quot;image-to-video&quot;:19,&quot;table-question-answering&quot;:85,&quot;text-classification&quot;:54824,&quot;object-detection&quot;:1667,&quot;audio-to-audio&quot;:3728,&quot;summarization&quot;:1634,&quot;voice-activity-detection&quot;:25,&quot;reinforcement-learning&quot;:39779,&quot;time-series-forecasting&quot;:1,&quot;depth-estimation&quot;:70,&quot;image-classification&quot;:11211,&quot;sentence-similarity&quot;:3695,&quot;image-text-to-text&quot;:82,&quot;graph-ml&quot;:28,&quot;text-to-audio&quot;:697,&quot;text2text-generation&quot;:26161,&quot;unconditional-image-generation&quot;:1101,&quot;question-answering&quot;:10117,&quot;table-to-text&quot;:2,&quot;image-segmentation&quot;:620,&quot;image-to-text&quot;:450,&quot;robotics&quot;:20,&quot;visual-question-answering&quot;:186,&quot;tabular-regression&quot;:109,&quot;audio-classification&quot;:2021,&quot;multiple-choice&quot;:1071,&quot;zero-shot-object-detection&quot;:25,&quot;video-classification&quot;:760,&quot;text-to-image&quot;:19040,&quot;tabular-classification&quot;:189,&quot;text-to-speech&quot;:1852,&quot;mask-generation&quot;:49,&quot;automatic-speech-recognition&quot;:15526,&quot;translation&quot;:3442,&quot;zero-shot-image-classification&quot;:381}}}},{&quot;name&quot;:&quot;createdAt&quot;,&quot;align&quot;:&quot;left&quot;,&quot;type&quot;:&quot;unknown&quot;},{&quot;name&quot;:&quot;card&quot;,&quot;align&quot;:&quot;depends on text direction&quot;,&quot;type&quot;:&quot;string&quot;,&quot;statistics&quot;:{&quot;column_name&quot;:&quot;card&quot;,&quot;column_type&quot;:&quot;string_text&quot;,&quot;column_statistics&quot;:{&quot;nan_count&quot;:0,&quot;nan_proportion&quot;:0,&quot;min&quot;:1,&quot;max&quot;:900560,&quot;mean&quot;:1602.04921,&quot;median&quot;:39,&quot;std&quot;:6812.70691,&quot;histogram&quot;:{&quot;hist&quot;:[581041,242,57,14,16,4,7,4,2,3],&quot;bin_edges&quot;:[1,90057,180113,270169,360225,450281,540337,630393,720449,810505,900560]}}}}],&quot;rows&quot;:[{&quot;rowIdx&quot;:0,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pysentimiento/robertuito-sentiment-analysis&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pysentimiento&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-27T20:46:41\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:100903751,&quot;string&quot;:&quot;100,903,751&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:50,&quot;string&quot;:&quot;50&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pysentimiento&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;pysentimiento&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;safetensors&quot;,&quot;roberta&quot;,&quot;twitter&quot;,&quot;sentiment-analysis&quot;,&quot;es&quot;,&quot;arxiv:2106.09462&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;pysentimiento\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;roberta\&quot;,\n  \&quot;twitter\&quot;,\n  \&quot;sentiment-analysis\&quot;,\n  \&quot;es\&quot;,\n  \&quot;arxiv:2106.09462\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;null&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: \n  - es\nlibrary_name: pysentimiento\n\ntags:\n  - twitter\n  - sentiment-analysis\n\n---\n# Sentiment Analysis in Spanish\n## robertuito-sentiment-analysis\n\nRepository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## Usage\n\nUse it directly with [pysentimiento](https://github.com/pysentimiento/pysentimiento)\n\n```python\nfrom pysentimiento import create_analyzer\nanalyzer = create_analyzer(task=\&quot;sentiment\&quot;, lang=\&quot;es\&quot;)\n\nanalyzer.predict(\&quot;Qué gran jugador es Messi\&quot;)\n# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n```\n\n\n## Results\n\nResults for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores\n\n\n| model         | emotion       | hate_speech   | irony         | sentiment     |\n|:--------------|:--------------|:--------------|:--------------|:--------------|\n| robertuito    | 0.560 ± 0.010 | 0.759 ± 0.007 | 0.739 ± 0.005 | 0.705 ± 0.003 |\n| roberta       | 0.527 ± 0.015 | 0.741 ± 0.012 | 0.721 ± 0.008 | 0.670 ± 0.006 |\n| bertin        | 0.524 ± 0.007 | 0.738 ± 0.007 | 0.713 ± 0.012 | 0.666 ± 0.005 |\n| beto_uncased  | 0.532 ± 0.012 | 0.727 ± 0.016 | 0.701 ± 0.007 | 0.651 ± 0.006 |\n| beto_cased    | 0.516 ± 0.012 | 0.724 ± 0.012 | 0.705 ± 0.009 | 0.662 ± 0.005 |\n| mbert_uncased | 0.493 ± 0.010 | 0.718 ± 0.011 | 0.681 ± 0.010 | 0.617 ± 0.003 |\n| biGRU         | 0.264 ± 0.007 | 0.592 ± 0.018 | 0.631 ± 0.011 | 0.585 ± 0.011 |\n\n\nNote that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B\n\n## Citation\n\nIf you use this model in your research, please cite pysentimiento and RoBERTuito papers:\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@inproceedings{perez-etal-2022-robertuito,\n    title = \&quot;{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish\&quot;,\n    author = \&quot;P{\\&#39;e}rez, Juan Manuel  and\n      Furman, Dami{\\&#39;a}n Ariel  and\n      Alonso Alemany, Laura  and\n      Luque, Franco M.\&quot;,\n    booktitle = \&quot;Proceedings of the Thirteenth Language Resources and Evaluation Conference\&quot;,\n    month = jun,\n    year = \&quot;2022\&quot;,\n    address = \&quot;Marseille, France\&quot;,\n    publisher = \&quot;European Language Resources Association\&quot;,\n    url = \&quot;https://aclanthology.org/2022.lrec-1.785\&quot;,\n    pages = \&quot;7235--7243\&quot;,\n    abstract = \&quot;Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.\&quot;,\n}\n\n@inproceedings{garcia2020overview,\n  title={Overview of TASS 2020: Introducing emotion detection},\n  author={Garc{\\&#39;\\i}a-Vega, Manuel and D{\\&#39;\\i}az-Galiano, MC and Garc{\\&#39;\\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\\&#39;a}ez, A and Jim{\\&#39;e}nez-Zafra, SM and Mart{\\&#39;\\i}nez C{\\&#39;a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},\n  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\\&#39;a}laga, Spain},\n  pages={163--170},\n  year={2020}\n}\n```&quot;}}},{&quot;rowIdx&quot;:1,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;cardiffnlp&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-05-28T05:45:10\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:88609082,&quot;string&quot;:&quot;88,609,082&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:359,&quot;string&quot;:&quot;359&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;roberta&quot;,&quot;text-classification&quot;,&quot;en&quot;,&quot;dataset:tweet_eval&quot;,&quot;arxiv:2202.03829&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;roberta\&quot;,\n  \&quot;text-classification\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:tweet_eval\&quot;,\n  \&quot;arxiv:2202.03829\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;text-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-15T01:21:58\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\nwidget:\n- text: Covid cases are increasing fast!\ndatasets:\n- tweet_eval\n---\n\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). \n- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).\n\n&lt;b&gt;Labels&lt;/b&gt;: \n0 -&gt; Negative;\n1 -&gt; Neutral;\n2 -&gt; Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(\&quot;sentiment-analysis\&quot;, model=model_path, tokenizer=model_path)\nsentiment_task(\&quot;Covid cases are increasing fast!\&quot;)\n```\n```\n[{&#39;label&#39;: &#39;Negative&#39;, &#39;score&#39;: 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\&quot; \&quot;):\n        t = &#39;@user&#39; if t.startswith(&#39;@&#39;) and len(t) &gt; 1 else t\n        t = &#39;http&#39; if t.startswith(&#39;http&#39;) else t\n        new_text.append(t)\n    return \&quot; \&quot;.join(new_text)\nMODEL = f\&quot;cardiffnlp/twitter-roberta-base-sentiment-latest\&quot;\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \&quot;Covid cases are increasing fast!\&quot;\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors=&#39;pt&#39;)\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \&quot;Covid cases are increasing fast!\&quot;\n# encoded_input = tokenizer(text, return_tensors=&#39;tf&#39;)\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\&quot;{i+1}) {l} {np.round(float(s), 4)}\&quot;)\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \&quot;{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\&quot;,\n    author = \&quot;Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\&#39;\\i}nez C{\\&#39;a}mara, Eugenio\&quot; and others,\n    booktitle = \&quot;Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\&quot;,\n    month = dec,\n    year = \&quot;2022\&quot;,\n    address = \&quot;Abu Dhabi, UAE\&quot;,\n    publisher = \&quot;Association for Computational Linguistics\&quot;,\n    url = \&quot;https://aclanthology.org/2022.emnlp-demos.5\&quot;,\n    pages = \&quot;38--49\&quot;\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \&quot;{T}ime{LM}s: Diachronic Language Models from {T}witter\&quot;,\n    author = \&quot;Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\&quot;,\n    booktitle = \&quot;Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\&quot;,\n    month = may,\n    year = \&quot;2022\&quot;,\n    address = \&quot;Dublin, Ireland\&quot;,\n    publisher = \&quot;Association for Computational Linguistics\&quot;,\n    url = \&quot;https://aclanthology.org/2022.acl-demo.25\&quot;,\n    doi = \&quot;10.18653/v1/2022.acl-demo.25\&quot;,\n    pages = \&quot;251--260\&quot;\n}\n\n```\n&quot;}}},{&quot;rowIdx&quot;:2,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;jonatasgrosman/wav2vec2-large-xlsr-53-english&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;jonatasgrosman&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-03-25T10:56:55\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:56129414,&quot;string&quot;:&quot;56,129,414&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:395,&quot;string&quot;:&quot;395&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;jax&quot;,&quot;safetensors&quot;,&quot;wav2vec2&quot;,&quot;automatic-speech-recognition&quot;,&quot;audio&quot;,&quot;en&quot;,&quot;hf-asr-leaderboard&quot;,&quot;mozilla-foundation/common_voice_6_0&quot;,&quot;robust-speech-event&quot;,&quot;speech&quot;,&quot;xlsr-fine-tuning-week&quot;,&quot;dataset:common_voice&quot;,&quot;dataset:mozilla-foundation/common_voice_6_0&quot;,&quot;license:apache-2.0&quot;,&quot;model-index&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;wav2vec2\&quot;,\n  \&quot;automatic-speech-recognition\&quot;,\n  \&quot;audio\&quot;,\n  \&quot;en\&quot;,\n  \&quot;hf-asr-leaderboard\&quot;,\n  \&quot;mozilla-foundation/common_voice_6_0\&quot;,\n  \&quot;robust-speech-event\&quot;,\n  \&quot;speech\&quot;,\n  \&quot;xlsr-fine-tuning-week\&quot;,\n  \&quot;dataset:common_voice\&quot;,\n  \&quot;dataset:mozilla-foundation/common_voice_6_0\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;model-index\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;automatic-speech-recognition&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\ndatasets:\n- common_voice\n- mozilla-foundation/common_voice_6_0\nmetrics:\n- wer\n- cer\ntags:\n- audio\n- automatic-speech-recognition\n- en\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_6_0\n- robust-speech-event\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 English by Jonatas Grosman\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice en\n      type: common_voice\n      args: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 19.06\n    - name: Test CER\n      type: cer\n      value: 7.69\n    - name: Test WER (+LM)\n      type: wer\n      value: 14.81\n    - name: Test CER (+LM)\n      type: cer\n      value: 6.84\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: en\n    metrics:\n    - name: Dev WER\n      type: wer\n      value: 27.72\n    - name: Dev CER\n      type: cer\n      value: 11.65\n    - name: Dev WER (+LM)\n      type: wer\n      value: 20.85\n    - name: Dev CER (+LM)\n      type: cer\n      value: 11.01\n---\n\n# Fine-tuned XLSR-53 large model for speech recognition in English\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on English using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\&quot;jonatasgrosman/wav2vec2-large-xlsr-53-english\&quot;)\naudio_paths = [\&quot;/path/to/file.mp3\&quot;, \&quot;/path/to/another_file.wav\&quot;]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \&quot;en\&quot;\nMODEL_ID = \&quot;jonatasgrosman/wav2vec2-large-xlsr-53-english\&quot;\nSAMPLES = 10\n\ntest_dataset = load_dataset(\&quot;common_voice\&quot;, LANG_ID, split=f\&quot;test[:{SAMPLES}]\&quot;)\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\&quot;path\&quot;], sr=16_000)\n    batch[\&quot;speech\&quot;] = speech_array\n    batch[\&quot;sentence\&quot;] = batch[\&quot;sentence\&quot;].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\&quot;speech\&quot;], sampling_rate=16_000, return_tensors=\&quot;pt\&quot;, padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\&quot;-\&quot; * 100)\n    print(\&quot;Reference:\&quot;, test_dataset[i][\&quot;sentence\&quot;])\n    print(\&quot;Prediction:\&quot;, predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \&quot;SHE&#39;LL BE ALL RIGHT.\&quot; | SHE&#39;LL BE ALL RIGHT |\n| SIX | SIX |\n| \&quot;ALL&#39;S WELL THAT ENDS WELL.\&quot; | ALL AS WELL THAT ENDS WELL |\n| DO YOU MEAN IT? | DO YOU MEAN IT |\n| THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE, BUT STILL CAUSES REGRESSIONS. | THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE BUT STILL CAUSES REGRESSION |\n| HOW IS MOZILLA GOING TO HANDLE AMBIGUITIES LIKE QUEUE AND CUE? | HOW IS MOSLILLAR GOING TO HANDLE ANDBEWOOTH HIS LIKE Q AND Q |\n| \&quot;I GUESS YOU MUST THINK I&#39;M KINDA BATTY.\&quot; | RUSTIAN WASTIN PAN ONTE BATTLY |\n| NO ONE NEAR THE REMOTE MACHINE YOU COULD RING? | NO ONE NEAR THE REMOTE MACHINE YOU COULD RING |\n| SAUCE FOR THE GOOSE IS SAUCE FOR THE GANDER. | SAUCE FOR THE GUICE IS SAUCE FOR THE GONDER |\n| GROVES STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD. | GRAFS STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD |\n\n## Evaluation\n\n1. To evaluate on `mozilla-foundation/common_voice_6_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-english --dataset mozilla-foundation/common_voice_6_0 --config en --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-english --dataset speech-recognition-community-v2/dev_data --config en --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-english,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {E}nglish},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english}},\n  year={2021}\n}\n```&quot;}}},{&quot;rowIdx&quot;:3,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai/clip-vit-large-patch14&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-09-15T15:49:35\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:51405955,&quot;string&quot;:&quot;51,405,955&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1063,&quot;string&quot;:&quot;1,063&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;safetensors&quot;,&quot;clip&quot;,&quot;zero-shot-image-classification&quot;,&quot;vision&quot;,&quot;arxiv:2103.00020&quot;,&quot;arxiv:1908.04913&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;clip\&quot;,\n  \&quot;zero-shot-image-classification\&quot;,\n  \&quot;vision\&quot;,\n  \&quot;arxiv:2103.00020\&quot;,\n  \&quot;arxiv:1908.04913\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;zero-shot-image-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat &amp; Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\&quot;openai/clip-vit-large-patch14\&quot;)\nprocessor = CLIPProcessor.from_pretrained(\&quot;openai/clip-vit-large-patch14\&quot;)\n\nurl = \&quot;http://images.cocodataset.org/val2017/000000039769.jpg\&quot;\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\&quot;a photo of a cat\&quot;, \&quot;a photo of a dog\&quot;], images=image, return_tensors=\&quot;pt\&quot;, padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy &gt;96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)&quot;}}},{&quot;rowIdx&quot;:4,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;google-bert/bert-base-uncased&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;google-bert&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-19T11:06:12\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:45346028,&quot;string&quot;:&quot;45,346,028&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1461,&quot;string&quot;:&quot;1,461&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;rust&quot;,&quot;coreml&quot;,&quot;onnx&quot;,&quot;safetensors&quot;,&quot;bert&quot;,&quot;fill-mask&quot;,&quot;exbert&quot;,&quot;en&quot;,&quot;dataset:bookcorpus&quot;,&quot;dataset:wikipedia&quot;,&quot;arxiv:1810.04805&quot;,&quot;license:apache-2.0&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;rust\&quot;,\n  \&quot;coreml\&quot;,\n  \&quot;onnx\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;exbert\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:bookcorpus\&quot;,\n  \&quot;dataset:wikipedia\&quot;,\n  \&quot;arxiv:1810.04805\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fill-mask&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses &amp; limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it&#39;s mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;bert-base-uncased&#39;)\n&gt;&gt;&gt; unmasker(\&quot;Hello I&#39;m a [MASK] model.\&quot;)\n\n[{&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a fashion model. [SEP]\&quot;,\n  &#39;score&#39;: 0.1073106899857521,\n  &#39;token&#39;: 4827,\n  &#39;token_str&#39;: &#39;fashion&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a role model. [SEP]\&quot;,\n  &#39;score&#39;: 0.08774490654468536,\n  &#39;token&#39;: 2535,\n  &#39;token_str&#39;: &#39;role&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a new model. [SEP]\&quot;,\n  &#39;score&#39;: 0.05338378623127937,\n  &#39;token&#39;: 2047,\n  &#39;token_str&#39;: &#39;new&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a super model. [SEP]\&quot;,\n  &#39;score&#39;: 0.04667217284440994,\n  &#39;token&#39;: 3565,\n  &#39;token_str&#39;: &#39;super&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a fine model. [SEP]\&quot;,\n  &#39;score&#39;: 0.027095865458250046,\n  &#39;token&#39;: 2986,\n  &#39;token_str&#39;: &#39;fine&#39;}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)\nmodel = BertModel.from_pretrained(\&quot;bert-base-uncased\&quot;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;pt&#39;)\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)\nmodel = TFBertModel.from_pretrained(\&quot;bert-base-uncased\&quot;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;tf&#39;)\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;bert-base-uncased&#39;)\n&gt;&gt;&gt; unmasker(\&quot;The man worked as a [MASK].\&quot;)\n\n[{&#39;sequence&#39;: &#39;[CLS] the man worked as a carpenter. [SEP]&#39;,\n  &#39;score&#39;: 0.09747550636529922,\n  &#39;token&#39;: 10533,\n  &#39;token_str&#39;: &#39;carpenter&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the man worked as a waiter. [SEP]&#39;,\n  &#39;score&#39;: 0.0523831807076931,\n  &#39;token&#39;: 15610,\n  &#39;token_str&#39;: &#39;waiter&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the man worked as a barber. [SEP]&#39;,\n  &#39;score&#39;: 0.04962705448269844,\n  &#39;token&#39;: 13362,\n  &#39;token_str&#39;: &#39;barber&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the man worked as a mechanic. [SEP]&#39;,\n  &#39;score&#39;: 0.03788609802722931,\n  &#39;token&#39;: 15893,\n  &#39;token_str&#39;: &#39;mechanic&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the man worked as a salesman. [SEP]&#39;,\n  &#39;score&#39;: 0.037680890411138535,\n  &#39;token&#39;: 18968,\n  &#39;token_str&#39;: &#39;salesman&#39;}]\n\n&gt;&gt;&gt; unmasker(\&quot;The woman worked as a [MASK].\&quot;)\n\n[{&#39;sequence&#39;: &#39;[CLS] the woman worked as a nurse. [SEP]&#39;,\n  &#39;score&#39;: 0.21981462836265564,\n  &#39;token&#39;: 6821,\n  &#39;token_str&#39;: &#39;nurse&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the woman worked as a waitress. [SEP]&#39;,\n  &#39;score&#39;: 0.1597415804862976,\n  &#39;token&#39;: 13877,\n  &#39;token_str&#39;: &#39;waitress&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the woman worked as a maid. [SEP]&#39;,\n  &#39;score&#39;: 0.1154729500412941,\n  &#39;token&#39;: 10850,\n  &#39;token_str&#39;: &#39;maid&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the woman worked as a prostitute. [SEP]&#39;,\n  &#39;score&#39;: 0.037968918681144714,\n  &#39;token&#39;: 19215,\n  &#39;token_str&#39;: &#39;prostitute&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the woman worked as a cook. [SEP]&#39;,\n  &#39;score&#39;: 0.03042375110089779,\n  &#39;token&#39;: 5660,\n  &#39;token_str&#39;: &#39;cook&#39;}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it&#39;s another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\&quot;sentences\&quot; has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n&lt;a href=\&quot;https://huggingface.co/exbert/?model=bert-base-uncased\&quot;&gt;\n\t&lt;img width=\&quot;300px\&quot; src=\&quot;https://cdn-media.huggingface.co/exbert/button.png\&quot;&gt;\n&lt;/a&gt;\n&quot;}}},{&quot;rowIdx&quot;:5,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;CAMeL-Lab&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2021-10-18T10:15:57\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:42126368,&quot;string&quot;:&quot;42,126,368&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1,&quot;string&quot;:&quot;1&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;bert&quot;,&quot;token-classification&quot;,&quot;ar&quot;,&quot;arxiv:2103.06678&quot;,&quot;license:apache-2.0&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;token-classification\&quot;,\n  \&quot;ar\&quot;,\n  \&quot;arxiv:2103.06678\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;token-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: \n- ar\nlicense: apache-2.0\nwidget:\n - text: &#39;عامل ايه ؟&#39;\n---\n# CAMeLBERT-Mix POS-EGY Model\n## Model description\n**CAMeLBERT-Mix POS-EGY Model** is a Egyptian Arabic POS tagging model that was built by fine-tuning the [CAMeLBERT-Mix](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix/) model.\nFor the fine-tuning, we used the ARZTB dataset .\nOur fine-tuning procedure and the hyperparameters we used can be found in our paper *\&quot;[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678).\&quot;* Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT).\n\n## Intended uses\nYou can use the CAMeLBERT-Mix POS-EGY model as part of the transformers pipeline.\nThis model will also be available in [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) soon.\n\n#### How to use\nTo use the model with a transformers pipeline:\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; pos = pipeline(&#39;token-classification&#39;, model=&#39;CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy&#39;)\n&gt;&gt;&gt; text = &#39;عامل ايه ؟&#39;\n&gt;&gt;&gt; pos(text)\n[{&#39;entity&#39;: &#39;adj&#39;, &#39;score&#39;: 0.9972628, &#39;index&#39;: 1, &#39;word&#39;: &#39;عامل&#39;, &#39;start&#39;: 0, &#39;end&#39;: 4}, {&#39;entity&#39;: &#39;pron_interrog&#39;, &#39;score&#39;: 0.9525163, &#39;index&#39;: 2, &#39;word&#39;: &#39;ايه&#39;, &#39;start&#39;: 5, &#39;end&#39;: 8}, {&#39;entity&#39;: &#39;punc&#39;, &#39;score&#39;: 0.99869114, &#39;index&#39;: 3, &#39;word&#39;: &#39;؟&#39;, &#39;start&#39;: 9, &#39;end&#39;: 10}]\n```\n*Note*: to download our models, you would need `transformers&gt;=3.5.0`.\nOtherwise, you could download the models manually.\n## Citation\n```bibtex\n@inproceedings{inoue-etal-2021-interplay,\n    title = \&quot;The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models\&quot;,\n    author = \&quot;Inoue, Go  and\n      Alhafni, Bashar  and\n      Baimukan, Nurpeiis  and\n      Bouamor, Houda  and\n      Habash, Nizar\&quot;,\n    booktitle = \&quot;Proceedings of the Sixth Arabic Natural Language Processing Workshop\&quot;,\n    month = apr,\n    year = \&quot;2021\&quot;,\n    address = \&quot;Kyiv, Ukraine (Online)\&quot;,\n    publisher = \&quot;Association for Computational Linguistics\&quot;,\n    abstract = \&quot;In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.\&quot;,\n}\n```&quot;}}},{&quot;rowIdx&quot;:6,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;tohoku-nlp/bert-base-japanese&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;tohoku-nlp&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-22T00:57:00\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:31374146,&quot;string&quot;:&quot;31,374,146&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:27,&quot;string&quot;:&quot;27&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;bert&quot;,&quot;fill-mask&quot;,&quot;ja&quot;,&quot;dataset:wikipedia&quot;,&quot;license:cc-by-sa-4.0&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;ja\&quot;,\n  \&quot;dataset:wikipedia\&quot;,\n  \&quot;license:cc-by-sa-4.0\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fill-mask&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: ja\nlicense: cc-by-sa-4.0\ndatasets:\n- wikipedia\nwidget:\n- text: 東北大学で[MASK]の研究をしています。\n---\n\n# BERT base Japanese (IPA dictionary)\n\nThis is a [BERT](https://github.com/google-research/bert) model pretrained on texts in the Japanese language.\n\nThis version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.\n\nThe codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0).\n\n## Model architecture\n\nThe model architecture is the same as the original BERT base model; 12 layers, 768 dimensions of hidden states, and 12 attention heads.\n\n## Training Data\n\nThe model is trained on Japanese Wikipedia as of September 1, 2019.\nTo generate the training corpus, [WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.\nThe text files used for the training are 2.6GB in size, consisting of approximately 17M sentences.\n\n## Tokenization\n\nThe texts are first tokenized by [MeCab](https://taku910.github.io/mecab/) morphological parser with the IPA dictionary and then split into subwords by the WordPiece algorithm.\nThe vocabulary size is 32000.\n\n## Training\n\nThe model is trained with the same configuration as the original BERT; 512 tokens per instance, 256 instances per batch, and 1M training steps.\n\n## Licenses\n\nThe pretrained models are distributed under the terms of the [Creative Commons Attribution-ShareAlike 3.0](https://creativecommons.org/licenses/by-sa/3.0/).\n\n## Acknowledgments\n\nFor training models, we used Cloud TPUs provided by [TensorFlow Research Cloud](https://www.tensorflow.org/tfrc/) program.\n&quot;}}},{&quot;rowIdx&quot;:7,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-transformers/all-MiniLM-L6-v2&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-transformers&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-03-27T09:43:07\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:26283676,&quot;string&quot;:&quot;26,283,676&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1584,&quot;string&quot;:&quot;1,584&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;sentence-transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;rust&quot;,&quot;safetensors&quot;,&quot;bert&quot;,&quot;feature-extraction&quot;,&quot;sentence-similarity&quot;,&quot;transformers&quot;,&quot;en&quot;,&quot;dataset:s2orc&quot;,&quot;dataset:flax-sentence-embeddings/stackexchange_xml&quot;,&quot;dataset:ms_marco&quot;,&quot;dataset:gooaq&quot;,&quot;dataset:yahoo_answers_topics&quot;,&quot;dataset:code_search_net&quot;,&quot;dataset:search_qa&quot;,&quot;dataset:eli5&quot;,&quot;dataset:snli&quot;,&quot;dataset:multi_nli&quot;,&quot;dataset:wikihow&quot;,&quot;dataset:natural_questions&quot;,&quot;dataset:trivia_qa&quot;,&quot;dataset:embedding-data/sentence-compression&quot;,&quot;dataset:embedding-data/flickr30k-captions&quot;,&quot;dataset:embedding-data/altlex&quot;,&quot;dataset:embedding-data/simple-wiki&quot;,&quot;dataset:embedding-data/QQP&quot;,&quot;dataset:embedding-data/SPECTER&quot;,&quot;dataset:embedding-data/PAQ_pairs&quot;,&quot;dataset:embedding-data/WikiAnswers&quot;,&quot;arxiv:1904.06472&quot;,&quot;arxiv:2102.07033&quot;,&quot;arxiv:2104.08727&quot;,&quot;arxiv:1704.05179&quot;,&quot;arxiv:1810.09305&quot;,&quot;license:apache-2.0&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;sentence-transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;rust\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;feature-extraction\&quot;,\n  \&quot;sentence-similarity\&quot;,\n  \&quot;transformers\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:s2orc\&quot;,\n  \&quot;dataset:flax-sentence-embeddings/stackexchange_xml\&quot;,\n  \&quot;dataset:ms_marco\&quot;,\n  \&quot;dataset:gooaq\&quot;,\n  \&quot;dataset:yahoo_answers_topics\&quot;,\n  \&quot;dataset:code_search_net\&quot;,\n  \&quot;dataset:search_qa\&quot;,\n  \&quot;dataset:eli5\&quot;,\n  \&quot;dataset:snli\&quot;,\n  \&quot;dataset:multi_nli\&quot;,\n  \&quot;dataset:wikihow\&quot;,\n  \&quot;dataset:natural_questions\&quot;,\n  \&quot;dataset:trivia_qa\&quot;,\n  \&quot;dataset:embedding-data/sentence-compression\&quot;,\n  \&quot;dataset:embedding-data/flickr30k-captions\&quot;,\n  \&quot;dataset:embedding-data/altlex\&quot;,\n  \&quot;dataset:embedding-data/simple-wiki\&quot;,\n  \&quot;dataset:embedding-data/QQP\&quot;,\n  \&quot;dataset:embedding-data/SPECTER\&quot;,\n  \&quot;dataset:embedding-data/PAQ_pairs\&quot;,\n  \&quot;dataset:embedding-data/WikiAnswers\&quot;,\n  \&quot;arxiv:1904.06472\&quot;,\n  \&quot;arxiv:2102.07033\&quot;,\n  \&quot;arxiv:2104.08727\&quot;,\n  \&quot;arxiv:1704.05179\&quot;,\n  \&quot;arxiv:1810.09305\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-similarity&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences &amp; paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\&quot;This is an example sentence\&quot;, \&quot;Each sentence is converted\&quot;]\n\nmodel = SentenceTransformer(&#39;sentence-transformers/all-MiniLM-L6-v2&#39;)\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [&#39;This is an example sentence&#39;, &#39;Each sentence is converted&#39;]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(&#39;sentence-transformers/all-MiniLM-L6-v2&#39;)\nmodel = AutoModel.from_pretrained(&#39;sentence-transformers/all-MiniLM-L6-v2&#39;)\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=&#39;pt&#39;)\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input[&#39;attention_mask&#39;])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\&quot;Sentence embeddings:\&quot;)\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-MiniLM-L6-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP &amp; CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |&quot;}}},{&quot;rowIdx&quot;:8,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;mrm8488&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-01-21T15:17:58\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:19914327,&quot;string&quot;:&quot;19,914,327&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:215,&quot;string&quot;:&quot;215&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tensorboard&quot;,&quot;safetensors&quot;,&quot;roberta&quot;,&quot;text-classification&quot;,&quot;generated_from_trainer&quot;,&quot;financial&quot;,&quot;stocks&quot;,&quot;sentiment&quot;,&quot;dataset:financial_phrasebank&quot;,&quot;license:apache-2.0&quot;,&quot;model-index&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tensorboard\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;roberta\&quot;,\n  \&quot;text-classification\&quot;,\n  \&quot;generated_from_trainer\&quot;,\n  \&quot;financial\&quot;,\n  \&quot;stocks\&quot;,\n  \&quot;sentiment\&quot;,\n  \&quot;dataset:financial_phrasebank\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;model-index\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;text-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlicense: apache-2.0\nthumbnail: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\ntags:\n- generated_from_trainer\n- financial\n- stocks\n- sentiment\nwidget:\n- text: \&quot;Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .\&quot;\ndatasets:\n- financial_phrasebank\nmetrics:\n- accuracy\nmodel-index:\n- name: distilRoberta-financial-sentiment\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: financial_phrasebank\n      type: financial_phrasebank\n      args: sentences_allagree\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9823008849557522\n---\n\n&lt;!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. --&gt;\n\n\n&lt;div style=\&quot;text-align:center;width:250px;height:250px;\&quot;&gt;\n    &lt;img src=\&quot;https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\&quot; alt=\&quot;logo\&quot;&gt;\n&lt;/div&gt;\n\n\n# DistilRoberta-financial-sentiment\n\n\nThis model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1116\n- Accuracy: **0.98**23\n\n## Base Model description\n\nThis model is a distilled version of the [RoBERTa-base model](https://huggingface.co/roberta-base). It follows the same training procedure as [DistilBERT](https://huggingface.co/distilbert-base-uncased).\nThe code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/distillation).\nThis model is case-sensitive: it makes a difference between English and English.\n\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).\nOn average DistilRoBERTa is twice as fast as Roberta-base.\n\n## Training Data\n\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| No log        | 1.0   | 255  | 0.1670          | 0.9646   |\n| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |\n| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |\n| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |\n| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n&quot;}}},{&quot;rowIdx&quot;:9,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;distilbert/distilbert-base-uncased&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;distilbert&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-08-18T14:59:41\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:16539195,&quot;string&quot;:&quot;16,539,195&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:391,&quot;string&quot;:&quot;391&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;rust&quot;,&quot;safetensors&quot;,&quot;distilbert&quot;,&quot;fill-mask&quot;,&quot;exbert&quot;,&quot;en&quot;,&quot;dataset:bookcorpus&quot;,&quot;dataset:wikipedia&quot;,&quot;arxiv:1910.01108&quot;,&quot;license:apache-2.0&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;rust\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;distilbert\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;exbert\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:bookcorpus\&quot;,\n  \&quot;dataset:wikipedia\&quot;,\n  \&quot;arxiv:1910.01108\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fill-mask&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was\nintroduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found\n[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses &amp; limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it&#39;s mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;distilbert-base-uncased&#39;)\n&gt;&gt;&gt; unmasker(\&quot;Hello I&#39;m a [MASK] model.\&quot;)\n\n[{&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a role model. [SEP]\&quot;,\n  &#39;score&#39;: 0.05292855575680733,\n  &#39;token&#39;: 2535,\n  &#39;token_str&#39;: &#39;role&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a fashion model. [SEP]\&quot;,\n  &#39;score&#39;: 0.03968575969338417,\n  &#39;token&#39;: 4827,\n  &#39;token_str&#39;: &#39;fashion&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a business model. [SEP]\&quot;,\n  &#39;score&#39;: 0.034743521362543106,\n  &#39;token&#39;: 2449,\n  &#39;token_str&#39;: &#39;business&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a model model. [SEP]\&quot;,\n  &#39;score&#39;: 0.03462274372577667,\n  &#39;token&#39;: 2944,\n  &#39;token_str&#39;: &#39;model&#39;},\n {&#39;sequence&#39;: \&quot;[CLS] hello i&#39;m a modeling model. [SEP]\&quot;,\n  &#39;score&#39;: 0.018145186826586723,\n  &#39;token&#39;: 11643,\n  &#39;token_str&#39;: &#39;modeling&#39;}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained(&#39;distilbert-base-uncased&#39;)\nmodel = DistilBertModel.from_pretrained(\&quot;distilbert-base-uncased\&quot;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;pt&#39;)\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained(&#39;distilbert-base-uncased&#39;)\nmodel = TFDistilBertModel.from_pretrained(\&quot;distilbert-base-uncased\&quot;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;tf&#39;)\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;distilbert-base-uncased&#39;)\n&gt;&gt;&gt; unmasker(\&quot;The White man worked as a [MASK].\&quot;)\n\n[{&#39;sequence&#39;: &#39;[CLS] the white man worked as a blacksmith. [SEP]&#39;,\n  &#39;score&#39;: 0.1235365942120552,\n  &#39;token&#39;: 20987,\n  &#39;token_str&#39;: &#39;blacksmith&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the white man worked as a carpenter. [SEP]&#39;,\n  &#39;score&#39;: 0.10142576694488525,\n  &#39;token&#39;: 10533,\n  &#39;token_str&#39;: &#39;carpenter&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the white man worked as a farmer. [SEP]&#39;,\n  &#39;score&#39;: 0.04985016956925392,\n  &#39;token&#39;: 7500,\n  &#39;token_str&#39;: &#39;farmer&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the white man worked as a miner. [SEP]&#39;,\n  &#39;score&#39;: 0.03932540491223335,\n  &#39;token&#39;: 18594,\n  &#39;token_str&#39;: &#39;miner&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the white man worked as a butcher. [SEP]&#39;,\n  &#39;score&#39;: 0.03351764753460884,\n  &#39;token&#39;: 14998,\n  &#39;token_str&#39;: &#39;butcher&#39;}]\n\n&gt;&gt;&gt; unmasker(\&quot;The Black woman worked as a [MASK].\&quot;)\n\n[{&#39;sequence&#39;: &#39;[CLS] the black woman worked as a waitress. [SEP]&#39;,\n  &#39;score&#39;: 0.13283951580524445,\n  &#39;token&#39;: 13877,\n  &#39;token_str&#39;: &#39;waitress&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the black woman worked as a nurse. [SEP]&#39;,\n  &#39;score&#39;: 0.12586183845996857,\n  &#39;token&#39;: 6821,\n  &#39;token_str&#39;: &#39;nurse&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the black woman worked as a maid. [SEP]&#39;,\n  &#39;score&#39;: 0.11708822101354599,\n  &#39;token&#39;: 10850,\n  &#39;token_str&#39;: &#39;maid&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the black woman worked as a prostitute. [SEP]&#39;,\n  &#39;score&#39;: 0.11499975621700287,\n  &#39;token&#39;: 19215,\n  &#39;token_str&#39;: &#39;prostitute&#39;},\n {&#39;sequence&#39;: &#39;[CLS] the black woman worked as a housekeeper. [SEP]&#39;,\n  &#39;score&#39;: 0.04722772538661957,\n  &#39;token&#39;: 22583,\n  &#39;token_str&#39;: &#39;housekeeper&#39;}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it&#39;s another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\&quot;sentences\&quot; has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\n&lt;a href=\&quot;https://huggingface.co/exbert/?model=distilbert-base-uncased\&quot;&gt;\n\t&lt;img width=\&quot;300px\&quot; src=\&quot;https://cdn-media.huggingface.co/exbert/button.png\&quot;&gt;\n&lt;/a&gt;\n&quot;}}},{&quot;rowIdx&quot;:10,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FacebookAI/roberta-base&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FacebookAI&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-19T12:39:28\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:13634529,&quot;string&quot;:&quot;13,634,529&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:324,&quot;string&quot;:&quot;324&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;rust&quot;,&quot;safetensors&quot;,&quot;roberta&quot;,&quot;fill-mask&quot;,&quot;exbert&quot;,&quot;en&quot;,&quot;dataset:bookcorpus&quot;,&quot;dataset:wikipedia&quot;,&quot;arxiv:1907.11692&quot;,&quot;arxiv:1806.02847&quot;,&quot;license:mit&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;rust\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;roberta\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;exbert\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:bookcorpus\&quot;,\n  \&quot;dataset:wikipedia\&quot;,\n  \&quot;arxiv:1907.11692\&quot;,\n  \&quot;arxiv:1806.02847\&quot;,\n  \&quot;license:mit\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fill-mask&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\ntags:\n- exbert\nlicense: mit\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses &amp; limitations\n\nYou can use the raw model for masked language modeling, but it&#39;s mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;roberta-base&#39;)\n&gt;&gt;&gt; unmasker(\&quot;Hello I&#39;m a &lt;mask&gt; model.\&quot;)\n\n[{&#39;sequence&#39;: \&quot;&lt;s&gt;Hello I&#39;m a male model.&lt;/s&gt;\&quot;,\n  &#39;score&#39;: 0.3306540250778198,\n  &#39;token&#39;: 2943,\n  &#39;token_str&#39;: &#39;Ġmale&#39;},\n {&#39;sequence&#39;: \&quot;&lt;s&gt;Hello I&#39;m a female model.&lt;/s&gt;\&quot;,\n  &#39;score&#39;: 0.04655390977859497,\n  &#39;token&#39;: 2182,\n  &#39;token_str&#39;: &#39;Ġfemale&#39;},\n {&#39;sequence&#39;: \&quot;&lt;s&gt;Hello I&#39;m a professional model.&lt;/s&gt;\&quot;,\n  &#39;score&#39;: 0.04232972860336304,\n  &#39;token&#39;: 2038,\n  &#39;token_str&#39;: &#39;Ġprofessional&#39;},\n {&#39;sequence&#39;: \&quot;&lt;s&gt;Hello I&#39;m a fashion model.&lt;/s&gt;\&quot;,\n  &#39;score&#39;: 0.037216778844594955,\n  &#39;token&#39;: 2734,\n  &#39;token_str&#39;: &#39;Ġfashion&#39;},\n {&#39;sequence&#39;: \&quot;&lt;s&gt;Hello I&#39;m a Russian model.&lt;/s&gt;\&quot;,\n  &#39;score&#39;: 0.03253649175167084,\n  &#39;token&#39;: 1083,\n  &#39;token_str&#39;: &#39;ĠRussian&#39;}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)\nmodel = RobertaModel.from_pretrained(&#39;roberta-base&#39;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;pt&#39;)\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)\nmodel = TFRobertaModel.from_pretrained(&#39;roberta-base&#39;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;tf&#39;)\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;roberta-base&#39;)\n&gt;&gt;&gt; unmasker(\&quot;The man worked as a &lt;mask&gt;.\&quot;)\n\n[{&#39;sequence&#39;: &#39;&lt;s&gt;The man worked as a mechanic.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.08702439814805984,\n  &#39;token&#39;: 25682,\n  &#39;token_str&#39;: &#39;Ġmechanic&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The man worked as a waiter.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.0819653645157814,\n  &#39;token&#39;: 38233,\n  &#39;token_str&#39;: &#39;Ġwaiter&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The man worked as a butcher.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.073323555290699,\n  &#39;token&#39;: 32364,\n  &#39;token_str&#39;: &#39;Ġbutcher&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The man worked as a miner.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.046322137117385864,\n  &#39;token&#39;: 18678,\n  &#39;token_str&#39;: &#39;Ġminer&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The man worked as a guard.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.040150221437215805,\n  &#39;token&#39;: 2510,\n  &#39;token_str&#39;: &#39;Ġguard&#39;}]\n\n&gt;&gt;&gt; unmasker(\&quot;The Black woman worked as a &lt;mask&gt;.\&quot;)\n\n[{&#39;sequence&#39;: &#39;&lt;s&gt;The Black woman worked as a waitress.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.22177888453006744,\n  &#39;token&#39;: 35698,\n  &#39;token_str&#39;: &#39;Ġwaitress&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The Black woman worked as a prostitute.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.19288744032382965,\n  &#39;token&#39;: 36289,\n  &#39;token_str&#39;: &#39;Ġprostitute&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The Black woman worked as a maid.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.06498628109693527,\n  &#39;token&#39;: 29754,\n  &#39;token_str&#39;: &#39;Ġmaid&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The Black woman worked as a secretary.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.05375480651855469,\n  &#39;token&#39;: 2971,\n  &#39;token_str&#39;: &#39;Ġsecretary&#39;},\n {&#39;sequence&#39;: &#39;&lt;s&gt;The Black woman worked as a nurse.&lt;/s&gt;&#39;,\n  &#39;score&#39;: 0.05245552211999893,\n  &#39;token&#39;: 9008,\n  &#39;token_str&#39;: &#39;Ġnurse&#39;}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `&lt;s&gt;` and the end of one by `&lt;/s&gt;`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `&lt;mask&gt;`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n&lt;a href=\&quot;https://huggingface.co/exbert/?model=roberta-base\&quot;&gt;\n\t&lt;img width=\&quot;300px\&quot; src=\&quot;https://cdn-media.huggingface.co/exbert/button.png\&quot;&gt;\n&lt;/a&gt;\n&quot;}}},{&quot;rowIdx&quot;:11,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-transformers/all-mpnet-base-v2&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-transformers&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-03-27T09:46:22\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:13139589,&quot;string&quot;:&quot;13,139,589&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:629,&quot;string&quot;:&quot;629&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;sentence-transformers&quot;,&quot;pytorch&quot;,&quot;safetensors&quot;,&quot;mpnet&quot;,&quot;fill-mask&quot;,&quot;feature-extraction&quot;,&quot;sentence-similarity&quot;,&quot;transformers&quot;,&quot;en&quot;,&quot;dataset:s2orc&quot;,&quot;dataset:flax-sentence-embeddings/stackexchange_xml&quot;,&quot;dataset:ms_marco&quot;,&quot;dataset:gooaq&quot;,&quot;dataset:yahoo_answers_topics&quot;,&quot;dataset:code_search_net&quot;,&quot;dataset:search_qa&quot;,&quot;dataset:eli5&quot;,&quot;dataset:snli&quot;,&quot;dataset:multi_nli&quot;,&quot;dataset:wikihow&quot;,&quot;dataset:natural_questions&quot;,&quot;dataset:trivia_qa&quot;,&quot;dataset:embedding-data/sentence-compression&quot;,&quot;dataset:embedding-data/flickr30k-captions&quot;,&quot;dataset:embedding-data/altlex&quot;,&quot;dataset:embedding-data/simple-wiki&quot;,&quot;dataset:embedding-data/QQP&quot;,&quot;dataset:embedding-data/SPECTER&quot;,&quot;dataset:embedding-data/PAQ_pairs&quot;,&quot;dataset:embedding-data/WikiAnswers&quot;,&quot;arxiv:1904.06472&quot;,&quot;arxiv:2102.07033&quot;,&quot;arxiv:2104.08727&quot;,&quot;arxiv:1704.05179&quot;,&quot;arxiv:1810.09305&quot;,&quot;license:apache-2.0&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;sentence-transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;mpnet\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;feature-extraction\&quot;,\n  \&quot;sentence-similarity\&quot;,\n  \&quot;transformers\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:s2orc\&quot;,\n  \&quot;dataset:flax-sentence-embeddings/stackexchange_xml\&quot;,\n  \&quot;dataset:ms_marco\&quot;,\n  \&quot;dataset:gooaq\&quot;,\n  \&quot;dataset:yahoo_answers_topics\&quot;,\n  \&quot;dataset:code_search_net\&quot;,\n  \&quot;dataset:search_qa\&quot;,\n  \&quot;dataset:eli5\&quot;,\n  \&quot;dataset:snli\&quot;,\n  \&quot;dataset:multi_nli\&quot;,\n  \&quot;dataset:wikihow\&quot;,\n  \&quot;dataset:natural_questions\&quot;,\n  \&quot;dataset:trivia_qa\&quot;,\n  \&quot;dataset:embedding-data/sentence-compression\&quot;,\n  \&quot;dataset:embedding-data/flickr30k-captions\&quot;,\n  \&quot;dataset:embedding-data/altlex\&quot;,\n  \&quot;dataset:embedding-data/simple-wiki\&quot;,\n  \&quot;dataset:embedding-data/QQP\&quot;,\n  \&quot;dataset:embedding-data/SPECTER\&quot;,\n  \&quot;dataset:embedding-data/PAQ_pairs\&quot;,\n  \&quot;dataset:embedding-data/WikiAnswers\&quot;,\n  \&quot;arxiv:1904.06472\&quot;,\n  \&quot;arxiv:2102.07033\&quot;,\n  \&quot;arxiv:2104.08727\&quot;,\n  \&quot;arxiv:1704.05179\&quot;,\n  \&quot;arxiv:1810.09305\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;sentence-similarity&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-mpnet-base-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\&quot;This is an example sentence\&quot;, \&quot;Each sentence is converted\&quot;]\n\nmodel = SentenceTransformer(&#39;sentence-transformers/all-mpnet-base-v2&#39;)\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [&#39;This is an example sentence&#39;, &#39;Each sentence is converted&#39;]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(&#39;sentence-transformers/all-mpnet-base-v2&#39;)\nmodel = AutoModel.from_pretrained(&#39;sentence-transformers/all-mpnet-base-v2&#39;)\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=&#39;pt&#39;)\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input[&#39;attention_mask&#39;])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\&quot;Sentence embeddings:\&quot;)\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-mpnet-base-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developped this model during the \n[Community week using JAX/Flax for NLP &amp; CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |&quot;}}},{&quot;rowIdx&quot;:12,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;lanwuwei/GigaBERT-v4-Arabic-and-English&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;lanwuwei&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2021-05-19T21:19:13\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:13057217,&quot;string&quot;:&quot;13,057,217&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:3,&quot;string&quot;:&quot;3&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;jax&quot;,&quot;bert&quot;,&quot;feature-extraction&quot;,&quot;endpoints_compatible&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;feature-extraction\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;feature-extraction&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;## GigaBERT-v4\nGigaBERT-v4 is a continued pre-training of [GigaBERT-v3](https://huggingface.co/lanwuwei/GigaBERT-v3-Arabic-and-English) on code-switched data, showing improved zero-shot transfer performance from English to Arabic on information extraction (IE) tasks. More details can be found in the following paper:\n\n\t@inproceedings{lan2020gigabert,\n\t  author     = {Lan, Wuwei and Chen, Yang and Xu, Wei and Ritter, Alan},\n  \t  title      = {GigaBERT: Zero-shot Transfer Learning from English to Arabic},\n  \t  booktitle  = {Proceedings of The 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP)},\n  \t  year       = {2020}\n  \t} \n\n## Download\n```\nfrom transformers import *\ntokenizer = BertTokenizer.from_pretrained(\&quot;lanwuwei/GigaBERT-v4-Arabic-and-English\&quot;, do_lower_case=True)\nmodel = BertForTokenClassification.from_pretrained(\&quot;lanwuwei/GigaBERT-v4-Arabic-and-English\&quot;)\n```\nHere is downloadable link [GigaBERT-v4](https://drive.google.com/drive/u/1/folders/1uFGzMuTOD7iNsmKQYp_zVuvsJwOaIdar).\n\n&quot;}}},{&quot;rowIdx&quot;:13,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;smilegate-ai/kor_unsmile&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;smilegate-ai&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-28T01:34:57\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10604648,&quot;string&quot;:&quot;10,604,648&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:7,&quot;string&quot;:&quot;7&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;bert&quot;,&quot;text-classification&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;text-classification\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;text-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-28T01:03:23\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;Entry not found&quot;}}},{&quot;rowIdx&quot;:14,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;microsoft/layoutlmv3-base&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;microsoft&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-04-12T12:49:21\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:10196485,&quot;string&quot;:&quot;10,196,485&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:252,&quot;string&quot;:&quot;252&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;onnx&quot;,&quot;layoutlmv3&quot;,&quot;en&quot;,&quot;arxiv:2204.08387&quot;,&quot;license:cc-by-nc-sa-4.0&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;onnx\&quot;,\n  \&quot;layoutlmv3\&quot;,\n  \&quot;en\&quot;,\n  \&quot;arxiv:2204.08387\&quot;,\n  \&quot;license:cc-by-nc-sa-4.0\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;null&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-04-18T06:53:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\nlicense: cc-by-nc-sa-4.0\n\n---\n# LayoutLMv3\n\n[Microsoft Document AI](https://www.microsoft.com/en-us/research/project/document-ai/) | [GitHub](https://aka.ms/layoutlmv3)\n\n## Model description\n\nLayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.\n\n[LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387)\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, ACM Multimedia 2022.\n\n## Citation\n\nIf you find LayoutLM useful in your research, please cite the following paper:\n\n```\n@inproceedings{huang2022layoutlmv3,\n  author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},\n  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},\n  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},\n  year={2022}\n}\n```\n\n## License\n\nThe content of this project itself is licensed under the [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/).\nPortions of the source code are based on the [transformers](https://github.com/huggingface/transformers) project.\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)\n&quot;}}},{&quot;rowIdx&quot;:15,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai/clip-vit-base-patch16&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-10-04T09:42:28\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:9194017,&quot;string&quot;:&quot;9,194,017&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:65,&quot;string&quot;:&quot;65&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;jax&quot;,&quot;clip&quot;,&quot;zero-shot-image-classification&quot;,&quot;vision&quot;,&quot;arxiv:2103.00020&quot;,&quot;arxiv:1908.04913&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;clip\&quot;,\n  \&quot;zero-shot-image-classification\&quot;,\n  \&quot;vision\&quot;,\n  \&quot;arxiv:2103.00020\&quot;,\n  \&quot;arxiv:1908.04913\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;zero-shot-image-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat &amp; Dog\n---\n# Model Card: CLIP\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n\n## Model Details\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.\n\n\n### Model Date\nJanuary 2021\n\n\n### Model Type\nThe base model uses a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n```python3\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\&quot;openai/clip-vit-base-patch16\&quot;)\nprocessor = CLIPProcessor.from_pretrained(\&quot;openai/clip-vit-base-patch16\&quot;)\nurl = \&quot;http://images.cocodataset.org/val2017/000000039769.jpg\&quot;\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[\&quot;a photo of a cat\&quot;, \&quot;a photo of a dog\&quot;], images=image, return_tensors=\&quot;pt\&quot;, padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n\n#### Primary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n\n### Out-of-Scope Use Cases\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n## Data\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n\n### Data Mission Statement\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n## Performance and Limitations\n\n\n### Performance\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n\n## Limitations\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n\n### Bias and Fairness\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy &gt;96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n## Feedback\n\n\n### Where to send questions or comments about the model\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)\n&quot;}}},{&quot;rowIdx&quot;:16,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;facebook/wav2vec2-base-960h&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;facebook&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-11-14T21:37:23\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:9109343,&quot;string&quot;:&quot;9,109,343&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:224,&quot;string&quot;:&quot;224&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;safetensors&quot;,&quot;wav2vec2&quot;,&quot;automatic-speech-recognition&quot;,&quot;audio&quot;,&quot;hf-asr-leaderboard&quot;,&quot;en&quot;,&quot;dataset:librispeech_asr&quot;,&quot;arxiv:2006.11477&quot;,&quot;license:apache-2.0&quot;,&quot;model-index&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;wav2vec2\&quot;,\n  \&quot;automatic-speech-recognition\&quot;,\n  \&quot;audio\&quot;,\n  \&quot;hf-asr-leaderboard\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:librispeech_asr\&quot;,\n  \&quot;arxiv:2006.11477\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;model-index\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;automatic-speech-recognition&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\ndatasets:\n- librispeech_asr\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nlicense: apache-2.0\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\nmodel-index:\n- name: wav2vec2-base-960h\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (clean)\n      type: librispeech_asr\n      config: clean\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 3.4\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (other)\n      type: librispeech_asr\n      config: other\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 8.6\n---\n\n# Wav2Vec2-Base-960h\n\n[Facebook&#39;s Wav2Vec2](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\n\nThe base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model\nmake sure that your speech input is also sampled at 16Khz.\n\n[Paper](https://arxiv.org/abs/2006.11477)\n\nAuthors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli\n\n**Abstract**\n\nWe show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.\n\nThe original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.\n\n\n# Usage\n\nTo transcribe audio files the model can be used as a standalone acoustic model as follows:\n\n```python\n from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n from datasets import load_dataset\n import torch\n \n # load model and tokenizer\n processor = Wav2Vec2Processor.from_pretrained(\&quot;facebook/wav2vec2-base-960h\&quot;)\n model = Wav2Vec2ForCTC.from_pretrained(\&quot;facebook/wav2vec2-base-960h\&quot;)\n     \n # load dummy dataset and read soundfiles\n ds = load_dataset(\&quot;patrickvonplaten/librispeech_asr_dummy\&quot;, \&quot;clean\&quot;, split=\&quot;validation\&quot;)\n \n # tokenize\n input_values = processor(ds[0][\&quot;audio\&quot;][\&quot;array\&quot;], return_tensors=\&quot;pt\&quot;, padding=\&quot;longest\&quot;).input_values  # Batch size 1\n \n # retrieve logits\n logits = model(input_values).logits\n \n # take argmax and decode\n predicted_ids = torch.argmax(logits, dim=-1)\n transcription = processor.batch_decode(predicted_ids)\n ```\n \n ## Evaluation\n \n This code snippet shows how to evaluate **facebook/wav2vec2-base-960h** on LibriSpeech&#39;s \&quot;clean\&quot; and \&quot;other\&quot; test data.\n \n```python\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport torch\nfrom jiwer import wer\n\n\nlibrispeech_eval = load_dataset(\&quot;librispeech_asr\&quot;, \&quot;clean\&quot;, split=\&quot;test\&quot;)\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\&quot;facebook/wav2vec2-base-960h\&quot;).to(\&quot;cuda\&quot;)\nprocessor = Wav2Vec2Processor.from_pretrained(\&quot;facebook/wav2vec2-base-960h\&quot;)\n\ndef map_to_pred(batch):\n    input_values = processor(batch[\&quot;audio\&quot;][\&quot;array\&quot;], return_tensors=\&quot;pt\&quot;, padding=\&quot;longest\&quot;).input_values\n    with torch.no_grad():\n        logits = model(input_values.to(\&quot;cuda\&quot;)).logits\n\n    predicted_ids = torch.argmax(logits, dim=-1)\n    transcription = processor.batch_decode(predicted_ids)\n    batch[\&quot;transcription\&quot;] = transcription\n    return batch\n\nresult = librispeech_eval.map(map_to_pred, batched=True, batch_size=1, remove_columns=[\&quot;audio\&quot;])\n\nprint(\&quot;WER:\&quot;, wer(result[\&quot;text\&quot;], result[\&quot;transcription\&quot;]))\n```\n\n*Result (WER)*:\n\n| \&quot;clean\&quot; | \&quot;other\&quot; |\n|---|---|\n| 3.4 | 8.6 |&quot;}}},{&quot;rowIdx&quot;:17,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai/clip-vit-base-patch32&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-29T09:45:55\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:9064050,&quot;string&quot;:&quot;9,064,050&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:350,&quot;string&quot;:&quot;350&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;clip&quot;,&quot;zero-shot-image-classification&quot;,&quot;vision&quot;,&quot;arxiv:2103.00020&quot;,&quot;arxiv:1908.04913&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;clip\&quot;,\n  \&quot;zero-shot-image-classification\&quot;,\n  \&quot;vision\&quot;,\n  \&quot;arxiv:2103.00020\&quot;,\n  \&quot;arxiv:1908.04913\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;zero-shot-image-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat &amp; Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python3\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\&quot;openai/clip-vit-base-patch32\&quot;)\nprocessor = CLIPProcessor.from_pretrained(\&quot;openai/clip-vit-base-patch32\&quot;)\n\nurl = \&quot;http://images.cocodataset.org/val2017/000000039769.jpg\&quot;\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\&quot;a photo of a cat\&quot;, \&quot;a photo of a dog\&quot;], images=image, return_tensors=\&quot;pt\&quot;, padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy &gt;96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)&quot;}}},{&quot;rowIdx&quot;:18,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;facebook/contriever&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;facebook&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-01-19T17:23:28\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:8397436,&quot;string&quot;:&quot;8,397,436&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:43,&quot;string&quot;:&quot;43&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;bert&quot;,&quot;arxiv:2112.09118&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;arxiv:2112.09118\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;null&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;This model has been trained without supervision following the approach described in [Towards Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118). The associated GitHub repository is available here https://github.com/facebookresearch/contriever.\n\n## Usage (HuggingFace Transformers)\nUsing the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding.\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(&#39;facebook/contriever&#39;)\nmodel = AutoModel.from_pretrained(&#39;facebook/contriever&#39;)\n\nsentences = [\n    \&quot;Where was Marie Curie born?\&quot;,\n    \&quot;Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\&quot;,\n    \&quot;Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\&quot;\n]\n\n# Apply tokenizer\ninputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=&#39;pt&#39;)\n\n# Compute token embeddings\noutputs = model(**inputs)\n\n# Mean pooling\ndef mean_pooling(token_embeddings, mask):\n    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n    return sentence_embeddings\nembeddings = mean_pooling(outputs[0], inputs[&#39;attention_mask&#39;])\n```&quot;}}},{&quot;rowIdx&quot;:19,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai-community/gpt2&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai-community&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-19T10:57:45\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:7787873,&quot;string&quot;:&quot;7,787,873&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:1791,&quot;string&quot;:&quot;1,791&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;tflite&quot;,&quot;rust&quot;,&quot;onnx&quot;,&quot;safetensors&quot;,&quot;gpt2&quot;,&quot;text-generation&quot;,&quot;exbert&quot;,&quot;en&quot;,&quot;doi:10.57967/hf/0039&quot;,&quot;license:mit&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;text-generation-inference&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;tflite\&quot;,\n  \&quot;rust\&quot;,\n  \&quot;onnx\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;gpt2\&quot;,\n  \&quot;text-generation\&quot;,\n  \&quot;exbert\&quot;,\n  \&quot;en\&quot;,\n  \&quot;doi:10.57967/hf/0039\&quot;,\n  \&quot;license:mit\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;text-generation-inference\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;text-generation&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\ntags:\n- exbert\n\nlicense: mit\n---\n\n\n# GPT-2\n\nTest the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\nand first released at [this page](https://openai.com/blog/better-language-models/).\n\nDisclaimer: The team releasing GPT-2 also wrote a\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\n## Model description\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\nThis is the **smallest** version of GPT-2, with 124M parameters. \n\n**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n\n## Intended uses &amp; limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline, set_seed\n&gt;&gt;&gt; generator = pipeline(&#39;text-generation&#39;, model=&#39;gpt2&#39;)\n&gt;&gt;&gt; set_seed(42)\n&gt;&gt;&gt; generator(\&quot;Hello, I&#39;m a language model,\&quot;, max_length=30, num_return_sequences=5)\n\n[{&#39;generated_text&#39;: \&quot;Hello, I&#39;m a language model, a language for thinking, a language for expressing thoughts.\&quot;},\n {&#39;generated_text&#39;: \&quot;Hello, I&#39;m a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\&quot;},\n {&#39;generated_text&#39;: \&quot;Hello, I&#39;m a language model, and also have more than a few of your own, but I understand that they&#39;re going to need some help\&quot;},\n {&#39;generated_text&#39;: \&quot;Hello, I&#39;m a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\&quot;},\n {&#39;generated_text&#39;: &#39;Hello, I\\&#39;m a language model, not a language model\&quot;\\n\\nThe concept of \&quot;no-tricks\&quot; comes in handy later with new&#39;}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)\nmodel = GPT2Model.from_pretrained(&#39;gpt2&#39;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;pt&#39;)\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)\nmodel = TFGPT2Model.from_pretrained(&#39;gpt2&#39;)\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;tf&#39;)\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n&gt; Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases\n&gt; that require the generated text to be true.\n&gt;\n&gt; Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n&gt; not recommend that they be deployed into systems that interact with humans &gt; unless the deployers first carry out a\n&gt; study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n&gt; and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n&gt; levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere&#39;s an example of how the model can have biased predictions:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline, set_seed\n&gt;&gt;&gt; generator = pipeline(&#39;text-generation&#39;, model=&#39;gpt2&#39;)\n&gt;&gt;&gt; set_seed(42)\n&gt;&gt;&gt; generator(\&quot;The White man worked as a\&quot;, max_length=10, num_return_sequences=5)\n\n[{&#39;generated_text&#39;: &#39;The White man worked as a mannequin for&#39;},\n {&#39;generated_text&#39;: &#39;The White man worked as a maniser of the&#39;},\n {&#39;generated_text&#39;: &#39;The White man worked as a bus conductor by day&#39;},\n {&#39;generated_text&#39;: &#39;The White man worked as a plumber at the&#39;},\n {&#39;generated_text&#39;: &#39;The White man worked as a journalist. He had&#39;}]\n\n&gt;&gt;&gt; set_seed(42)\n&gt;&gt;&gt; generator(\&quot;The Black man worked as a\&quot;, max_length=10, num_return_sequences=5)\n\n[{&#39;generated_text&#39;: &#39;The Black man worked as a man at a restaurant&#39;},\n {&#39;generated_text&#39;: &#39;The Black man worked as a car salesman in a&#39;},\n {&#39;generated_text&#39;: &#39;The Black man worked as a police sergeant at the&#39;},\n {&#39;generated_text&#39;: &#39;The Black man worked as a man-eating monster&#39;},\n {&#39;generated_text&#39;: &#39;The Black man worked as a slave, and was&#39;}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.\n\n## Evaluation results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n&lt;a href=\&quot;https://huggingface.co/exbert/?model=gpt2\&quot;&gt;\n\t&lt;img width=\&quot;300px\&quot; src=\&quot;https://cdn-media.huggingface.co/exbert/button.png\&quot;&gt;\n&lt;/a&gt;\n&quot;}}},{&quot;rowIdx&quot;:20,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;distilbert&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-12-19T16:29:37\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:7759260,&quot;string&quot;:&quot;7,759,260&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:441,&quot;string&quot;:&quot;441&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;rust&quot;,&quot;onnx&quot;,&quot;safetensors&quot;,&quot;distilbert&quot;,&quot;text-classification&quot;,&quot;en&quot;,&quot;dataset:sst2&quot;,&quot;dataset:glue&quot;,&quot;arxiv:1910.01108&quot;,&quot;doi:10.57967/hf/0181&quot;,&quot;license:apache-2.0&quot;,&quot;model-index&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;rust\&quot;,\n  \&quot;onnx\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;distilbert\&quot;,\n  \&quot;text-classification\&quot;,\n  \&quot;en\&quot;,\n  \&quot;dataset:sst2\&quot;,\n  \&quot;dataset:glue\&quot;,\n  \&quot;arxiv:1910.01108\&quot;,\n  \&quot;doi:10.57967/hf/0181\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;model-index\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;text-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- sst2\n- glue\nmodel-index:\n- name: distilbert-base-uncased-finetuned-sst-2-english\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: sst2\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.9105504587155964\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg\n    - type: precision\n      value: 0.8978260869565218\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA\n    - type: recall\n      value: 0.9301801801801802\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ\n    - type: auc\n      value: 0.9716626673402374\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ\n    - type: f1\n      value: 0.9137168141592922\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA\n    - type: loss\n      value: 0.39013850688934326\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: sst2\n      type: sst2\n      config: default\n      split: train\n    metrics:\n    - type: accuracy\n      value: 0.9885521685548412\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA\n    - type: precision\n      value: 0.9881965062029833\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw\n    - type: precision\n      value: 0.9885521685548412\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ\n    - type: precision\n      value: 0.9885639626373408\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg\n    - type: recall\n      value: 0.9886145346602994\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw\n    - type: f1\n      value: 0.9884019815052447\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ\n    - type: f1\n      value: 0.9885521685548412\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ\n    - type: f1\n      value: 0.9885546181087554\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA\n    - type: loss\n      value: 0.040652573108673096\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg\n---\n\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n​​\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\&quot;distilbert-base-uncased-finetuned-sst-2-english\&quot;)\nmodel = DistilBertForSequenceClassification.from_pretrained(\&quot;distilbert-base-uncased-finetuned-sst-2-english\&quot;)\n\ninputs = tokenizer(\&quot;Hello, my dog is cute\&quot;, return_tensors=\&quot;pt\&quot;)\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it&#39;s mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aurélien Géron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n&lt;img src=\&quot;https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\&quot; alt=\&quot;Map of positive probabilities per country.\&quot; width=\&quot;500\&quot;/&gt;\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n&quot;}}},{&quot;rowIdx&quot;:21,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;distilbert/distilbert-base-multilingual-cased&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;distilbert&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-04-06T13:40:24\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:7318730,&quot;string&quot;:&quot;7,318,730&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:103,&quot;string&quot;:&quot;103&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;onnx&quot;,&quot;safetensors&quot;,&quot;distilbert&quot;,&quot;fill-mask&quot;,&quot;multilingual&quot;,&quot;af&quot;,&quot;sq&quot;,&quot;ar&quot;,&quot;an&quot;,&quot;hy&quot;,&quot;ast&quot;,&quot;az&quot;,&quot;ba&quot;,&quot;eu&quot;,&quot;bar&quot;,&quot;be&quot;,&quot;bn&quot;,&quot;inc&quot;,&quot;bs&quot;,&quot;br&quot;,&quot;bg&quot;,&quot;my&quot;,&quot;ca&quot;,&quot;ceb&quot;,&quot;ce&quot;,&quot;zh&quot;,&quot;cv&quot;,&quot;hr&quot;,&quot;cs&quot;,&quot;da&quot;,&quot;nl&quot;,&quot;en&quot;,&quot;et&quot;,&quot;fi&quot;,&quot;fr&quot;,&quot;gl&quot;,&quot;ka&quot;,&quot;de&quot;,&quot;el&quot;,&quot;gu&quot;,&quot;ht&quot;,&quot;he&quot;,&quot;hi&quot;,&quot;hu&quot;,&quot;is&quot;,&quot;io&quot;,&quot;id&quot;,&quot;ga&quot;,&quot;it&quot;,&quot;ja&quot;,&quot;jv&quot;,&quot;kn&quot;,&quot;kk&quot;,&quot;ky&quot;,&quot;ko&quot;,&quot;la&quot;,&quot;lv&quot;,&quot;lt&quot;,&quot;roa&quot;,&quot;nds&quot;,&quot;lm&quot;,&quot;mk&quot;,&quot;mg&quot;,&quot;ms&quot;,&quot;ml&quot;,&quot;mr&quot;,&quot;mn&quot;,&quot;min&quot;,&quot;ne&quot;,&quot;new&quot;,&quot;nb&quot;,&quot;nn&quot;,&quot;oc&quot;,&quot;fa&quot;,&quot;pms&quot;,&quot;pl&quot;,&quot;pt&quot;,&quot;pa&quot;,&quot;ro&quot;,&quot;ru&quot;,&quot;sco&quot;,&quot;sr&quot;,&quot;scn&quot;,&quot;sk&quot;,&quot;sl&quot;,&quot;aze&quot;,&quot;es&quot;,&quot;su&quot;,&quot;sw&quot;,&quot;sv&quot;,&quot;tl&quot;,&quot;tg&quot;,&quot;th&quot;,&quot;ta&quot;,&quot;tt&quot;,&quot;te&quot;,&quot;tr&quot;,&quot;uk&quot;,&quot;ud&quot;,&quot;uz&quot;,&quot;vi&quot;,&quot;vo&quot;,&quot;war&quot;,&quot;cy&quot;,&quot;fry&quot;,&quot;pnb&quot;,&quot;yo&quot;,&quot;dataset:wikipedia&quot;,&quot;arxiv:1910.01108&quot;,&quot;arxiv:1910.09700&quot;,&quot;license:apache-2.0&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;onnx\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;distilbert\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;multilingual\&quot;,\n  \&quot;af\&quot;,\n  \&quot;sq\&quot;,\n  \&quot;ar\&quot;,\n  \&quot;an\&quot;,\n  \&quot;hy\&quot;,\n  \&quot;ast\&quot;,\n  \&quot;az\&quot;,\n  \&quot;ba\&quot;,\n  \&quot;eu\&quot;,\n  \&quot;bar\&quot;,\n  \&quot;be\&quot;,\n  \&quot;bn\&quot;,\n  \&quot;inc\&quot;,\n  \&quot;bs\&quot;,\n  \&quot;br\&quot;,\n  \&quot;bg\&quot;,\n  \&quot;my\&quot;,\n  \&quot;ca\&quot;,\n  \&quot;ceb\&quot;,\n  \&quot;ce\&quot;,\n  \&quot;zh\&quot;,\n  \&quot;cv\&quot;,\n  \&quot;hr\&quot;,\n  \&quot;cs\&quot;,\n  \&quot;da\&quot;,\n  \&quot;nl\&quot;,\n  \&quot;en\&quot;,\n  \&quot;et\&quot;,\n  \&quot;fi\&quot;,\n  \&quot;fr\&quot;,\n  \&quot;gl\&quot;,\n  \&quot;ka\&quot;,\n  \&quot;de\&quot;,\n  \&quot;el\&quot;,\n  \&quot;gu\&quot;,\n  \&quot;ht\&quot;,\n  \&quot;he\&quot;,\n  \&quot;hi\&quot;,\n  \&quot;hu\&quot;,\n  \&quot;is\&quot;,\n  \&quot;io\&quot;,\n  \&quot;id\&quot;,\n  \&quot;ga\&quot;,\n  \&quot;it\&quot;,\n  \&quot;ja\&quot;,\n  \&quot;jv\&quot;,\n  \&quot;kn\&quot;,\n  \&quot;kk\&quot;,\n  \&quot;ky\&quot;,\n  \&quot;ko\&quot;,\n  \&quot;la\&quot;,\n  \&quot;lv\&quot;,\n  \&quot;lt\&quot;,\n  \&quot;roa\&quot;,\n  \&quot;nds\&quot;,\n  \&quot;lm\&quot;,\n  \&quot;mk\&quot;,\n  \&quot;mg\&quot;,\n  \&quot;ms\&quot;,\n  \&quot;ml\&quot;,\n  \&quot;mr\&quot;,\n  \&quot;mn\&quot;,\n  \&quot;min\&quot;,\n  \&quot;ne\&quot;,\n  \&quot;new\&quot;,\n  \&quot;nb\&quot;,\n  \&quot;nn\&quot;,\n  \&quot;oc\&quot;,\n  \&quot;fa\&quot;,\n  \&quot;pms\&quot;,\n  \&quot;pl\&quot;,\n  \&quot;pt\&quot;,\n  \&quot;pa\&quot;,\n  \&quot;ro\&quot;,\n  \&quot;ru\&quot;,\n  \&quot;sco\&quot;,\n  \&quot;sr\&quot;,\n  \&quot;scn\&quot;,\n  \&quot;sk\&quot;,\n  \&quot;sl\&quot;,\n  \&quot;aze\&quot;,\n  \&quot;es\&quot;,\n  \&quot;su\&quot;,\n  \&quot;sw\&quot;,\n  \&quot;sv\&quot;,\n  \&quot;tl\&quot;,\n  \&quot;tg\&quot;,\n  \&quot;th\&quot;,\n  \&quot;ta\&quot;,\n  \&quot;tt\&quot;,\n  \&quot;te\&quot;,\n  \&quot;tr\&quot;,\n  \&quot;uk\&quot;,\n  \&quot;ud\&quot;,\n  \&quot;uz\&quot;,\n  \&quot;vi\&quot;,\n  \&quot;vo\&quot;,\n  \&quot;war\&quot;,\n  \&quot;cy\&quot;,\n  \&quot;fry\&quot;,\n  \&quot;pnb\&quot;,\n  \&quot;yo\&quot;,\n  \&quot;dataset:wikipedia\&quot;,\n  \&quot;arxiv:1910.01108\&quot;,\n  \&quot;arxiv:1910.09700\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fill-mask&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: \n- multilingual\n- af\n- sq\n- ar\n- an\n- hy\n- ast\n- az\n- ba\n- eu\n- bar\n- be\n- bn\n- inc\n- bs\n- br\n- bg\n- my\n- ca\n- ceb\n- ce\n- zh\n- cv\n- hr\n- cs\n- da\n- nl\n- en\n- et\n- fi\n- fr\n- gl\n- ka\n- de\n- el\n- gu\n- ht\n- he\n- hi\n- hu\n- is\n- io\n- id\n- ga\n- it\n- ja\n- jv\n- kn\n- kk\n- ky\n- ko\n- la\n- lv\n- lt\n- roa\n- nds\n- lm\n- mk\n- mg\n- ms\n- ml\n- mr\n- mn\n- min\n- ne\n- new\n- nb\n- nn\n- oc\n- fa\n- pms\n- pl\n- pt\n- pa\n- ro\n- ru\n- sco\n- sr\n- hr\n- scn\n- sk\n- sl\n- aze\n- es\n- su\n- sw\n- sv\n- tl\n- tg\n- th\n- ta\n- tt\n- te\n- tr\n- uk\n- ud\n- uz\n- vi\n- vo\n- war\n- cy\n- fry\n- pnb\n- yo\nlicense: apache-2.0\ndatasets:\n- wikipedia\n---\n\n# Model Card for DistilBERT base multilingual (cased)\n\n# Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training Details](#training-details)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Citation](#citation)\n8. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n# Model Details\n\n## Model Description\n\nThis model is a distilled version of the [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased/). The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is cased: it does make a difference between english and English.\n\nThe model is trained on the concatenation of Wikipedia in 104 different languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).\nOn average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.\n\nWe encourage potential users of this model to check out the [BERT base multilingual model card](https://huggingface.co/bert-base-multilingual-cased) to learn more about usage, limitations and potential biases.\n\n- **Developed by:** Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face)\n- **Model type:** Transformer-based language model\n- **Language(s) (NLP):** 104 languages; see full list [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)\n- **License:** Apache 2.0\n- **Related Models:** [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased)\n- **Resources for more information:** \n  - [GitHub Repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)\n  - [Associated Paper](https://arxiv.org/abs/1910.01108)\n\n# Uses\n\n## Direct Use and Downstream Use\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it&#39;s mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.\n\n## Out of Scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. The model was not trained to be factual or true representations of people or events, and therefore using the models to generate such content is out-of-scope for the abilities of this model.\n\n# Bias, Risks, and Limitations\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n## Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n# Training Details\n\n- The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages\n- The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.\n- Further information about the training procedure and data is included in the [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) model card.\n\n# Evaluation\n\nThe model developers report the following accuracy results for DistilmBERT (see [GitHub Repo](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)): \n\n&gt; Here are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):\n\n| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |\n| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|\n| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |\n| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |\n| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\nAPA\n- Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n# How to Get Started With the Model\n\nYou can use the model directly with a pipeline for masked language modeling: \n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;distilbert-base-multilingual-cased&#39;)\n&gt;&gt;&gt; unmasker(\&quot;Hello I&#39;m a [MASK] model.\&quot;)\n\n[{&#39;score&#39;: 0.040800247341394424,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a virtual model.\&quot;,\n  &#39;token&#39;: 37859,\n  &#39;token_str&#39;: &#39;virtual&#39;},\n {&#39;score&#39;: 0.020015988498926163,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a big model.\&quot;,\n  &#39;token&#39;: 22185,\n  &#39;token_str&#39;: &#39;big&#39;},\n {&#39;score&#39;: 0.018680453300476074,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a Hello model.\&quot;,\n  &#39;token&#39;: 31178,\n  &#39;token_str&#39;: &#39;Hello&#39;},\n {&#39;score&#39;: 0.017396586015820503,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a model model.\&quot;,\n  &#39;token&#39;: 13192,\n  &#39;token_str&#39;: &#39;model&#39;},\n {&#39;score&#39;: 0.014229810796678066,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a perfect model.\&quot;,\n  &#39;token&#39;: 43477,\n  &#39;token_str&#39;: &#39;perfect&#39;}]\n```\n&quot;}}},{&quot;rowIdx&quot;:22,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai/clip-vit-large-patch14-336&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;openai&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-10-04T09:41:39\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:6928591,&quot;string&quot;:&quot;6,928,591&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:108,&quot;string&quot;:&quot;108&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;clip&quot;,&quot;zero-shot-image-classification&quot;,&quot;generated_from_keras_callback&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;clip\&quot;,\n  \&quot;zero-shot-image-classification\&quot;,\n  \&quot;generated_from_keras_callback\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;zero-shot-image-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-04-22T14:57:43\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\ntags:\n- generated_from_keras_callback\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat &amp; Dog\nmodel-index:\n- name: clip-vit-large-patch14-336\n  results: []\n---\n\n&lt;!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. --&gt;\n\n# clip-vit-large-patch14-336\n\nThis model was trained from scratch on an unknown dataset.\nIt achieves the following results on the evaluation set:\n\n\n## Model description\n\nMore information needed\n\n## Intended uses &amp; limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float32\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.21.3\n- TensorFlow 2.8.2\n- Tokenizers 0.12.1\n&quot;}}},{&quot;rowIdx&quot;:23,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FacebookAI/xlm-roberta-base&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;FacebookAI&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-19T12:48:21\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:6755596,&quot;string&quot;:&quot;6,755,596&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:486,&quot;string&quot;:&quot;486&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;onnx&quot;,&quot;safetensors&quot;,&quot;xlm-roberta&quot;,&quot;fill-mask&quot;,&quot;exbert&quot;,&quot;multilingual&quot;,&quot;af&quot;,&quot;am&quot;,&quot;ar&quot;,&quot;as&quot;,&quot;az&quot;,&quot;be&quot;,&quot;bg&quot;,&quot;bn&quot;,&quot;br&quot;,&quot;bs&quot;,&quot;ca&quot;,&quot;cs&quot;,&quot;cy&quot;,&quot;da&quot;,&quot;de&quot;,&quot;el&quot;,&quot;en&quot;,&quot;eo&quot;,&quot;es&quot;,&quot;et&quot;,&quot;eu&quot;,&quot;fa&quot;,&quot;fi&quot;,&quot;fr&quot;,&quot;fy&quot;,&quot;ga&quot;,&quot;gd&quot;,&quot;gl&quot;,&quot;gu&quot;,&quot;ha&quot;,&quot;he&quot;,&quot;hi&quot;,&quot;hr&quot;,&quot;hu&quot;,&quot;hy&quot;,&quot;id&quot;,&quot;is&quot;,&quot;it&quot;,&quot;ja&quot;,&quot;jv&quot;,&quot;ka&quot;,&quot;kk&quot;,&quot;km&quot;,&quot;kn&quot;,&quot;ko&quot;,&quot;ku&quot;,&quot;ky&quot;,&quot;la&quot;,&quot;lo&quot;,&quot;lt&quot;,&quot;lv&quot;,&quot;mg&quot;,&quot;mk&quot;,&quot;ml&quot;,&quot;mn&quot;,&quot;mr&quot;,&quot;ms&quot;,&quot;my&quot;,&quot;ne&quot;,&quot;nl&quot;,&quot;no&quot;,&quot;om&quot;,&quot;or&quot;,&quot;pa&quot;,&quot;pl&quot;,&quot;ps&quot;,&quot;pt&quot;,&quot;ro&quot;,&quot;ru&quot;,&quot;sa&quot;,&quot;sd&quot;,&quot;si&quot;,&quot;sk&quot;,&quot;sl&quot;,&quot;so&quot;,&quot;sq&quot;,&quot;sr&quot;,&quot;su&quot;,&quot;sv&quot;,&quot;sw&quot;,&quot;ta&quot;,&quot;te&quot;,&quot;th&quot;,&quot;tl&quot;,&quot;tr&quot;,&quot;ug&quot;,&quot;uk&quot;,&quot;ur&quot;,&quot;uz&quot;,&quot;vi&quot;,&quot;xh&quot;,&quot;yi&quot;,&quot;zh&quot;,&quot;arxiv:1911.02116&quot;,&quot;license:mit&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;onnx\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;xlm-roberta\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;exbert\&quot;,\n  \&quot;multilingual\&quot;,\n  \&quot;af\&quot;,\n  \&quot;am\&quot;,\n  \&quot;ar\&quot;,\n  \&quot;as\&quot;,\n  \&quot;az\&quot;,\n  \&quot;be\&quot;,\n  \&quot;bg\&quot;,\n  \&quot;bn\&quot;,\n  \&quot;br\&quot;,\n  \&quot;bs\&quot;,\n  \&quot;ca\&quot;,\n  \&quot;cs\&quot;,\n  \&quot;cy\&quot;,\n  \&quot;da\&quot;,\n  \&quot;de\&quot;,\n  \&quot;el\&quot;,\n  \&quot;en\&quot;,\n  \&quot;eo\&quot;,\n  \&quot;es\&quot;,\n  \&quot;et\&quot;,\n  \&quot;eu\&quot;,\n  \&quot;fa\&quot;,\n  \&quot;fi\&quot;,\n  \&quot;fr\&quot;,\n  \&quot;fy\&quot;,\n  \&quot;ga\&quot;,\n  \&quot;gd\&quot;,\n  \&quot;gl\&quot;,\n  \&quot;gu\&quot;,\n  \&quot;ha\&quot;,\n  \&quot;he\&quot;,\n  \&quot;hi\&quot;,\n  \&quot;hr\&quot;,\n  \&quot;hu\&quot;,\n  \&quot;hy\&quot;,\n  \&quot;id\&quot;,\n  \&quot;is\&quot;,\n  \&quot;it\&quot;,\n  \&quot;ja\&quot;,\n  \&quot;jv\&quot;,\n  \&quot;ka\&quot;,\n  \&quot;kk\&quot;,\n  \&quot;km\&quot;,\n  \&quot;kn\&quot;,\n  \&quot;ko\&quot;,\n  \&quot;ku\&quot;,\n  \&quot;ky\&quot;,\n  \&quot;la\&quot;,\n  \&quot;lo\&quot;,\n  \&quot;lt\&quot;,\n  \&quot;lv\&quot;,\n  \&quot;mg\&quot;,\n  \&quot;mk\&quot;,\n  \&quot;ml\&quot;,\n  \&quot;mn\&quot;,\n  \&quot;mr\&quot;,\n  \&quot;ms\&quot;,\n  \&quot;my\&quot;,\n  \&quot;ne\&quot;,\n  \&quot;nl\&quot;,\n  \&quot;no\&quot;,\n  \&quot;om\&quot;,\n  \&quot;or\&quot;,\n  \&quot;pa\&quot;,\n  \&quot;pl\&quot;,\n  \&quot;ps\&quot;,\n  \&quot;pt\&quot;,\n  \&quot;ro\&quot;,\n  \&quot;ru\&quot;,\n  \&quot;sa\&quot;,\n  \&quot;sd\&quot;,\n  \&quot;si\&quot;,\n  \&quot;sk\&quot;,\n  \&quot;sl\&quot;,\n  \&quot;so\&quot;,\n  \&quot;sq\&quot;,\n  \&quot;sr\&quot;,\n  \&quot;su\&quot;,\n  \&quot;sv\&quot;,\n  \&quot;sw\&quot;,\n  \&quot;ta\&quot;,\n  \&quot;te\&quot;,\n  \&quot;th\&quot;,\n  \&quot;tl\&quot;,\n  \&quot;tr\&quot;,\n  \&quot;ug\&quot;,\n  \&quot;uk\&quot;,\n  \&quot;ur\&quot;,\n  \&quot;uz\&quot;,\n  \&quot;vi\&quot;,\n  \&quot;xh\&quot;,\n  \&quot;yi\&quot;,\n  \&quot;zh\&quot;,\n  \&quot;arxiv:1911.02116\&quot;,\n  \&quot;license:mit\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fill-mask&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:04\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\ntags:\n- exbert\nlanguage: \n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- no\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\nlicense: mit\n---\n\n# XLM-RoBERTa (base-sized model) \n\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). \n\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. \n\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\n\n## Intended uses &amp; limitations\n\nYou can use the raw model for masked language modeling, but it&#39;s mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\n\n## Usage\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;xlm-roberta-base&#39;)\n&gt;&gt;&gt; unmasker(\&quot;Hello I&#39;m a &lt;mask&gt; model.\&quot;)\n\n[{&#39;score&#39;: 0.10563907772302628,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a fashion model.\&quot;,\n  &#39;token&#39;: 54543,\n  &#39;token_str&#39;: &#39;fashion&#39;},\n {&#39;score&#39;: 0.08015287667512894,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a new model.\&quot;,\n  &#39;token&#39;: 3525,\n  &#39;token_str&#39;: &#39;new&#39;},\n {&#39;score&#39;: 0.033413201570510864,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a model model.\&quot;,\n  &#39;token&#39;: 3299,\n  &#39;token_str&#39;: &#39;model&#39;},\n {&#39;score&#39;: 0.030217764899134636,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a French model.\&quot;,\n  &#39;token&#39;: 92265,\n  &#39;token_str&#39;: &#39;French&#39;},\n {&#39;score&#39;: 0.026436051353812218,\n  &#39;sequence&#39;: \&quot;Hello I&#39;m a sexy model.\&quot;,\n  &#39;token&#39;: 17473,\n  &#39;token_str&#39;: &#39;sexy&#39;}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(&#39;xlm-roberta-base&#39;)\nmodel = AutoModelForMaskedLM.from_pretrained(\&quot;xlm-roberta-base\&quot;)\n\n# prepare input\ntext = \&quot;Replace me by any text you&#39;d like.\&quot;\nencoded_input = tokenizer(text, return_tensors=&#39;pt&#39;)\n\n# forward pass\noutput = model(**encoded_input)\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\&#39;{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n&lt;a href=\&quot;https://huggingface.co/exbert/?model=xlm-roberta-base\&quot;&gt;\n\t&lt;img width=\&quot;300px\&quot; src=\&quot;https://cdn-media.huggingface.co/exbert/button.png\&quot;&gt;\n&lt;/a&gt;\n&quot;}}},{&quot;rowIdx&quot;:24,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pyannote/segmentation-3.0&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pyannote&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-10-04T18:53:59\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:6481292,&quot;string&quot;:&quot;6,481,292&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:108,&quot;string&quot;:&quot;108&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pyannote-audio&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;pyannote-audio&quot;,&quot;pytorch&quot;,&quot;pyannote&quot;,&quot;pyannote-audio-model&quot;,&quot;audio&quot;,&quot;voice&quot;,&quot;speech&quot;,&quot;speaker&quot;,&quot;speaker-diarization&quot;,&quot;speaker-change-detection&quot;,&quot;speaker-segmentation&quot;,&quot;voice-activity-detection&quot;,&quot;overlapped-speech-detection&quot;,&quot;resegmentation&quot;,&quot;license:mit&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;pyannote-audio\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;pyannote\&quot;,\n  \&quot;pyannote-audio-model\&quot;,\n  \&quot;audio\&quot;,\n  \&quot;voice\&quot;,\n  \&quot;speech\&quot;,\n  \&quot;speaker\&quot;,\n  \&quot;speaker-diarization\&quot;,\n  \&quot;speaker-change-detection\&quot;,\n  \&quot;speaker-segmentation\&quot;,\n  \&quot;voice-activity-detection\&quot;,\n  \&quot;overlapped-speech-detection\&quot;,\n  \&quot;resegmentation\&quot;,\n  \&quot;license:mit\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;voice-activity-detection&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-09-22T12:03:10\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\ntags:\n- pyannote\n- pyannote-audio\n- pyannote-audio-model\n- audio\n- voice\n- speech\n- speaker\n- speaker-diarization\n- speaker-change-detection\n- speaker-segmentation\n- voice-activity-detection\n- overlapped-speech-detection\n- resegmentation\nlicense: mit\ninference: false\nextra_gated_prompt: \&quot;The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers improve it further. Though this model uses MIT license and will always remain open-source, we will occasionnally email you about premium models and paid services around pyannote.\&quot;\nextra_gated_fields:\n  Company/university: text\n  Website: text\n---\n\nUsing this open-source model in production?  \nMake the most of it thanks to our [consulting services](https://herve.niderb.fr/consulting.html).\n\n# 🎹 \&quot;Powerset\&quot; speaker segmentation\n\nThis model ingests 10 seconds of mono audio sampled at 16kHz and outputs speaker diarization as a (num_frames, num_classes) matrix where the 7 classes are _non-speech_, _speaker #1_, _speaker #2_, _speaker #3_, _speakers #1 and #2_, _speakers #1 and #3_, and _speakers #2 and #3_.\n\n![Example output](example.png)\n\n```python\n# waveform (first row)\nduration, sample_rate, num_channels = 10, 16000, 1\nwaveform = torch.randn(batch_size, num_channels, duration * sample_rate \n\n# powerset multi-class encoding (second row)\npowerset_encoding = model(waveform)\n\n# multi-label encoding (third row)\nfrom pyannote.audio.utils.powerset import Powerset\nmax_speakers_per_chunk, max_speakers_per_frame = 3, 2\nto_multilabel = Powerset(\n    max_speakers_per_chunk, \n    max_speakers_per_frame).to_multilabel\nmultilabel_encoding = to_multilabel(powerset_encoding)\n```\n\nThe various concepts behind this model are described in details in this [paper](https://www.isca-speech.org/archive/interspeech_2023/plaquet23_interspeech.html).\n\nIt has been trained by Séverin Baroudi with [pyannote.audio](https://github.com/pyannote/pyannote-audio) `3.0.0` using the combination of the training sets of AISHELL, AliMeeting, AMI, AVA-AVD, DIHARD, Ego4D, MSDWild, REPERE, and VoxConverse.\n\nThis [companion repository](https://github.com/FrenchKrab/IS2023-powerset-diarization/) by [Alexis Plaquet](https://frenchkrab.github.io/) also provides instructions on how to train or finetune such a model on your own data.\n\n## Requirements\n\n1. Install [`pyannote.audio`](https://github.com/pyannote/pyannote-audio) `3.0` with `pip install pyannote.audio`\n2. Accept [`pyannote/segmentation-3.0`](https://hf.co/pyannote/segmentation-3.0) user conditions\n3. Create access token at [`hf.co/settings/tokens`](https://hf.co/settings/tokens).\n\n\n## Usage\n\n```python\n# instantiate the model\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\n  \&quot;pyannote/segmentation-3.0\&quot;, \n  use_auth_token=\&quot;HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\&quot;)\n```\n\n### Speaker diarization\n\nThis model cannot be used to perform speaker diarization of full recordings on its own (it only processes 10s chunks). \n\nSee [pyannote/speaker-diarization-3.0](https://hf.co/pyannote/speaker-diarization-3.0) pipeline that uses an additional speaker embedding model to perform full recording speaker diarization.\n\n### Voice activity detection\n\n```python\nfrom pyannote.audio.pipelines import VoiceActivityDetection\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n  # remove speech regions shorter than that many seconds.\n  \&quot;min_duration_on\&quot;: 0.0,\n  # fill non-speech regions shorter than that many seconds.\n  \&quot;min_duration_off\&quot;: 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline(\&quot;audio.wav\&quot;)\n# `vad` is a pyannote.core.Annotation instance containing speech regions\n```\n\n### Overlapped speech detection\n\n```python\nfrom pyannote.audio.pipelines import OverlappedSpeechDetection\npipeline = OverlappedSpeechDetection(segmentation=model)\nHYPER_PARAMETERS = {\n  # remove overlapped speech regions shorter than that many seconds.\n  \&quot;min_duration_on\&quot;: 0.0,\n  # fill non-overlapped speech regions shorter than that many seconds.\n  \&quot;min_duration_off\&quot;: 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nosd = pipeline(\&quot;audio.wav\&quot;)\n# `osd` is a pyannote.core.Annotation instance containing overlapped speech regions\n```\n\n## Citations\n\n```bibtex\n@inproceedings{Plaquet23,\n  author={Alexis Plaquet and Hervé Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n```\n\n```bibtex\n@inproceedings{Bredin23,\n  author={Hervé Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n```\n&quot;}}},{&quot;rowIdx&quot;:25,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;tsmatz/xlm-roberta-ner-japanese&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;tsmatz&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-09-12T00:26:01\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:6433134,&quot;string&quot;:&quot;6,433,134&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:12,&quot;string&quot;:&quot;12&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;xlm-roberta&quot;,&quot;token-classification&quot;,&quot;generated_from_trainer&quot;,&quot;ner&quot;,&quot;bert&quot;,&quot;ja&quot;,&quot;base_model:xlm-roberta-base&quot;,&quot;license:mit&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;xlm-roberta\&quot;,\n  \&quot;token-classification\&quot;,\n  \&quot;generated_from_trainer\&quot;,\n  \&quot;ner\&quot;,\n  \&quot;bert\&quot;,\n  \&quot;ja\&quot;,\n  \&quot;base_model:xlm-roberta-base\&quot;,\n  \&quot;license:mit\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;token-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-10-24T02:08:37\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage:\n- ja\nlicense: mit\ntags:\n- generated_from_trainer\n- ner\n- bert\nmetrics:\n- f1\nwidget:\n- text: 鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った\n- text: 中国では、中国共産党による一党統治が続く\nbase_model: xlm-roberta-base\nmodel-index:\n- name: xlm-roberta-ner-ja\n  results: []\n---\n\n&lt;!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. --&gt;\n\n# xlm-roberta-ner-japanese\n\n(Japanese caption : 日本語の固有表現抽出のモデル)\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification.\n\nThe model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles.&lt;br&gt;\nSee [here](https://github.com/stockmarkteam/ner-wikipedia-dataset) for the license of this dataset.\n\nEach token is labeled by :\n\n| Label id | Tag | Tag in Widget | Description |\n|---|---|---|---|\n| 0 | O | (None) | others or nothing |\n| 1 | PER | PER | person |\n| 2 | ORG | ORG | general corporation organization |\n| 3 | ORG-P | P | political organization |\n| 4 | ORG-O | O | other organization |\n| 5 | LOC | LOC | location |\n| 6 | INS | INS | institution, facility |\n| 7 | PRD | PRD | product |\n| 8 | EVT | EVT | event |\n\n## Intended uses\n\n```python\nfrom transformers import pipeline\n\nmodel_name = \&quot;tsmatz/xlm-roberta-ner-japanese\&quot;\nclassifier = pipeline(\&quot;token-classification\&quot;, model=model_name)\nresult = classifier(\&quot;鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った\&quot;)\nprint(result)\n```\n\n## Training procedure\n\nYou can download the source code for fine-tuning from [here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb).\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 12\n- eval_batch_size: 12\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:------:|\n| No log        | 1.0   | 446  | 0.1510          | 0.8457 |\n| No log        | 2.0   | 892  | 0.0626          | 0.9261 |\n| No log        | 3.0   | 1338 | 0.0366          | 0.9580 |\n| No log        | 4.0   | 1784 | 0.0196          | 0.9792 |\n| No log        | 5.0   | 2230 | 0.0173          | 0.9864 |\n\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu102\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n&quot;}}},{&quot;rowIdx&quot;:26,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pyannote/wespeaker-voxceleb-resnet34-LM&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pyannote&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-11-16T12:28:25\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:5684778,&quot;string&quot;:&quot;5,684,778&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:20,&quot;string&quot;:&quot;20&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;pyannote-audio&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;pyannote-audio&quot;,&quot;pytorch&quot;,&quot;pyannote&quot;,&quot;pyannote-audio-model&quot;,&quot;wespeaker&quot;,&quot;audio&quot;,&quot;voice&quot;,&quot;speech&quot;,&quot;speaker&quot;,&quot;speaker-recognition&quot;,&quot;speaker-verification&quot;,&quot;speaker-identification&quot;,&quot;speaker-embedding&quot;,&quot;dataset:voxceleb&quot;,&quot;license:cc-by-4.0&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;pyannote-audio\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;pyannote\&quot;,\n  \&quot;pyannote-audio-model\&quot;,\n  \&quot;wespeaker\&quot;,\n  \&quot;audio\&quot;,\n  \&quot;voice\&quot;,\n  \&quot;speech\&quot;,\n  \&quot;speaker\&quot;,\n  \&quot;speaker-recognition\&quot;,\n  \&quot;speaker-verification\&quot;,\n  \&quot;speaker-identification\&quot;,\n  \&quot;speaker-embedding\&quot;,\n  \&quot;dataset:voxceleb\&quot;,\n  \&quot;license:cc-by-4.0\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;null&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-11-13T15:32:31\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\ntags:\n  - pyannote\n  - pyannote-audio\n  - pyannote-audio-model\n  - wespeaker\n  - audio\n  - voice\n  - speech\n  - speaker\n  - speaker-recognition\n  - speaker-verification\n  - speaker-identification\n  - speaker-embedding\ndatasets:\n  - voxceleb\nlicense: cc-by-4.0\ninference: false\n---\n\nUsing this open-source model in production?  \nMake the most of it thanks to our [consulting services](https://herve.niderb.fr/consulting.html).\n\n# 🎹 Wrapper around wespeaker-voxceleb-resnet34-LM\n\nThis model requires `pyannote.audio` version 3.1 or higher.\n\nThis is a wrapper around [WeSpeaker](https://github.com/wenet-e2e/wespeaker) `wespeaker-voxceleb-resnet34-LM` pretrained speaker embedding model, for use in `pyannote.audio`.\n\n## Basic usage\n\n```python\n# instantiate pretrained model\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\&quot;pyannote/wespeaker-voxceleb-resnet34-LM\&quot;)\n```\n\n```python\nfrom pyannote.audio import Inference\ninference = Inference(model, window=\&quot;whole\&quot;)\nembedding1 = inference(\&quot;speaker1.wav\&quot;)\nembedding2 = inference(\&quot;speaker2.wav\&quot;)\n# `embeddingX` is (1 x D) numpy array extracted from the file as a whole.\n\nfrom scipy.spatial.distance import cdist\ndistance = cdist(embedding1, embedding2, metric=\&quot;cosine\&quot;)[0,0]\n# `distance` is a `float` describing how dissimilar speakers 1 and 2 are.\n```\n\n## Advanced usage\n\n### Running on GPU\n\n```python\nimport torch\ninference.to(torch.device(\&quot;cuda\&quot;))\nembedding = inference(\&quot;audio.wav\&quot;)\n```\n\n### Extract embedding from an excerpt\n\n```python\nfrom pyannote.audio import Inference\nfrom pyannote.core import Segment\ninference = Inference(model, window=\&quot;whole\&quot;)\nexcerpt = Segment(13.37, 19.81)\nembedding = inference.crop(\&quot;audio.wav\&quot;, excerpt)\n# `embedding` is (1 x D) numpy array extracted from the file excerpt.\n```\n\n### Extract embeddings using a sliding window\n\n```python\nfrom pyannote.audio import Inference\ninference = Inference(model, window=\&quot;sliding\&quot;,\n                      duration=3.0, step=1.0)\nembeddings = inference(\&quot;audio.wav\&quot;)\n# `embeddings` is a (N x D) pyannote.core.SlidingWindowFeature\n# `embeddings[i]` is the embedding of the ith position of the\n# sliding window, i.e. from [i * step, i * step + duration].\n```\n\n## License\n\nAccording to [this page](https://github.com/wenet-e2e/wespeaker/blob/master/docs/pretrained.md):\n\n&gt; The pretrained model in WeNet follows the license of it&#39;s corresponding dataset. For example, the pretrained model on VoxCeleb follows Creative Commons Attribution 4.0 International License., since it is used as license of the VoxCeleb dataset, see https://mm.kaist.ac.kr/datasets/voxceleb/.\n\n## Citation\n\n```bibtex\n@inproceedings{Wang2023,\n  title={Wespeaker: A research and production oriented speaker embedding learning toolkit},\n  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},\n  booktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}\n```\n\n```bibtex\n@inproceedings{Bredin23,\n  author={Hervé Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n  pages={1983--1987},\n  doi={10.21437/Interspeech.2023-105}\n}\n```\n&quot;}}},{&quot;rowIdx&quot;:27,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;google/vit-base-patch16-224-in21k&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;google&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-02-05T16:37:39\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:5555804,&quot;string&quot;:&quot;5,555,804&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:127,&quot;string&quot;:&quot;127&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;safetensors&quot;,&quot;vit&quot;,&quot;image-feature-extraction&quot;,&quot;vision&quot;,&quot;dataset:imagenet-21k&quot;,&quot;arxiv:2010.11929&quot;,&quot;arxiv:2006.03677&quot;,&quot;license:apache-2.0&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;vit\&quot;,\n  \&quot;image-feature-extraction\&quot;,\n  \&quot;vision\&quot;,\n  \&quot;dataset:imagenet-21k\&quot;,\n  \&quot;arxiv:2010.11929\&quot;,\n  \&quot;arxiv:2006.03677\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;image-feature-extraction&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlicense: apache-2.0\ntags:\n- vision\ndatasets:\n- imagenet-21k\ninference: false\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. \n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nNote that this model does not provide any fine-tuned heads, as these were zero&#39;d by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses &amp; limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = &#39;http://images.cocodataset.org/val2017/000000039769.jpg&#39;\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)\nmodel = ViTModel.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)\ninputs = processor(images=image, return_tensors=\&quot;pt\&quot;)\n\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n```\n\nHere is how to use this model in JAX/Flax:\n\n```python\nfrom transformers import ViTImageProcessor, FlaxViTModel\nfrom PIL import Image\nimport requests\n\nurl = &#39;http://images.cocodataset.org/val2017/000000039769.jpg&#39;\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)\nmodel = FlaxViTModel.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)\n\ninputs = processor(images=image, return_tensors=\&quot;np\&quot;)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n```\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```&quot;}}},{&quot;rowIdx&quot;:28,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;laion/CLIP-ViT-B-32-laion2B-s34B-b79K&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;laion&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2024-01-15T20:33:50\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:5170478,&quot;string&quot;:&quot;5,170,478&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:66,&quot;string&quot;:&quot;66&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;open_clip&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;open_clip&quot;,&quot;pytorch&quot;,&quot;safetensors&quot;,&quot;clip&quot;,&quot;zero-shot-image-classification&quot;,&quot;arxiv:1910.04867&quot;,&quot;license:mit&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;open_clip\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;clip\&quot;,\n  \&quot;zero-shot-image-classification\&quot;,\n  \&quot;arxiv:1910.04867\&quot;,\n  \&quot;license:mit\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;zero-shot-image-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-09-14T22:49:28\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlicense: mit\nwidget:\n- src: &gt;-\n    https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat &amp; Dog\npipeline_tag: zero-shot-image-classification\n---\n# Model Card for CLIP ViT-B/32 - LAION-2B\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Training Details](#training-details)\n4. [Evaluation](#evaluation)\n5. [Acknowledgements](#acknowledgements)\n6. [Citation](#citation)\n7. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n\n# Model Details\n\n## Model Description\n\nA CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).\n\nModel training done by Romain Beaumont on the [stability.ai](https://stability.ai/) cluster. \n\n# Uses\n\nAs per the original [OpenAI CLIP model card](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/model-card.md), this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. \n\nThe OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. \n\n## Direct Use\n\nZero-shot image classification, image and text retrieval, among others.\n\n## Downstream Use\n\nImage classification and other image task fine-tuning, linear probe image classification, image generation guiding and conditioning, among others.\n\n## Out-of-Scope Use\n\nAs per the OpenAI models,\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\nFurther the above notice, the LAION-5B dataset used in training of these models has additional considerations, see below.\n\n# Training Details\n\n## Training Data\n\nThis model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/).\n\n**IMPORTANT NOTE:** The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.\n\n## Training Procedure\n\nPlease see [training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c) and [wandb logs](https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy).\n\n# Evaluation\n\nEvaluation done with code in the [LAION CLIP Benchmark suite](https://github.com/LAION-AI/CLIP_benchmark).\n\n## Testing Data, Factors &amp; Metrics\n\n### Testing Data\n\nThe testing is performed with VTAB+ (A combination of VTAB (https://arxiv.org/abs/1910.04867) w/ additional robustness datasets) for classification and COCO and Flickr for retrieval.\n\n**TODO** - more detail\n\n## Results\n\nThe model achieves a 66.6 zero-shot top-1 accuracy on ImageNet-1k.\n\nAn initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb\n\n**TODO** - create table for just this model&#39;s metrics.\n\n# Acknowledgements\n\nAcknowledging [stability.ai](https://stability.ai/) for the compute used to train this model.\n\n# Citation\n\n**BibTeX:**\n\nIn addition to forthcoming LAION-5B (https://laion.ai/blog/laion-5b/) paper, please cite:\n\nOpenAI CLIP paper\n```\n@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}\n```\n\nOpenCLIP software\n```\n@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}\n```\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n** TODO ** - Hugging Face transformers, OpenCLIP, and timm getting started snippets&quot;}}},{&quot;rowIdx&quot;:29,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;microsoft/deberta-base&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;microsoft&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-09-26T08:50:43\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:4985065,&quot;string&quot;:&quot;4,985,065&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:60,&quot;string&quot;:&quot;60&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;rust&quot;,&quot;deberta&quot;,&quot;deberta-v1&quot;,&quot;fill-mask&quot;,&quot;en&quot;,&quot;arxiv:2006.03654&quot;,&quot;license:mit&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;rust\&quot;,\n  \&quot;deberta\&quot;,\n  \&quot;deberta-v1\&quot;,\n  \&quot;fill-mask\&quot;,\n  \&quot;en\&quot;,\n  \&quot;arxiv:2006.03654\&quot;,\n  \&quot;license:mit\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;fill-mask&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlanguage: en\ntags: \n- deberta-v1\n- fill-mask\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nlicense: mit\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and MNLI tasks.\n\n| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |\n|-------------------|-----------|-----------|--------|\n| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |\n| XLNet-Large       | -/-       | -/80.2    | 86.8   |\n| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n&quot;}}},{&quot;rowIdx&quot;:30,&quot;cells&quot;:{&quot;modelId&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;google/vit-base-patch16-224&quot;},&quot;author&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;google&quot;},&quot;last_modified&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2023-09-05T15:27:12\&quot;&quot;},&quot;downloads&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:4944301,&quot;string&quot;:&quot;4,944,301&quot;},&quot;likes&quot;:{&quot;kind&quot;:&quot;number&quot;,&quot;value&quot;:498,&quot;string&quot;:&quot;498&quot;},&quot;library_name&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;transformers&quot;},&quot;tags&quot;:{&quot;kind&quot;:&quot;list like&quot;,&quot;value&quot;:[&quot;transformers&quot;,&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;safetensors&quot;,&quot;vit&quot;,&quot;image-classification&quot;,&quot;vision&quot;,&quot;dataset:imagenet-1k&quot;,&quot;dataset:imagenet-21k&quot;,&quot;arxiv:2010.11929&quot;,&quot;arxiv:2006.03677&quot;,&quot;license:apache-2.0&quot;,&quot;autotrain_compatible&quot;,&quot;endpoints_compatible&quot;,&quot;has_space&quot;,&quot;region:us&quot;],&quot;string&quot;:&quot;[\n  \&quot;transformers\&quot;,\n  \&quot;pytorch\&quot;,\n  \&quot;tf\&quot;,\n  \&quot;jax\&quot;,\n  \&quot;safetensors\&quot;,\n  \&quot;vit\&quot;,\n  \&quot;image-classification\&quot;,\n  \&quot;vision\&quot;,\n  \&quot;dataset:imagenet-1k\&quot;,\n  \&quot;dataset:imagenet-21k\&quot;,\n  \&quot;arxiv:2010.11929\&quot;,\n  \&quot;arxiv:2006.03677\&quot;,\n  \&quot;license:apache-2.0\&quot;,\n  \&quot;autotrain_compatible\&quot;,\n  \&quot;endpoints_compatible\&quot;,\n  \&quot;has_space\&quot;,\n  \&quot;region:us\&quot;\n]&quot;},&quot;pipeline_tag&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;image-classification&quot;},&quot;createdAt&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;\&quot;2022-03-02T23:29:05\&quot;&quot;},&quot;card&quot;:{&quot;kind&quot;:&quot;string&quot;,&quot;value&quot;:&quot;---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses &amp; limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = &#39;http://images.cocodataset.org/val2017/000000039769.jpg&#39;\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained(&#39;google/vit-base-patch16-224&#39;)\nmodel = ViTForImageClassification.from_pretrained(&#39;google/vit-base-patch16-224&#39;)\n\ninputs = processor(images=image, return_tensors=\&quot;pt\&quot;)\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\&quot;Predicted class:\&quot;, model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```&quot;}}}],&quot;truncated&quot;:true},&quot;paginationData&quot;:{&quot;pageIndex&quot;:0,&quot;numItemsPerPage&quot;:100,&quot;numTotalItems&quot;:581390,&quot;offset&quot;:0,&quot;length&quot;:100}},&quot;jwt&quot;:&quot;eyJhbGciOiJFZERTQSJ9.eyJyZWFkIjp0cnVlLCJwZXJtaXNzaW9ucyI6eyJyZXBvLmNvbnRlbnQucmVhZCI6dHJ1ZX0sIm9uQmVoYWxmT2YiOnsia2luZCI6InVzZXIiLCJfaWQiOiI2Mzk4NmNjNzk4MjM0Y2EyMmY4NzBlN2QiLCJ1c2VyIjoibmVsc29uMjQyNCJ9LCJpYXQiOjE3MTI2NTUzMjcsInN1YiI6Ii9kYXRhc2V0cy9saWJyYXJpYW4tYm90cy9tb2RlbF9jYXJkc193aXRoX21ldGFkYXRhIiwiZXhwIjoxNzEyNjU4OTI3LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.RZ5FKYVwHG-baF8G1xk_7I3o4X8De0UxPtc9tqhXdFe8rDVBMgqwzXzJIH1OMt2FDgpznA2kXf9ih7fDNjmrCA&quot;,&quot;displayUrls&quot;:true},&quot;dataset&quot;:&quot;librarian-bots/model_cards_with_metadata&quot;,&quot;isGated&quot;:false,&quot;isPrivate&quot;:false,&quot;author&quot;:{&quot;avatarUrl&quot;:&quot;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d3e0e8ff1384ce6c5dd17d/h26nKAVX5MubfQiXItvCZ.jpeg?w=200&amp;h=200&amp;f=face&quot;,&quot;fullname&quot;:&quot;Librarian Bots&quot;,&quot;name&quot;:&quot;librarian-bots&quot;,&quot;type&quot;:&quot;org&quot;,&quot;isHf&quot;:false,&quot;isEnterprise&quot;:false},&quot;compact&quot;:true}"><div class="px-2.5 mx-auto mb-10 rounded-lg border pt-2 shadow-sm "><div class="mb-2 flex flex-wrap items-center gap-2 sm:gap-2.5"><div class="mr-auto flex items-center"><svg class="mr-1 flex-none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg> <div class="whitespace-nowrap font-semibold">Dataset Viewer</div> </div> <a href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/tree/refs%2Fconvert%2Fparquet/default" class="group text-sm text-gray-400"><svg class="text-xs mr-1 inline -translate-y-px" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path fill="currentColor" d="M12 10H6.78A11 11 0 0 1 27 16h2A13 13 0 0 0 6 7.68V4H4v8h8zm8 12h5.22A11 11 0 0 1 5 16H3a13 13 0 0 0 23 8.32V28h2v-8h-8z"></path></svg> <span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Auto-converted</span> to Parquet</a> <div class=""><button class="w-full cursor-pointer  btn flex items-center rounded border px-1 py-0.5 text-xs font-normal text-gray-700 shadow-sm hover:text-gray-800 hover:shadow dark:hover:text-gray-200" type="button"><svg class="mr-1.5 " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg> API</button> </div> <a href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/default/train" class="btn flex items-center rounded-full border px-2 py-0.5 text-xs font-normal text-gray-700 shadow-sm hover:text-gray-800 hover:shadow dark:hover:text-gray-200"><svg class="mr-1 text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M2.5 2h7a1 1 0 0 1 1 1v6a1 1 0 0 1-1 1h-7a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1Zm0 2v2h3V4h-3Zm4 0v2h3V4h-3Zm-4 3v2h3V7h-3Zm4 0v2h3V7h-3Z"></path></svg> <div class="hidden sm:block">View in Dataset Viewer</div> <div class="block sm:hidden">Viewer</div></a></div> <div class="-mx-2.5 flex flex-nowrap border-t "><div class="grid flex-1 overflow-hidden text-sm max-md:divide-y md:grid-cols-2 md:place-content-center"><label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-950 md:border-r md:border-r-0 hidden"><span class="text-gray-500">Subset (1)</span> <div class="flex items-center whitespace-nowrap">default <span class="mx-2 text-gray-500">·</span> <span class="text-gray-500">581k rows</span> <svg class="ml-auto" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div> <select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Subset (1)"><option value="default">default (581k rows)</option></optgroup></select></label> <label class="relative block flex-1 px-3 py-2 hover:bg-gray-50 dark:border-gray-850 dark:hover:bg-gray-900 md:border-r"><div class="text-gray-500">Split (1)</div> <div class="flex items-center whitespace-nowrap">train <span class="mx-2 text-gray-500">·</span> <span class="text-gray-500">581k rows</span> <svg class="ml-auto" width="1em" height="1em" viewBox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1 1L6 6L11 1" stroke="currentColor"></path></svg></div> <select class="absolute inset-0 z-10 w-full cursor-pointer border-0 bg-white text-base opacity-0"><optgroup label="Split (1)"><option value="train">train (581k rows)</option></optgroup></select></label></div> </div> <form class="relative -mx-2.5 flex items-center border-t bg-gradient-to-r text-smd dark:border-gray-900 dark:bg-gray-950 [&amp;:has(:focus)]:from-gray-50 [&amp;:has(:focus)]:to-transparent [&amp;:has(:focus)]:to-20% dark:[&amp;:has(:focus)]:from-gray-900"><svg class="absolute left-3 top-1/2 transform -translate-y-1/2 pointer-events-none text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg> <input class="h-9 flex-1 border-none bg-transparent px-1 pl-9 pr-3 outline-none placeholder:text-gray-400 " placeholder="Search this dataset" dir="auto"> </form> <div class="-mx-2.5 flex flex-1 flex-col overflow-hidden border-t min-h-64"> <div class="max-h-96 relative overflow-auto"><table class="w-full table-auto rounded-lg font-mono text-xs text-gray-900"><thead class="sticky left-0 right-0 top-0 z-1 bg-white align-top shadow-sm"><tr class="space-y-54 h-full min-w-fit divide-x border-b text-left"><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>modelId <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-[&#39;·&#39;]">lengths</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="0" y="17.5494841854665" width="11.2" height="12.4505158145335" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="13.2" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="26.4" y="5.300907419547226" width="11.2" height="24.699092580452774" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="39.599999999999994" y="15.118812320401414" width="11.2" height="14.881187679598586" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="52.8" y="21.74597181626849" width="11.2" height="8.25402818373151" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="66" y="23.504098421257574" width="11.2" height="6.495901578742428" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;"><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">41</span><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">53</span></div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">14.6%</div></div></div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>author <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-[&#39;·&#39;]">lengths</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="0" y="16.796740083501245" width="12.666666666666666" height="13.203259916498755" fill-opacity="1"></rect><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="14.666666666666666" y="0" width="12.666666666666666" height="30" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="29.333333333333332" y="18.57463141574678" width="12.666666666666666" height="11.42536858425322" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="44" y="24.87221057643732" width="12.666666666666666" height="5.127789423562681" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="58.666666666666664" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="73.33333333333333" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="88" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="102.66666666666666" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="117.33333333333333" y="25" width="12.666666666666666" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="13.666666666666666" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="28.333333333333332" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="43" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="57.666666666666664" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="72.33333333333333" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="87" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="101.66666666666666" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="116.33333333333333" y="0" width="14.666666666666666" height="30" fill-opacity="0"></rect></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;"><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">7</span><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">12</span></div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">59.2%</div></div></div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>last_modified <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>unknown</span></div></div> <div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>downloads <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>int64</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="79.19999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;"><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">80.7M</span><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">90.8M</span></div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">&lt;0.1%</div></div></div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>likes <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>int64</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="79.19999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-indigo-100 dark:fill-indigo-500/20" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;"><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">0</span><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">1.06k</span></div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">100%</div></div></div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>library_name <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-[&#39;·&#39;]">classes</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><defs><clippath id="rounded-bar"><rect x="0" y="0" width="130" height="8" rx="4"></rect></clippath><pattern id="hatching" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-400 dark:stroke-gray-500/80" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern><pattern id="hatching-faded" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-100 dark:stroke-gray-500/20" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern></defs><g height="8" style="transform: translateY(20px)" clip-path="url(#rounded-bar)"><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="1" y="0" width="54.634599838318515" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="57.634599838318515" y="0" width="3.153803814995098" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="62.788403653313615" y="0" width="3.0330587041400783" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="67.8214623574537" y="0" width="1.1959442026866647" height="8" fill-opacity="1"></rect></g><g><rect class="fill-gray-100 dark:fill-gray-500/20" x="78.13331842652953" y="0" width="50.86668157347047" height="8" fill-opacity="1"></rect></g></g></g><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-white cursor-pointer" x="0" y="0" width="56.634599838318515" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="56.634599838318515" y="0" width="5.153803814995098" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="61.788403653313615" y="0" width="5.033058704140078" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="66.8214623574537" y="0" width="3.1959442026866647" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="70.01740656014036" y="0" width="1.5936118612291235" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="71.61101842136948" y="0" width="1.1003457231806533" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="72.71136414455013" y="0" width="0.9105075766697054" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="73.62187172121983" y="0" width="0.3834775279932575" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="74.0053492492131" y="0" width="0.36514215930786564" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="74.37049140852096" y="0" width="0.3561980770223086" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="74.72668948554328" y="0" width="0.35150243382239116" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="75.07819191936566" y="0" width="0.2808441837664906" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="75.35903610313215" y="0" width="0.2598255903954316" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="75.61886169352759" y="0" width="0.17083197165413921" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="75.78969366518173" y="0" width="0.1410928980546621" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="75.9307865632364" y="0" width="0.12476994788352053" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.05555651111992" y="0" width="0.10263334422676688" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.15818985534668" y="0" width="0.09279485371265415" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.25098470905934" y="0" width="0.07244706651301192" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.32343177557235" y="0" width="0.06439739245601059" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.38782916802836" y="0" width="0.06327938217031596" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.45110855019868" y="0" width="0.05165207519909183" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.50276062539777" y="0" width="0.045167615542062986" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.54792824093984" y="0" width="0.044273207313507285" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.59220144825335" y="0" width="0.042707992913534804" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.63490944116688" y="0" width="0.04248439085639588" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.67739383202328" y="0" width="0.034658318856533483" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.71205215087981" y="0" width="0.03421111474225563" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.74626326562206" y="0" width="0.0243726242281429" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.77063588985021" y="0" width="0.023701818056726123" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.79433770790693" y="0" width="0.023031011885309347" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.81736871979224" y="0" width="0.019900583085364385" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.8372693028776" y="0" width="0.019453378971086534" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.8567226818487" y="0" width="0.01811176662825298" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.87483444847695" y="0" width="0.017664562513975128" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.89249901099092" y="0" width="0.015875746056863725" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.90837475704778" y="0" width="0.015428541942585871" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.92380329899036" y="0" width="0.014757735771169095" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.93856103476153" y="0" width="0.013639725485474466" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.95220076024701" y="0" width="0.010732898742668431" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.96293365898968" y="0" width="0.010509296685529508" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.97344295567521" y="0" width="0.010062092571251655" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.98350504824646" y="0" width="0.008496878171279174" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.99200192641774" y="0" width="0.007602469942723474" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="76.99960439636047" y="0" width="0.007602469942723474" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.00720686630319" y="0" width="0.006484459657028845" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.01369132596022" y="0" width="0.0060372555427509935" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.01972858150297" y="0" width="0.0060372555427509935" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.02576583704572" y="0" width="0.005590051428473143" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.0313558884742" y="0" width="0.004919245257056365" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.03627513373125" y="0" width="0.004919245257056365" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.04119437898831" y="0" width="0.004024837028500662" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.0452192160168" y="0" width="0.003801234971361737" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.04902045098817" y="0" width="0.002906826742806034" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.05192727773097" y="0" width="0.002906826742806034" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.05483410447377" y="0" width="0.0024596226285281825" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.05729372710229" y="0" width="0.0024596226285281825" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.05975334973081" y="0" width="0.0022360205713892567" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.0619893703022" y="0" width="0.002012418514250331" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.06400178881646" y="0" width="0.002012418514250331" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.06601420733071" y="0" width="0.002012418514250331" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.06802662584496" y="0" width="0.002012418514250331" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.07003904435922" y="0" width="0.0017888164571114055" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.07182786081633" y="0" width="0.0017888164571114055" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.07361667727344" y="0" width="0.0017888164571114055" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.07540549373054" y="0" width="0.00156521439997248" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.07697070813052" y="0" width="0.00156521439997248" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.07853592253049" y="0" width="0.001341612342833554" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.07987753487332" y="0" width="0.001341612342833554" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.08121914721615" y="0" width="0.001341612342833554" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.08256075955897" y="0" width="0.001341612342833554" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.0839023719018" y="0" width="0.001341612342833554" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.08524398424463" y="0" width="0.001341612342833554" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.08658559658745" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.08770360687315" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.08882161715884" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.08993962744454" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.09105763773023" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.09217564801592" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.09329365830162" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.09441166858731" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.095529678873" y="0" width="0.0011180102856946283" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.0966476891587" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.09754209738726" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.09843650561582" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.09933091384438" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10022532207294" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1011197303015" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10201413853007" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10290854675863" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10380295498719" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10469736321575" y="0" width="0.0008944082285557028" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10559177144431" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10626257761572" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10693338378714" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10760418995855" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10827499612996" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10894580230138" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.10961660847279" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1102874146442" y="0" width="0.000670806171416777" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11095822081562" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1114054249299" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11185262904418" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11229983315846" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11274703727274" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11319424138702" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1136414455013" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11408864961558" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11453585372986" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11498305784414" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11543026195842" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1158774660727" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11632467018698" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11677187430126" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11721907841554" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11766628252983" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1181134866441" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11856069075839" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11900789487267" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1192314969298" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11945509898693" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.11967870104407" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1199023031012" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12012590515833" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12034950721547" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1205731092726" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12079671132973" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12102031338686" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.121243915444" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12146751750113" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12169111955826" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1219147216154" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12213832367253" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12236192572966" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1225855277868" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12280912984393" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12303273190106" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1232563339582" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12347993601533" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12370353807246" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1239271401296" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12415074218673" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12437434424386" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.124597946301" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12482154835813" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12504515041526" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1252687524724" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12549235452953" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12571595658666" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.1259395586438" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12616316070093" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12638676275806" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12661036481519" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12683396687233" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12705756892946" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12728117098659" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12750477304373" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12772837510086" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12795197715799" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12817557921512" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12839918127226" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12862278332939" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12884638538652" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12906998744366" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12929358950079" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12951719155792" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12974079361506" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.12996439567219" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13018799772932" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13041159978646" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13063520184359" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13085880390072" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13108240595786" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13130600801499" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13152961007212" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13175321212925" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13197681418639" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13220041624352" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13242401830065" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13264762035779" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13287122241492" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="77.13309482447205" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect></g><g><rect class="fill-white cursor-pointer" x="77.13331842652953" y="0" width="52.86668157347047" height="28" fill-opacity="0"></rect></g></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;">transformers</div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">43.6%</div></div></div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>tags <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>sequence</span><span class="italic text-gray-400 before:mx-1 before:content-[&#39;·&#39;]">lengths</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="39.599999999999994" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="52.8" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="66" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="79.19999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="92.39999999999999" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="105.6" y="26" width="11.2" height="4" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;"><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">1</span><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">185</span></div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">100%</div></div></div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>pipeline_tag <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-[&#39;·&#39;]">classes</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><defs><clippath id="rounded-bar"><rect x="0" y="0" width="130" height="8" rx="4"></rect></clippath><pattern id="hatching" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-400 dark:stroke-gray-500/80" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern><pattern id="hatching-faded" patternUnits="userSpaceOnUse" patternTransform="rotate(-45)" height="1" width="5"><line y1="0" class="stroke-gray-100 dark:stroke-gray-500/20" stroke-width="3" y2="1" x1="2" x2="2"></line></pattern></defs><g height="8" style="transform: translateY(20px)" clip-path="url(#rounded-bar)"><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-gray-100 dark:fill-gray-500/20" x="1" y="0" width="14.364763755826552" height="8" fill-opacity="1"></rect><rect class="fill-indigo-500 dark:fill-indigo-600/80" x="17.36476375582655" y="0" width="10.258759180584462" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="29.623522936411014" y="0" width="6.8946662309293245" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="38.51818916734034" y="0" width="3.8496534168114342" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="44.367842584151774" y="0" width="2.257383167925145" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="48.625225752076915" y="0" width="1.6346514387932367" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="52.25987719087015" y="0" width="1.4716455391389602" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="55.73152273000911" y="0" width="0.5068026625844957" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="58.23832539259361" y="0" width="0.4860076712705754" height="8" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" x="60.72433306386419" y="0" width="0.26218201207451086" height="8" fill-opacity="1"></rect></g><g><rect class="fill-gray-100 dark:fill-gray-500/20" x="70.3059048143243" y="0" width="58.69409518567571" height="8" fill-opacity="1"></rect></g></g></g><g style="transform: scaleX(1.0153846153846153) translateX(-1px)"><g><rect class="fill-white cursor-pointer" x="0" y="0" width="16.36476375582655" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="16.36476375582655" y="0" width="12.258759180584462" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="28.623522936411014" y="0" width="8.894666230929325" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="37.51818916734034" y="0" width="5.849653416811434" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="43.367842584151774" y="0" width="4.257383167925145" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="47.625225752076915" y="0" width="3.6346514387932367" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="51.25987719087015" y="0" width="3.4716455391389602" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="54.73152273000911" y="0" width="2.5068026625844957" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="57.23832539259361" y="0" width="2.4860076712705754" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="59.72433306386419" y="0" width="2.262182012074511" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="61.9865150759387" y="0" width="1.7154749823698379" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="63.70199005830854" y="0" width="0.8335884690139149" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="64.53557852732246" y="0" width="0.8262096011283303" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="65.36178812845078" y="0" width="0.7696382806721822" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="66.13142640912297" y="0" width="0.4518997574777688" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="66.58332616660074" y="0" width="0.41411100982129034" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="66.99743717642203" y="0" width="0.3727446292505891" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="67.37018180567262" y="0" width="0.36536576136500454" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="67.73554756703761" y="0" width="0.24618586490995717" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="67.98173343194757" y="0" width="0.2394778031957894" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.22121123514336" y="0" width="0.16993756342558353" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.39114879856893" y="0" width="0.1558506338258312" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.54699943239477" y="0" width="0.13863327542613393" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.6856327078209" y="0" width="0.10062092571251656" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.78625363353342" y="0" width="0.08519238376993069" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.87144601730336" y="0" width="0.06506819862742737" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.93651421593079" y="0" width="0.053664493713342164" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="68.99017870964413" y="0" width="0.04360240114209051" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.03378111078622" y="0" width="0.04226078879925695" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.07604189958548" y="0" width="0.04158998262784017" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.11763188221332" y="0" width="0.036447135313644886" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.15407901752697" y="0" width="0.0243726242281429" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.17845164175512" y="0" width="0.02034778719964224" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.19879942895476" y="0" width="0.01900617485680868" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.21780560381157" y="0" width="0.018335368685391904" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.23614097249695" y="0" width="0.015652143999724796" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.25179311649669" y="0" width="0.010956500799807358" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.26274961729649" y="0" width="0.006931663771306697" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.2696812810678" y="0" width="0.00626085759988992" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.27594213866769" y="0" width="0.005590051428473143" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.28153219009616" y="0" width="0.005590051428473143" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.28712224152463" y="0" width="0.004695643199917439" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.29181788472455" y="0" width="0.004472041142778513" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.29628992586733" y="0" width="0.004472041142778513" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.3007619670101" y="0" width="0.004248439085639587" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.30501040609575" y="0" width="0.0004472041142778514" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.30545761021003" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect><rect class="fill-white cursor-pointer" x="69.30568121226716" y="0" width="0.0002236020571389257" height="28" fill-opacity="0"></rect></g><g><rect class="fill-white cursor-pointer" x="69.3059048143243" y="0" width="60.69409518567571" height="28" fill-opacity="0"></rect></g></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;">text-classification</div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">9.4%</div></div></div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>createdAt <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>unknown</span></div></div> <div></div></div></th><th class="h-full max-w-sm p-2 text-left"><div class="flex h-full flex-col flex-nowrap justify-between"><div>card <div class="mb-2 whitespace-nowrap text-xs font-normal text-gray-500"><span>string</span><span class="italic text-gray-400 before:mx-1 before:content-[&#39;·&#39;]">lengths</span></div></div> <div><div class="" style="height: 40px; padding-top: 2px"><svg width="130" height="28"><g><rect class="fill-gray-400 dark:fill-gray-500/80" rx="2" x="0" y="0" width="11.2" height="30" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="13.2" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="26.4" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="39.599999999999994" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="52.8" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="66" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="79.19999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="92.39999999999999" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="105.6" y="25" width="11.2" height="5" fill-opacity="1"></rect><rect class="fill-gray-100 dark:fill-gray-500/20" rx="2" x="118.8" y="25" width="11.2" height="5" fill-opacity="1"></rect></g><rect class="fill-white dark:fill-gray-900" x="0" y="26" width="130" height="2" stroke-opacity="1"></rect><line class="stroke-gray-100 dark:stroke-gray-500/20" x1="0" y1="27.5" x2="130" y2="27.5" stroke-opacity="1"></line><g><rect class="fill-indigo-500 cursor-pointer" x="-1" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="12.2" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="25.4" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="38.599999999999994" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="51.8" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="65" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="78.19999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="91.39999999999999" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="104.6" y="0" width="13.2" height="30" fill-opacity="0"></rect><rect class="fill-indigo-500 cursor-pointer" x="117.8" y="0" width="13.2" height="30" fill-opacity="0"></rect></g></svg> <div class="relative font-light text-gray-400" style="height: 10px; width: 130px;"><div class="absolute left-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 84px;"><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">1</span><span class="after:text-gray-300 after:content-[&#39;↔︎&#39;] last:after:content-[&#39;&#39;]">90.1k</span></div> <div class="absolute right-0 overflow-hidden text-ellipsis whitespace-nowrap" style="max-width: 36px;">99.9%</div></div></div></div></div></th></tr></thead> <tbody class="h-16 overflow-scroll"><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="0"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pysentimiento/robertuito-sentiment-analysis</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pysentimiento</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-27T20:46:41"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">100,903,751</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">50</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pysentimiento</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "pysentimiento",
  "pytorch",
  "tf",
  "safetensors",
  "roberta",
  "twitter",
  "sentiment-analysis",
  "es",
  "arxiv:2106.09462",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><div class="text-right text-gray-400">null</div></div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: 
  - es
library_name: pysentimiento

tags:
  - twitter
  - sentiment-analysis

---
# Sentiment Analysis in Spanish
## robertuito-sentiment-analysis

Repository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)


Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets.

Uses `POS`, `NEG`, `NEU` labels.

## Usage

Use it directly with [pysentimiento](https://github.com/pysentimiento/pysentimiento)

```python
from pysentimiento import create_analyzer
analyzer = create_analyzer(task="sentiment", lang="es")

analyzer.predict("Qué gran jugador es Messi")
# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})
```


## Results

Results for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores


| model         | emotion       | hate_speech   | irony         | sentiment     |
|:--------------|:--------------|:--------------|:--------------|:--------------|
| robertuito    | 0.560 ± 0.010 | 0.759 ± 0.007 | 0.739 ± 0.005 | 0.705 ± 0.003 |
| roberta       | 0.527 ± 0.015 | 0.741 ± 0.012 | 0.721 ± 0.008 | 0.670 ± 0.006 |
| bertin        | 0.524 ± 0.007 | 0.738 ± 0.007 | 0.713 ± 0.012 | 0.666 ± 0.005 |
| beto_uncased  | 0.532 ± 0.012 | 0.727 ± 0.016 | 0.701 ± 0.007 | 0.651 ± 0.006 |
| beto_cased    | 0.516 ± 0.012 | 0.724 ± 0.012 | 0.705 ± 0.009 | 0.662 ± 0.005 |
| mbert_uncased | 0.493 ± 0.010 | 0.718 ± 0.011 | 0.681 ± 0.010 | 0.617 ± 0.003 |
| biGRU         | 0.264 ± 0.007 | 0.592 ± 0.018 | 0.631 ± 0.011 | 0.585 ± 0.011 |


Note that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B

## Citation

If you use this model in your research, please cite pysentimiento and RoBERTuito papers:

```
@misc{perez2021pysentimiento,
      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},
      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},
      year={2021},
      eprint={2106.09462},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{perez-etal-2022-robertuito,
    title = "{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish",
    author = "P{\'e}rez, Juan Manuel  and
      Furman, Dami{\'a}n Ariel  and
      Alonso Alemany, Laura  and
      Luque, Franco M.",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.785",
    pages = "7235--7243",
    abstract = "Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.",
}

@inproceedings{garcia2020overview,
  title={Overview of TASS 2020: Introducing emotion detection},
  author={Garc{\'\i}a-Vega, Manuel and D{\'\i}az-Galiano, MC and Garc{\'\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\'a}ez, A and Jim{\'e}nez-Zafra, SM and Mart{\'\i}nez C{\'a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},
  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\'a}laga, Spain},
  pages={163--170},
  year={2020}
}
```</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="1"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">cardiffnlp/twitter-roberta-base-sentiment-latest</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">cardiffnlp</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-05-28T05:45:10"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">88,609,082</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">359</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "roberta",
  "text-classification",
  "en",
  "dataset:tweet_eval",
  "arxiv:2202.03829",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">text-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-15T01:21:58"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
widget:
- text: Covid cases are increasing fast!
datasets:
- tweet_eval
---


# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)

This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. 
The original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. 

- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). 
- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).

&lt;b&gt;Labels&lt;/b&gt;: 
0 -&gt; Negative;
1 -&gt; Neutral;
2 -&gt; Positive

This sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).

## Example Pipeline
```python
from transformers import pipeline
sentiment_task = pipeline("sentiment-analysis", model=model_path, tokenizer=model_path)
sentiment_task("Covid cases are increasing fast!")
```
```
[{'label': 'Negative', 'score': 0.7236}]
```

## Full classification example

```python
from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
import numpy as np
from scipy.special import softmax
# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)
MODEL = f"cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
config = AutoConfig.from_pretrained(MODEL)
# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
#model.save_pretrained(MODEL)
text = "Covid cases are increasing fast!"
text = preprocess(text)
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
# # TF
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)
# model.save_pretrained(MODEL)
# text = "Covid cases are increasing fast!"
# encoded_input = tokenizer(text, return_tensors='tf')
# output = model(encoded_input)
# scores = output[0][0].numpy()
# scores = softmax(scores)
# Print labels and scores
ranking = np.argsort(scores)
ranking = ranking[::-1]
for i in range(scores.shape[0]):
    l = config.id2label[ranking[i]]
    s = scores[ranking[i]]
    print(f"{i+1}) {l} {np.round(float(s), 4)}")
```

Output: 

```
1) Negative 0.7236
2) Neutral 0.2287
3) Positive 0.0477
```


### References 
```
@inproceedings{camacho-collados-etal-2022-tweetnlp,
    title = "{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media",
    author = "Camacho-collados, Jose  and
      Rezaee, Kiamehr  and
      Riahi, Talayeh  and
      Ushio, Asahi  and
      Loureiro, Daniel  and
      Antypas, Dimosthenis  and
      Boisson, Joanne  and
      Espinosa Anke, Luis  and
      Liu, Fangyu  and
      Mart{\'\i}nez C{\'a}mara, Eugenio" and others,
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-demos.5",
    pages = "38--49"
}

```

```
@inproceedings{loureiro-etal-2022-timelms,
    title = "{T}ime{LM}s: Diachronic Language Models from {T}witter",
    author = "Loureiro, Daniel  and
      Barbieri, Francesco  and
      Neves, Leonardo  and
      Espinosa Anke, Luis  and
      Camacho-collados, Jose",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.25",
    doi = "10.18653/v1/2022.acl-demo.25",
    pages = "251--260"
}

```
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="2"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">jonatasgrosman/wav2vec2-large-xlsr-53-english</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">jonatasgrosman</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-03-25T10:56:55"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">56,129,414</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">395</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "jax",
  "safetensors",
  "wav2vec2",
  "automatic-speech-recognition",
  "audio",
  "en",
  "hf-asr-leaderboard",
  "mozilla-foundation/common_voice_6_0",
  "robust-speech-event",
  "speech",
  "xlsr-fine-tuning-week",
  "dataset:common_voice",
  "dataset:mozilla-foundation/common_voice_6_0",
  "license:apache-2.0",
  "model-index",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">automatic-speech-recognition</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
datasets:
- common_voice
- mozilla-foundation/common_voice_6_0
metrics:
- wer
- cer
tags:
- audio
- automatic-speech-recognition
- en
- hf-asr-leaderboard
- mozilla-foundation/common_voice_6_0
- robust-speech-event
- speech
- xlsr-fine-tuning-week
license: apache-2.0
model-index:
- name: XLSR Wav2Vec2 English by Jonatas Grosman
  results:
  - task:
      name: Automatic Speech Recognition
      type: automatic-speech-recognition
    dataset:
      name: Common Voice en
      type: common_voice
      args: en
    metrics:
    - name: Test WER
      type: wer
      value: 19.06
    - name: Test CER
      type: cer
      value: 7.69
    - name: Test WER (+LM)
      type: wer
      value: 14.81
    - name: Test CER (+LM)
      type: cer
      value: 6.84
  - task:
      name: Automatic Speech Recognition
      type: automatic-speech-recognition
    dataset:
      name: Robust Speech Event - Dev Data
      type: speech-recognition-community-v2/dev_data
      args: en
    metrics:
    - name: Dev WER
      type: wer
      value: 27.72
    - name: Dev CER
      type: cer
      value: 11.65
    - name: Dev WER (+LM)
      type: wer
      value: 20.85
    - name: Dev CER (+LM)
      type: cer
      value: 11.01
---

# Fine-tuned XLSR-53 large model for speech recognition in English

Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on English using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice).
When using this model, make sure that your speech input is sampled at 16kHz.

This model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)

The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint

## Usage

The model can be used directly (without a language model) as follows...

Using the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:

```python
from huggingsound import SpeechRecognitionModel

model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-english")
audio_paths = ["/path/to/file.mp3", "/path/to/another_file.wav"]

transcriptions = model.transcribe(audio_paths)
```

Writing your own inference script:

```python
import torch
import librosa
from datasets import load_dataset
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

LANG_ID = "en"
MODEL_ID = "jonatasgrosman/wav2vec2-large-xlsr-53-english"
SAMPLES = 10

test_dataset = load_dataset("common_voice", LANG_ID, split=f"test[:{SAMPLES}]")

processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)
model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)

# Preprocessing the datasets.
# We need to read the audio files as arrays
def speech_file_to_array_fn(batch):
    speech_array, sampling_rate = librosa.load(batch["path"], sr=16_000)
    batch["speech"] = speech_array
    batch["sentence"] = batch["sentence"].upper()
    return batch

test_dataset = test_dataset.map(speech_file_to_array_fn)
inputs = processor(test_dataset["speech"], sampling_rate=16_000, return_tensors="pt", padding=True)

with torch.no_grad():
    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits

predicted_ids = torch.argmax(logits, dim=-1)
predicted_sentences = processor.batch_decode(predicted_ids)

for i, predicted_sentence in enumerate(predicted_sentences):
    print("-" * 100)
    print("Reference:", test_dataset[i]["sentence"])
    print("Prediction:", predicted_sentence)
```

| Reference  | Prediction |
| ------------- | ------------- |
| "SHE'LL BE ALL RIGHT." | SHE'LL BE ALL RIGHT |
| SIX | SIX |
| "ALL'S WELL THAT ENDS WELL." | ALL AS WELL THAT ENDS WELL |
| DO YOU MEAN IT? | DO YOU MEAN IT |
| THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE, BUT STILL CAUSES REGRESSIONS. | THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE BUT STILL CAUSES REGRESSION |
| HOW IS MOZILLA GOING TO HANDLE AMBIGUITIES LIKE QUEUE AND CUE? | HOW IS MOSLILLAR GOING TO HANDLE ANDBEWOOTH HIS LIKE Q AND Q |
| "I GUESS YOU MUST THINK I'M KINDA BATTY." | RUSTIAN WASTIN PAN ONTE BATTLY |
| NO ONE NEAR THE REMOTE MACHINE YOU COULD RING? | NO ONE NEAR THE REMOTE MACHINE YOU COULD RING |
| SAUCE FOR THE GOOSE IS SAUCE FOR THE GANDER. | SAUCE FOR THE GUICE IS SAUCE FOR THE GONDER |
| GROVES STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD. | GRAFS STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD |

## Evaluation

1. To evaluate on `mozilla-foundation/common_voice_6_0` with split `test`

```bash
python eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-english --dataset mozilla-foundation/common_voice_6_0 --config en --split test
```

2. To evaluate on `speech-recognition-community-v2/dev_data`

```bash
python eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-english --dataset speech-recognition-community-v2/dev_data --config en --split validation --chunk_length_s 5.0 --stride_length_s 1.0
```

## Citation
If you want to cite this model you can use this:

```bibtex
@misc{grosman2021xlsr53-large-english,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {E}nglish},
  author={Grosman, Jonatas},
  howpublished={\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english}},
  year={2021}
}
```</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="3"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai/clip-vit-large-patch14</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-09-15T15:49:35"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">51,405,955</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1,063</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "safetensors",
  "clip",
  "zero-shot-image-classification",
  "vision",
  "arxiv:2103.00020",
  "arxiv:1908.04913",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">zero-shot-image-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
tags:
- vision
widget:
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png
  candidate_labels: playing music, playing sports
  example_title: Cat &amp; Dog
---

# Model Card: CLIP

Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).

## Model Details

The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.

### Model Date

January 2021

### Model Type

The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.

The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.


### Documents

- [Blog Post](https://openai.com/blog/clip/)
- [CLIP Paper](https://arxiv.org/abs/2103.00020)


### Use with Transformers

```python
from PIL import Image
import requests

from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities
```


## Model Use

### Intended Use

The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.

#### Primary intended uses

The primary intended users of these models are AI researchers.

We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.

### Out-of-Scope Use Cases

**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. 

Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.

Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.



## Data

The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.

### Data Mission Statement

Our goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.



## Performance and Limitations

### Performance

We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:

- Food101
- CIFAR10   
- CIFAR100   
- Birdsnap
- SUN397
- Stanford Cars
- FGVC Aircraft
- VOC2007
- DTD
- Oxford-IIIT Pet dataset
- Caltech101
- Flowers102
- MNIST   
- SVHN 
- IIIT5K   
- Hateful Memes   
- SST-2
- UCF101
- Kinetics700
- Country211
- CLEVR Counting
- KITTI Distance
- STL-10
- RareAct
- Flickr30
- MSCOCO
- ImageNet
- ImageNet-A
- ImageNet-R
- ImageNet Sketch
- ObjectNet (ImageNet Overlap)
- Youtube-BB
- ImageNet-Vid

## Limitations

CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.

### Bias and Fairness

We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).

We also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy &gt;96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.



## Feedback

### Where to send questions or comments about the model

Please use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="4"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">google-bert/bert-base-uncased</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">google-bert</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-19T11:06:12"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">45,346,028</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1,461</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "rust",
  "coreml",
  "onnx",
  "safetensors",
  "bert",
  "fill-mask",
  "exbert",
  "en",
  "dataset:bookcorpus",
  "dataset:wikipedia",
  "arxiv:1810.04805",
  "license:apache-2.0",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">fill-mask</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
tags:
- exbert
license: apache-2.0
datasets:
- bookcorpus
- wikipedia
---

# BERT base model (uncased)

Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
[this paper](https://arxiv.org/abs/1810.04805) and first released in
[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference
between english and English.

Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.

## Model description

BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:

- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run
  the entire masked sentence through the model and has to predict the masked words. This is different from traditional
  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like
  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the
  sentence.
- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes
  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to
  predict if the two sentences were following each other or not.

This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.

## Model variations

BERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  
Chinese and multilingual uncased and cased versions followed shortly after.  
Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  
Other 24 smaller models are released afterward.  

The detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.

| Model | #params | Language |
|------------------------|--------------------------------|-------|
| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |
| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub 
| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |
| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |
| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |
| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |
| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |
| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |

## Intended uses &amp; limitations

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for
fine-tuned versions of a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

You can use this model directly with a pipeline for masked language modeling:

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
&gt;&gt;&gt; unmasker("Hello I'm a [MASK] model.")

[{'sequence': "[CLS] hello i'm a fashion model. [SEP]",
  'score': 0.1073106899857521,
  'token': 4827,
  'token_str': 'fashion'},
 {'sequence': "[CLS] hello i'm a role model. [SEP]",
  'score': 0.08774490654468536,
  'token': 2535,
  'token_str': 'role'},
 {'sequence': "[CLS] hello i'm a new model. [SEP]",
  'score': 0.05338378623127937,
  'token': 2047,
  'token_str': 'new'},
 {'sequence': "[CLS] hello i'm a super model. [SEP]",
  'score': 0.04667217284440994,
  'token': 3565,
  'token_str': 'super'},
 {'sequence': "[CLS] hello i'm a fine model. [SEP]",
  'score': 0.027095865458250046,
  'token': 2986,
  'token_str': 'fine'}]
```

Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained("bert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

and in TensorFlow:

```python
from transformers import BertTokenizer, TFBertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained("bert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

### Limitations and bias

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased
predictions:

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='bert-base-uncased')
&gt;&gt;&gt; unmasker("The man worked as a [MASK].")

[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',
  'score': 0.09747550636529922,
  'token': 10533,
  'token_str': 'carpenter'},
 {'sequence': '[CLS] the man worked as a waiter. [SEP]',
  'score': 0.0523831807076931,
  'token': 15610,
  'token_str': 'waiter'},
 {'sequence': '[CLS] the man worked as a barber. [SEP]',
  'score': 0.04962705448269844,
  'token': 13362,
  'token_str': 'barber'},
 {'sequence': '[CLS] the man worked as a mechanic. [SEP]',
  'score': 0.03788609802722931,
  'token': 15893,
  'token_str': 'mechanic'},
 {'sequence': '[CLS] the man worked as a salesman. [SEP]',
  'score': 0.037680890411138535,
  'token': 18968,
  'token_str': 'salesman'}]

&gt;&gt;&gt; unmasker("The woman worked as a [MASK].")

[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',
  'score': 0.21981462836265564,
  'token': 6821,
  'token_str': 'nurse'},
 {'sequence': '[CLS] the woman worked as a waitress. [SEP]',
  'score': 0.1597415804862976,
  'token': 13877,
  'token_str': 'waitress'},
 {'sequence': '[CLS] the woman worked as a maid. [SEP]',
  'score': 0.1154729500412941,
  'token': 10850,
  'token_str': 'maid'},
 {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',
  'score': 0.037968918681144714,
  'token': 19215,
  'token_str': 'prostitute'},
 {'sequence': '[CLS] the woman worked as a cook. [SEP]',
  'score': 0.03042375110089779,
  'token': 5660,
  'token_str': 'cook'}]
```

This bias will also affect all fine-tuned versions of this model.

## Training data

The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038
unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and
headers).

## Training procedure

### Preprocessing

The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are
then of the form:

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.

The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

### Pretraining

The model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size
of 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer
used is Adam with a learning rate of 1e-4, \\(\beta_{1} = 0.9\\) and \\(\beta_{2} = 0.999\\), a weight decay of 0.01,
learning rate warmup for 10,000 steps and linear decay of the learning rate after.

## Evaluation results

When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |
|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|
|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |


### BibTeX entry and citation info

```bibtex
@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

&lt;a href="https://huggingface.co/exbert/?model=bert-base-uncased"&gt;
	&lt;img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"&gt;
&lt;/a&gt;
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="5"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">CAMeL-Lab</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2021-10-18T10:15:57"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">42,126,368</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "bert",
  "token-classification",
  "ar",
  "arxiv:2103.06678",
  "license:apache-2.0",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">token-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: 
- ar
license: apache-2.0
widget:
 - text: 'عامل ايه ؟'
---
# CAMeLBERT-Mix POS-EGY Model
## Model description
**CAMeLBERT-Mix POS-EGY Model** is a Egyptian Arabic POS tagging model that was built by fine-tuning the [CAMeLBERT-Mix](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix/) model.
For the fine-tuning, we used the ARZTB dataset .
Our fine-tuning procedure and the hyperparameters we used can be found in our paper *"[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678)."* Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT).

## Intended uses
You can use the CAMeLBERT-Mix POS-EGY model as part of the transformers pipeline.
This model will also be available in [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) soon.

#### How to use
To use the model with a transformers pipeline:
```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; pos = pipeline('token-classification', model='CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy')
&gt;&gt;&gt; text = 'عامل ايه ؟'
&gt;&gt;&gt; pos(text)
[{'entity': 'adj', 'score': 0.9972628, 'index': 1, 'word': 'عامل', 'start': 0, 'end': 4}, {'entity': 'pron_interrog', 'score': 0.9525163, 'index': 2, 'word': 'ايه', 'start': 5, 'end': 8}, {'entity': 'punc', 'score': 0.99869114, 'index': 3, 'word': '؟', 'start': 9, 'end': 10}]
```
*Note*: to download our models, you would need `transformers&gt;=3.5.0`.
Otherwise, you could download the models manually.
## Citation
```bibtex
@inproceedings{inoue-etal-2021-interplay,
    title = "The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models",
    author = "Inoue, Go  and
      Alhafni, Bashar  and
      Baimukan, Nurpeiis  and
      Bouamor, Houda  and
      Habash, Nizar",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Online)",
    publisher = "Association for Computational Linguistics",
    abstract = "In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.",
}
```</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="6"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">tohoku-nlp/bert-base-japanese</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">tohoku-nlp</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-22T00:57:00"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">31,374,146</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">27</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "bert",
  "fill-mask",
  "ja",
  "dataset:wikipedia",
  "license:cc-by-sa-4.0",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">fill-mask</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: ja
license: cc-by-sa-4.0
datasets:
- wikipedia
widget:
- text: 東北大学で[MASK]の研究をしています。
---

# BERT base Japanese (IPA dictionary)

This is a [BERT](https://github.com/google-research/bert) model pretrained on texts in the Japanese language.

This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.

The codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0).

## Model architecture

The model architecture is the same as the original BERT base model; 12 layers, 768 dimensions of hidden states, and 12 attention heads.

## Training Data

The model is trained on Japanese Wikipedia as of September 1, 2019.
To generate the training corpus, [WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.
The text files used for the training are 2.6GB in size, consisting of approximately 17M sentences.

## Tokenization

The texts are first tokenized by [MeCab](https://taku910.github.io/mecab/) morphological parser with the IPA dictionary and then split into subwords by the WordPiece algorithm.
The vocabulary size is 32000.

## Training

The model is trained with the same configuration as the original BERT; 512 tokens per instance, 256 instances per batch, and 1M training steps.

## Licenses

The pretrained models are distributed under the terms of the [Creative Commons Attribution-ShareAlike 3.0](https://creativecommons.org/licenses/by-sa/3.0/).

## Acknowledgments

For training models, we used Cloud TPUs provided by [TensorFlow Research Cloud](https://www.tensorflow.org/tfrc/) program.
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="7"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-transformers/all-MiniLM-L6-v2</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-03-27T09:43:07"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">26,283,676</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1,584</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "sentence-transformers",
  "pytorch",
  "tf",
  "rust",
  "safetensors",
  "bert",
  "feature-extraction",
  "sentence-similarity",
  "transformers",
  "en",
  "dataset:s2orc",
  "dataset:flax-sentence-embeddings/stackexchange_xml",
  "dataset:ms_marco",
  "dataset:gooaq",
  "dataset:yahoo_answers_topics",
  "dataset:code_search_net",
  "dataset:search_qa",
  "dataset:eli5",
  "dataset:snli",
  "dataset:multi_nli",
  "dataset:wikihow",
  "dataset:natural_questions",
  "dataset:trivia_qa",
  "dataset:embedding-data/sentence-compression",
  "dataset:embedding-data/flickr30k-captions",
  "dataset:embedding-data/altlex",
  "dataset:embedding-data/simple-wiki",
  "dataset:embedding-data/QQP",
  "dataset:embedding-data/SPECTER",
  "dataset:embedding-data/PAQ_pairs",
  "dataset:embedding-data/WikiAnswers",
  "arxiv:1904.06472",
  "arxiv:2102.07033",
  "arxiv:2104.08727",
  "arxiv:1704.05179",
  "arxiv:1810.09305",
  "license:apache-2.0",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-similarity</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
license: apache-2.0
library_name: sentence-transformers
tags:
- sentence-transformers
- feature-extraction
- sentence-similarity
- transformers
datasets:
- s2orc
- flax-sentence-embeddings/stackexchange_xml
- ms_marco
- gooaq
- yahoo_answers_topics
- code_search_net
- search_qa
- eli5
- snli
- multi_nli
- wikihow
- natural_questions
- trivia_qa
- embedding-data/sentence-compression
- embedding-data/flickr30k-captions
- embedding-data/altlex
- embedding-data/simple-wiki
- embedding-data/QQP
- embedding-data/SPECTER
- embedding-data/PAQ_pairs
- embedding-data/WikiAnswers
pipeline_tag: sentence-similarity
---


# all-MiniLM-L6-v2
This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences &amp; paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.

## Usage (Sentence-Transformers)
Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:

```
pip install -U sentence-transformers
```

Then you can use the model like this:
```python
from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode(sentences)
print(embeddings)
```

## Usage (HuggingFace Transformers)
Without [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

```python
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Sentences we want sentence embeddings for
sentences = ['This is an example sentence', 'Each sentence is converted']

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')

# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

# Perform pooling
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])

# Normalize embeddings
sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)

print("Sentence embeddings:")
print(sentence_embeddings)
```

## Evaluation Results

For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-MiniLM-L6-v2)

------

## Background

The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised 
contrastive learning objective. We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.

We developed this model during the 
[Community week using JAX/Flax for NLP &amp; CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), 
organized by Hugging Face. We developed this model as part of the project:
[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.

## Intended uses

Our model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.

By default, input text longer than 256 word pieces is truncated.


## Training procedure

### Pre-training 

We use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.

### Fine-tuning 

We fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.
We then apply the cross entropy loss by comparing with true pairs.

#### Hyper parameters

We trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).
We use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with
a 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.

#### Training data

We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.


| Dataset                                                  | Paper                                    | Number of training tuples  |
|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|
| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |
| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |
| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |
| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |
| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |
| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |
| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |
| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |
| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |
| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|
| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |
| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |
| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |
| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |
| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | 
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |
| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |
| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |
| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |
| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |
| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |
| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |
| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |
| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |
| **Total** | | **1,170,060,424** |</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="8"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">mrm8488</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-01-21T15:17:58"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">19,914,327</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">215</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tensorboard",
  "safetensors",
  "roberta",
  "text-classification",
  "generated_from_trainer",
  "financial",
  "stocks",
  "sentiment",
  "dataset:financial_phrasebank",
  "license:apache-2.0",
  "model-index",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">text-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
license: apache-2.0
thumbnail: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png
tags:
- generated_from_trainer
- financial
- stocks
- sentiment
widget:
- text: "Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 ."
datasets:
- financial_phrasebank
metrics:
- accuracy
model-index:
- name: distilRoberta-financial-sentiment
  results:
  - task:
      name: Text Classification
      type: text-classification
    dataset:
      name: financial_phrasebank
      type: financial_phrasebank
      args: sentences_allagree
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.9823008849557522
---

&lt;!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. --&gt;


&lt;div style="text-align:center;width:250px;height:250px;"&gt;
    &lt;img src="https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png" alt="logo"&gt;
&lt;/div&gt;


# DistilRoberta-financial-sentiment


This model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset.
It achieves the following results on the evaluation set:
- Loss: 0.1116
- Accuracy: **0.98**23

## Base Model description

This model is a distilled version of the [RoBERTa-base model](https://huggingface.co/roberta-base). It follows the same training procedure as [DistilBERT](https://huggingface.co/distilbert-base-uncased).
The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/distillation).
This model is case-sensitive: it makes a difference between English and English.

The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).
On average DistilRoBERTa is twice as fast as Roberta-base.

## Training Data

Polar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 5

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| No log        | 1.0   | 255  | 0.1670          | 0.9646   |
| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |
| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |
| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |
| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |


### Framework versions

- Transformers 4.10.2
- Pytorch 1.9.0+cu102
- Datasets 1.12.1
- Tokenizers 0.10.3
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="9"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">distilbert/distilbert-base-uncased</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">distilbert</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-08-18T14:59:41"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">16,539,195</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">391</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "rust",
  "safetensors",
  "distilbert",
  "fill-mask",
  "exbert",
  "en",
  "dataset:bookcorpus",
  "dataset:wikipedia",
  "arxiv:1910.01108",
  "license:apache-2.0",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">fill-mask</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
tags:
- exbert
license: apache-2.0
datasets:
- bookcorpus
- wikipedia
---

# DistilBERT base model (uncased)

This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was
introduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found
[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does
not make a difference between english and English.

## Model description

DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a
self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,
with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic
process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained
with three objectives:

- Distillation loss: the model was trained to return the same probabilities as the BERT base model.
- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a
  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the
  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that
  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future
  tokens. It allows the model to learn a bidirectional representation of the sentence.
- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base
  model.

This way, the model learns the same inner representation of the English language than its teacher model, while being
faster for inference or downstream tasks.

## Intended uses &amp; limitations

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for
fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

You can use this model directly with a pipeline for masked language modeling:

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='distilbert-base-uncased')
&gt;&gt;&gt; unmasker("Hello I'm a [MASK] model.")

[{'sequence': "[CLS] hello i'm a role model. [SEP]",
  'score': 0.05292855575680733,
  'token': 2535,
  'token_str': 'role'},
 {'sequence': "[CLS] hello i'm a fashion model. [SEP]",
  'score': 0.03968575969338417,
  'token': 4827,
  'token_str': 'fashion'},
 {'sequence': "[CLS] hello i'm a business model. [SEP]",
  'score': 0.034743521362543106,
  'token': 2449,
  'token_str': 'business'},
 {'sequence': "[CLS] hello i'm a model model. [SEP]",
  'score': 0.03462274372577667,
  'token': 2944,
  'token_str': 'model'},
 {'sequence': "[CLS] hello i'm a modeling model. [SEP]",
  'score': 0.018145186826586723,
  'token': 11643,
  'token_str': 'modeling'}]
```

Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import DistilBertTokenizer, DistilBertModel
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained("distilbert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

and in TensorFlow:

```python
from transformers import DistilBertTokenizer, TFDistilBertModel
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

### Limitations and bias

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased
predictions. It also inherits some of
[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='distilbert-base-uncased')
&gt;&gt;&gt; unmasker("The White man worked as a [MASK].")

[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',
  'score': 0.1235365942120552,
  'token': 20987,
  'token_str': 'blacksmith'},
 {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',
  'score': 0.10142576694488525,
  'token': 10533,
  'token_str': 'carpenter'},
 {'sequence': '[CLS] the white man worked as a farmer. [SEP]',
  'score': 0.04985016956925392,
  'token': 7500,
  'token_str': 'farmer'},
 {'sequence': '[CLS] the white man worked as a miner. [SEP]',
  'score': 0.03932540491223335,
  'token': 18594,
  'token_str': 'miner'},
 {'sequence': '[CLS] the white man worked as a butcher. [SEP]',
  'score': 0.03351764753460884,
  'token': 14998,
  'token_str': 'butcher'}]

&gt;&gt;&gt; unmasker("The Black woman worked as a [MASK].")

[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',
  'score': 0.13283951580524445,
  'token': 13877,
  'token_str': 'waitress'},
 {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',
  'score': 0.12586183845996857,
  'token': 6821,
  'token_str': 'nurse'},
 {'sequence': '[CLS] the black woman worked as a maid. [SEP]',
  'score': 0.11708822101354599,
  'token': 10850,
  'token_str': 'maid'},
 {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',
  'score': 0.11499975621700287,
  'token': 19215,
  'token_str': 'prostitute'},
 {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',
  'score': 0.04722772538661957,
  'token': 22583,
  'token_str': 'housekeeper'}]
```

This bias will also affect all fine-tuned versions of this model.

## Training data

DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset
consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)
(excluding lists, tables and headers).

## Training procedure

### Preprocessing

The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are
then of the form:

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.

The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

### Pretraining

The model was trained on 8 16 GB V100 for 90 hours. See the
[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters
details.

## Evaluation results

When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |


### BibTeX entry and citation info

```bibtex
@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}
```

&lt;a href="https://huggingface.co/exbert/?model=distilbert-base-uncased"&gt;
	&lt;img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"&gt;
&lt;/a&gt;
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="10"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">FacebookAI/roberta-base</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">FacebookAI</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-19T12:39:28"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">13,634,529</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">324</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "rust",
  "safetensors",
  "roberta",
  "fill-mask",
  "exbert",
  "en",
  "dataset:bookcorpus",
  "dataset:wikipedia",
  "arxiv:1907.11692",
  "arxiv:1806.02847",
  "license:mit",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">fill-mask</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
tags:
- exbert
license: mit
datasets:
- bookcorpus
- wikipedia
---

# RoBERTa base model

Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
[this paper](https://arxiv.org/abs/1907.11692) and first released in
[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it
makes a difference between english and English.

Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by
the Hugging Face team.

## Model description

RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. 

More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model
randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict
the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one
after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to
learn a bidirectional representation of the sentence.

This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.

## Intended uses &amp; limitations

You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.
See the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that
interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at a model like GPT2.

### How to use

You can use this model directly with a pipeline for masked language modeling:

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='roberta-base')
&gt;&gt;&gt; unmasker("Hello I'm a &lt;mask&gt; model.")

[{'sequence': "&lt;s&gt;Hello I'm a male model.&lt;/s&gt;",
  'score': 0.3306540250778198,
  'token': 2943,
  'token_str': 'Ġmale'},
 {'sequence': "&lt;s&gt;Hello I'm a female model.&lt;/s&gt;",
  'score': 0.04655390977859497,
  'token': 2182,
  'token_str': 'Ġfemale'},
 {'sequence': "&lt;s&gt;Hello I'm a professional model.&lt;/s&gt;",
  'score': 0.04232972860336304,
  'token': 2038,
  'token_str': 'Ġprofessional'},
 {'sequence': "&lt;s&gt;Hello I'm a fashion model.&lt;/s&gt;",
  'score': 0.037216778844594955,
  'token': 2734,
  'token_str': 'Ġfashion'},
 {'sequence': "&lt;s&gt;Hello I'm a Russian model.&lt;/s&gt;",
  'score': 0.03253649175167084,
  'token': 1083,
  'token_str': 'ĠRussian'}]
```

Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import RobertaTokenizer, RobertaModel
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

and in TensorFlow:

```python
from transformers import RobertaTokenizer, TFRobertaModel
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = TFRobertaModel.from_pretrained('roberta-base')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

### Limitations and bias

The training data used for this model contains a lot of unfiltered content from the internet, which is far from
neutral. Therefore, the model can have biased predictions:

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='roberta-base')
&gt;&gt;&gt; unmasker("The man worked as a &lt;mask&gt;.")

[{'sequence': '&lt;s&gt;The man worked as a mechanic.&lt;/s&gt;',
  'score': 0.08702439814805984,
  'token': 25682,
  'token_str': 'Ġmechanic'},
 {'sequence': '&lt;s&gt;The man worked as a waiter.&lt;/s&gt;',
  'score': 0.0819653645157814,
  'token': 38233,
  'token_str': 'Ġwaiter'},
 {'sequence': '&lt;s&gt;The man worked as a butcher.&lt;/s&gt;',
  'score': 0.073323555290699,
  'token': 32364,
  'token_str': 'Ġbutcher'},
 {'sequence': '&lt;s&gt;The man worked as a miner.&lt;/s&gt;',
  'score': 0.046322137117385864,
  'token': 18678,
  'token_str': 'Ġminer'},
 {'sequence': '&lt;s&gt;The man worked as a guard.&lt;/s&gt;',
  'score': 0.040150221437215805,
  'token': 2510,
  'token_str': 'Ġguard'}]

&gt;&gt;&gt; unmasker("The Black woman worked as a &lt;mask&gt;.")

[{'sequence': '&lt;s&gt;The Black woman worked as a waitress.&lt;/s&gt;',
  'score': 0.22177888453006744,
  'token': 35698,
  'token_str': 'Ġwaitress'},
 {'sequence': '&lt;s&gt;The Black woman worked as a prostitute.&lt;/s&gt;',
  'score': 0.19288744032382965,
  'token': 36289,
  'token_str': 'Ġprostitute'},
 {'sequence': '&lt;s&gt;The Black woman worked as a maid.&lt;/s&gt;',
  'score': 0.06498628109693527,
  'token': 29754,
  'token_str': 'Ġmaid'},
 {'sequence': '&lt;s&gt;The Black woman worked as a secretary.&lt;/s&gt;',
  'score': 0.05375480651855469,
  'token': 2971,
  'token_str': 'Ġsecretary'},
 {'sequence': '&lt;s&gt;The Black woman worked as a nurse.&lt;/s&gt;',
  'score': 0.05245552211999893,
  'token': 9008,
  'token_str': 'Ġnurse'}]
```

This bias will also affect all fine-tuned versions of this model.

## Training data

The RoBERTa model was pretrained on the reunion of five datasets:
- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;
- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;
- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news
  articles crawled between September 2016 and February 2019.
- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to
  train GPT-2,
- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the
  story-like style of Winograd schemas.

Together these datasets weigh 160GB of text.

## Training procedure

### Preprocessing

The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of
the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked
with `&lt;s&gt;` and the end of one by `&lt;/s&gt;`

The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `&lt;mask&gt;`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

Contrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).

### Pretraining

The model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The
optimizer used is Adam with a learning rate of 6e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\) and
\\(\epsilon = 1e-6\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning
rate after.

## Evaluation results

When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |


### BibTeX entry and citation info

```bibtex
@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

&lt;a href="https://huggingface.co/exbert/?model=roberta-base"&gt;
	&lt;img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"&gt;
&lt;/a&gt;
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="11"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-transformers/all-mpnet-base-v2</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-03-27T09:46:22"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">13,139,589</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">629</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "sentence-transformers",
  "pytorch",
  "safetensors",
  "mpnet",
  "fill-mask",
  "feature-extraction",
  "sentence-similarity",
  "transformers",
  "en",
  "dataset:s2orc",
  "dataset:flax-sentence-embeddings/stackexchange_xml",
  "dataset:ms_marco",
  "dataset:gooaq",
  "dataset:yahoo_answers_topics",
  "dataset:code_search_net",
  "dataset:search_qa",
  "dataset:eli5",
  "dataset:snli",
  "dataset:multi_nli",
  "dataset:wikihow",
  "dataset:natural_questions",
  "dataset:trivia_qa",
  "dataset:embedding-data/sentence-compression",
  "dataset:embedding-data/flickr30k-captions",
  "dataset:embedding-data/altlex",
  "dataset:embedding-data/simple-wiki",
  "dataset:embedding-data/QQP",
  "dataset:embedding-data/SPECTER",
  "dataset:embedding-data/PAQ_pairs",
  "dataset:embedding-data/WikiAnswers",
  "arxiv:1904.06472",
  "arxiv:2102.07033",
  "arxiv:2104.08727",
  "arxiv:1704.05179",
  "arxiv:1810.09305",
  "license:apache-2.0",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">sentence-similarity</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
license: apache-2.0
library_name: sentence-transformers
tags:
- sentence-transformers
- feature-extraction
- sentence-similarity
- transformers
datasets:
- s2orc
- flax-sentence-embeddings/stackexchange_xml
- ms_marco
- gooaq
- yahoo_answers_topics
- code_search_net
- search_qa
- eli5
- snli
- multi_nli
- wikihow
- natural_questions
- trivia_qa
- embedding-data/sentence-compression
- embedding-data/flickr30k-captions
- embedding-data/altlex
- embedding-data/simple-wiki
- embedding-data/QQP
- embedding-data/SPECTER
- embedding-data/PAQ_pairs
- embedding-data/WikiAnswers
pipeline_tag: sentence-similarity
---


# all-mpnet-base-v2
This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.

## Usage (Sentence-Transformers)
Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:

```
pip install -U sentence-transformers
```

Then you can use the model like this:
```python
from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
embeddings = model.encode(sentences)
print(embeddings)
```

## Usage (HuggingFace Transformers)
Without [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

```python
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Sentences we want sentence embeddings for
sentences = ['This is an example sentence', 'Each sentence is converted']

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')
model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')

# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

# Perform pooling
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])

# Normalize embeddings
sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)

print("Sentence embeddings:")
print(sentence_embeddings)
```

## Evaluation Results

For an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-mpnet-base-v2)

------

## Background

The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised 
contrastive learning objective. We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.

We developped this model during the 
[Community week using JAX/Flax for NLP &amp; CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), 
organized by Hugging Face. We developped this model as part of the project:
[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.

## Intended uses

Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.

By default, input text longer than 384 word pieces is truncated.


## Training procedure

### Pre-training 

We use the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.

### Fine-tuning 

We fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.
We then apply the cross entropy loss by comparing with true pairs.

#### Hyper parameters

We trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).
We use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with
a 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.

#### Training data

We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.


| Dataset                                                  | Paper                                    | Number of training tuples  |
|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|
| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |
| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |
| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |
| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |
| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |
| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |
| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |
| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |
| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |
| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|
| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |
| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |
| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |
| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |
| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | 
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |
| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |
| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |
| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |
| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |
| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |
| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |
| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |
| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |
| **Total** | | **1,170,060,424** |</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="12"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">lanwuwei/GigaBERT-v4-Arabic-and-English</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">lanwuwei</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2021-05-19T21:19:13"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">13,057,217</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">3</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "jax",
  "bert",
  "feature-extraction",
  "endpoints_compatible",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">feature-extraction</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">## GigaBERT-v4
GigaBERT-v4 is a continued pre-training of [GigaBERT-v3](https://huggingface.co/lanwuwei/GigaBERT-v3-Arabic-and-English) on code-switched data, showing improved zero-shot transfer performance from English to Arabic on information extraction (IE) tasks. More details can be found in the following paper:

	@inproceedings{lan2020gigabert,
	  author     = {Lan, Wuwei and Chen, Yang and Xu, Wei and Ritter, Alan},
  	  title      = {GigaBERT: Zero-shot Transfer Learning from English to Arabic},
  	  booktitle  = {Proceedings of The 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
  	  year       = {2020}
  	} 

## Download
```
from transformers import *
tokenizer = BertTokenizer.from_pretrained("lanwuwei/GigaBERT-v4-Arabic-and-English", do_lower_case=True)
model = BertForTokenClassification.from_pretrained("lanwuwei/GigaBERT-v4-Arabic-and-English")
```
Here is downloadable link [GigaBERT-v4](https://drive.google.com/drive/u/1/folders/1uFGzMuTOD7iNsmKQYp_zVuvsJwOaIdar).

</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="13"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">smilegate-ai/kor_unsmile</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">smilegate-ai</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-28T01:34:57"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10,604,648</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">7</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "bert",
  "text-classification",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">text-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-28T01:03:23"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">Entry not found</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="14"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">microsoft/layoutlmv3-base</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">microsoft</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-04-12T12:49:21"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">10,196,485</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">252</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "onnx",
  "layoutlmv3",
  "en",
  "arxiv:2204.08387",
  "license:cc-by-nc-sa-4.0",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><div class="text-right text-gray-400">null</div></div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-04-18T06:53:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
license: cc-by-nc-sa-4.0

---
# LayoutLMv3

[Microsoft Document AI](https://www.microsoft.com/en-us/research/project/document-ai/) | [GitHub](https://aka.ms/layoutlmv3)

## Model description

LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.

[LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387)
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, ACM Multimedia 2022.

## Citation

If you find LayoutLM useful in your research, please cite the following paper:

```
@inproceedings{huang2022layoutlmv3,
  author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},
  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  year={2022}
}
```

## License

The content of this project itself is licensed under the [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/).
Portions of the source code are based on the [transformers](https://github.com/huggingface/transformers) project.
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="15"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai/clip-vit-base-patch16</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-10-04T09:42:28"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">9,194,017</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">65</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "jax",
  "clip",
  "zero-shot-image-classification",
  "vision",
  "arxiv:2103.00020",
  "arxiv:1908.04913",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">zero-shot-image-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
tags:
- vision
widget:
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png
  candidate_labels: playing music, playing sports
  example_title: Cat &amp; Dog
---
# Model Card: CLIP
Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).


## Model Details
The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.


### Model Date
January 2021


### Model Type
The base model uses a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.

The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.


### Documents
- [Blog Post](https://openai.com/blog/clip/)
- [CLIP Paper](https://arxiv.org/abs/2103.00020)


### Use with Transformers
```python3
from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities
```


## Model Use

### Intended Use
The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.


#### Primary intended uses
The primary intended users of these models are AI researchers.
We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.


### Out-of-Scope Use Cases
**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. 
Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.
Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.


## Data
The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.


### Data Mission Statement
Our goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.


## Performance and Limitations


### Performance
We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:
- Food101
- CIFAR10   
- CIFAR100   
- Birdsnap
- SUN397
- Stanford Cars
- FGVC Aircraft
- VOC2007
- DTD
- Oxford-IIIT Pet dataset
- Caltech101
- Flowers102
- MNIST   
- SVHN 
- IIIT5K   
- Hateful Memes   
- SST-2
- UCF101
- Kinetics700
- Country211
- CLEVR Counting
- KITTI Distance
- STL-10
- RareAct
- Flickr30
- MSCOCO
- ImageNet
- ImageNet-A
- ImageNet-R
- ImageNet Sketch
- ObjectNet (ImageNet Overlap)
- Youtube-BB
- ImageNet-Vid


## Limitations
CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.


### Bias and Fairness
We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).
We also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy &gt;96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.


## Feedback


### Where to send questions or comments about the model
Please use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="16"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">facebook/wav2vec2-base-960h</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">facebook</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-11-14T21:37:23"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">9,109,343</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">224</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "safetensors",
  "wav2vec2",
  "automatic-speech-recognition",
  "audio",
  "hf-asr-leaderboard",
  "en",
  "dataset:librispeech_asr",
  "arxiv:2006.11477",
  "license:apache-2.0",
  "model-index",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">automatic-speech-recognition</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
datasets:
- librispeech_asr
tags:
- audio
- automatic-speech-recognition
- hf-asr-leaderboard
license: apache-2.0
widget:
- example_title: Librispeech sample 1
  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac
- example_title: Librispeech sample 2
  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac
model-index:
- name: wav2vec2-base-960h
  results:
  - task:
      name: Automatic Speech Recognition
      type: automatic-speech-recognition
    dataset:
      name: LibriSpeech (clean)
      type: librispeech_asr
      config: clean
      split: test
      args: 
        language: en
    metrics:
    - name: Test WER
      type: wer
      value: 3.4
  - task:
      name: Automatic Speech Recognition
      type: automatic-speech-recognition
    dataset:
      name: LibriSpeech (other)
      type: librispeech_asr
      config: other
      split: test
      args: 
        language: en
    metrics:
    - name: Test WER
      type: wer
      value: 8.6
---

# Wav2Vec2-Base-960h

[Facebook's Wav2Vec2](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)

The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model
make sure that your speech input is also sampled at 16Khz.

[Paper](https://arxiv.org/abs/2006.11477)

Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli

**Abstract**

We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.

The original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.


# Usage

To transcribe audio files the model can be used as a standalone acoustic model as follows:

```python
 from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
 from datasets import load_dataset
 import torch
 
 # load model and tokenizer
 processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
 model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
     
 # load dummy dataset and read soundfiles
 ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation")
 
 # tokenize
 input_values = processor(ds[0]["audio"]["array"], return_tensors="pt", padding="longest").input_values  # Batch size 1
 
 # retrieve logits
 logits = model(input_values).logits
 
 # take argmax and decode
 predicted_ids = torch.argmax(logits, dim=-1)
 transcription = processor.batch_decode(predicted_ids)
 ```
 
 ## Evaluation
 
 This code snippet shows how to evaluate **facebook/wav2vec2-base-960h** on LibriSpeech's "clean" and "other" test data.
 
```python
from datasets import load_dataset
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch
from jiwer import wer


librispeech_eval = load_dataset("librispeech_asr", "clean", split="test")

model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h").to("cuda")
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")

def map_to_pred(batch):
    input_values = processor(batch["audio"]["array"], return_tensors="pt", padding="longest").input_values
    with torch.no_grad():
        logits = model(input_values.to("cuda")).logits

    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)
    batch["transcription"] = transcription
    return batch

result = librispeech_eval.map(map_to_pred, batched=True, batch_size=1, remove_columns=["audio"])

print("WER:", wer(result["text"], result["transcription"]))
```

*Result (WER)*:

| "clean" | "other" |
|---|---|
| 3.4 | 8.6 |</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="17"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai/clip-vit-base-patch32</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-29T09:45:55"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">9,064,050</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">350</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "clip",
  "zero-shot-image-classification",
  "vision",
  "arxiv:2103.00020",
  "arxiv:1908.04913",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">zero-shot-image-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
tags:
- vision
widget:
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png
  candidate_labels: playing music, playing sports
  example_title: Cat &amp; Dog
---

# Model Card: CLIP

Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).

## Model Details

The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.

### Model Date

January 2021

### Model Type

The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. 

The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.


### Documents

- [Blog Post](https://openai.com/blog/clip/)
- [CLIP Paper](https://arxiv.org/abs/2103.00020)


### Use with Transformers

```python3
from PIL import Image
import requests

from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities
```


## Model Use

### Intended Use

The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.

#### Primary intended uses

The primary intended users of these models are AI researchers.

We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.

### Out-of-Scope Use Cases

**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. 

Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.

Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.



## Data

The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.

### Data Mission Statement

Our goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.



## Performance and Limitations

### Performance

We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:

- Food101
- CIFAR10   
- CIFAR100   
- Birdsnap
- SUN397
- Stanford Cars
- FGVC Aircraft
- VOC2007
- DTD
- Oxford-IIIT Pet dataset
- Caltech101
- Flowers102
- MNIST   
- SVHN 
- IIIT5K   
- Hateful Memes   
- SST-2
- UCF101
- Kinetics700
- Country211
- CLEVR Counting
- KITTI Distance
- STL-10
- RareAct
- Flickr30
- MSCOCO
- ImageNet
- ImageNet-A
- ImageNet-R
- ImageNet Sketch
- ObjectNet (ImageNet Overlap)
- Youtube-BB
- ImageNet-Vid

## Limitations

CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.

### Bias and Fairness

We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).

We also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy &gt;96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.



## Feedback

### Where to send questions or comments about the model

Please use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="18"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">facebook/contriever</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">facebook</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-01-19T17:23:28"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">8,397,436</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">43</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "bert",
  "arxiv:2112.09118",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><div class="text-right text-gray-400">null</div></div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">This model has been trained without supervision following the approach described in [Towards Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118). The associated GitHub repository is available here https://github.com/facebookresearch/contriever.

## Usage (HuggingFace Transformers)
Using the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding.

```python
import torch
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')
model = AutoModel.from_pretrained('facebook/contriever')

sentences = [
    "Where was Marie Curie born?",
    "Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.",
    "Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace."
]

# Apply tokenizer
inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
outputs = model(**inputs)

# Mean pooling
def mean_pooling(token_embeddings, mask):
    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)
    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]
    return sentence_embeddings
embeddings = mean_pooling(outputs[0], inputs['attention_mask'])
```</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="19"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai-community/gpt2</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai-community</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-19T10:57:45"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">7,787,873</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">1,791</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "tflite",
  "rust",
  "onnx",
  "safetensors",
  "gpt2",
  "text-generation",
  "exbert",
  "en",
  "doi:10.57967/hf/0039",
  "license:mit",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "text-generation-inference",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">text-generation</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
tags:
- exbert

license: mit
---


# GPT-2

Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large

Pretrained model on English language using a causal language modeling (CLM) objective. It was introduced in
[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
and first released at [this page](https://openai.com/blog/better-language-models/).

Disclaimer: The team releasing GPT-2 also wrote a
[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card
has been written by the Hugging Face team to complete the information they provided and give specific examples of bias.

## Model description

GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This
means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots
of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,
it was trained to guess the next word in sentences.

More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,
shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the
predictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.

This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a
prompt.

This is the **smallest** version of GPT-2, with 124M parameters. 

**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)

## Intended uses &amp; limitations

You can use the raw model for text generation or fine-tune it to a downstream task. See the
[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.

### How to use

You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:

```python
&gt;&gt;&gt; from transformers import pipeline, set_seed
&gt;&gt;&gt; generator = pipeline('text-generation', model='gpt2')
&gt;&gt;&gt; set_seed(42)
&gt;&gt;&gt; generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)

[{'generated_text': "Hello, I'm a language model, a language for thinking, a language for expressing thoughts."},
 {'generated_text': "Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don"},
 {'generated_text': "Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help"},
 {'generated_text': "Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly"},
 {'generated_text': 'Hello, I\'m a language model, not a language model"\n\nThe concept of "no-tricks" comes in handy later with new'}]
```

Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

and in TensorFlow:

```python
from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = TFGPT2Model.from_pretrained('gpt2')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

### Limitations and bias

The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of
unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their
[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):

&gt; Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases
&gt; that require the generated text to be true.
&gt;
&gt; Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do
&gt; not recommend that they be deployed into systems that interact with humans &gt; unless the deployers first carry out a
&gt; study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,
&gt; and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar
&gt; levels of caution around use cases that are sensitive to biases around human attributes.

Here's an example of how the model can have biased predictions:

```python
&gt;&gt;&gt; from transformers import pipeline, set_seed
&gt;&gt;&gt; generator = pipeline('text-generation', model='gpt2')
&gt;&gt;&gt; set_seed(42)
&gt;&gt;&gt; generator("The White man worked as a", max_length=10, num_return_sequences=5)

[{'generated_text': 'The White man worked as a mannequin for'},
 {'generated_text': 'The White man worked as a maniser of the'},
 {'generated_text': 'The White man worked as a bus conductor by day'},
 {'generated_text': 'The White man worked as a plumber at the'},
 {'generated_text': 'The White man worked as a journalist. He had'}]

&gt;&gt;&gt; set_seed(42)
&gt;&gt;&gt; generator("The Black man worked as a", max_length=10, num_return_sequences=5)

[{'generated_text': 'The Black man worked as a man at a restaurant'},
 {'generated_text': 'The Black man worked as a car salesman in a'},
 {'generated_text': 'The Black man worked as a police sergeant at the'},
 {'generated_text': 'The Black man worked as a man-eating monster'},
 {'generated_text': 'The Black man worked as a slave, and was'}]
```

This bias will also affect all fine-tuned versions of this model.

## Training data

The OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web
pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from
this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights
40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText
[here](https://github.com/openai/gpt-2/blob/master/domains.txt).

## Training procedure

### Preprocessing

The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a
vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.

The larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact
details of training.

## Evaluation results

The model achieves the following results without any fine-tuning (zero-shot):

| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |
|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|
| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |
|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |


### BibTeX entry and citation info

```bibtex
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
```

&lt;a href="https://huggingface.co/exbert/?model=gpt2"&gt;
	&lt;img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"&gt;
&lt;/a&gt;
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="20"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">distilbert/distilbert-base-uncased-finetuned-sst-2-english</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">distilbert</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-12-19T16:29:37"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">7,759,260</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">441</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "rust",
  "onnx",
  "safetensors",
  "distilbert",
  "text-classification",
  "en",
  "dataset:sst2",
  "dataset:glue",
  "arxiv:1910.01108",
  "doi:10.57967/hf/0181",
  "license:apache-2.0",
  "model-index",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">text-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
license: apache-2.0
datasets:
- sst2
- glue
model-index:
- name: distilbert-base-uncased-finetuned-sst-2-english
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: glue
      type: glue
      config: sst2
      split: validation
    metrics:
    - type: accuracy
      value: 0.9105504587155964
      name: Accuracy
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg
    - type: precision
      value: 0.8978260869565218
      name: Precision
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA
    - type: recall
      value: 0.9301801801801802
      name: Recall
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ
    - type: auc
      value: 0.9716626673402374
      name: AUC
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ
    - type: f1
      value: 0.9137168141592922
      name: F1
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA
    - type: loss
      value: 0.39013850688934326
      name: loss
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: sst2
      type: sst2
      config: default
      split: train
    metrics:
    - type: accuracy
      value: 0.9885521685548412
      name: Accuracy
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA
    - type: precision
      value: 0.9881965062029833
      name: Precision Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw
    - type: precision
      value: 0.9885521685548412
      name: Precision Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ
    - type: precision
      value: 0.9885639626373408
      name: Precision Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg
    - type: recall
      value: 0.9886145346602994
      name: Recall Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA
    - type: recall
      value: 0.9885521685548412
      name: Recall Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ
    - type: recall
      value: 0.9885521685548412
      name: Recall Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw
    - type: f1
      value: 0.9884019815052447
      name: F1 Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ
    - type: f1
      value: 0.9885521685548412
      name: F1 Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ
    - type: f1
      value: 0.9885546181087554
      name: F1 Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA
    - type: loss
      value: 0.040652573108673096
      name: loss
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg
---

# DistilBERT base uncased finetuned SST-2

## Table of Contents
- [Model Details](#model-details)
- [How to Get Started With the Model](#how-to-get-started-with-the-model)
- [Uses](#uses)
- [Risks, Limitations and Biases](#risks-limitations-and-biases)
- [Training](#training)

## Model Details
**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.
This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).
- **Developed by:** Hugging Face
- **Model Type:** Text Classification
- **Language(s):** English
- **License:** Apache-2.0
- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).
- **Resources for more information:**
    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)
    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)

## How to Get Started With the Model

Example of single-label classification:
​​
```python
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]

```

## Uses

#### Direct Use

This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.

#### Misuse and Out-of-scope Use
The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.


## Risks, Limitations and Biases

Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.

For instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aurélien Géron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.

&lt;img src="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg" alt="Map of positive probabilities per country." width="500"/&gt;

We strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).



# Training


#### Training Data


The authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.

#### Training Procedure

###### Fine-tuning hyper-parameters


- learning_rate = 1e-5
- batch_size = 32
- warmup = 600
- max_seq_length = 128
- num_train_epochs = 3.0


</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="21"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">distilbert/distilbert-base-multilingual-cased</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">distilbert</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-04-06T13:40:24"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">7,318,730</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">103</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "onnx",
  "safetensors",
  "distilbert",
  "fill-mask",
  "multilingual",
  "af",
  "sq",
  "ar",
  "an",
  "hy",
  "ast",
  "az",
  "ba",
  "eu",
  "bar",
  "be",
  "bn",
  "inc",
  "bs",
  "br",
  "bg",
  "my",
  "ca",
  "ceb",
  "ce",
  "zh",
  "cv",
  "hr",
  "cs",
  "da",
  "nl",
  "en",
  "et",
  "fi",
  "fr",
  "gl",
  "ka",
  "de",
  "el",
  "gu",
  "ht",
  "he",
  "hi",
  "hu",
  "is",
  "io",
  "id",
  "ga",
  "it",
  "ja",
  "jv",
  "kn",
  "kk",
  "ky",
  "ko",
  "la",
  "lv",
  "lt",
  "roa",
  "nds",
  "lm",
  "mk",
  "mg",
  "ms",
  "ml",
  "mr",
  "mn",
  "min",
  "ne",
  "new",
  "nb",
  "nn",
  "oc",
  "fa",
  "pms",
  "pl",
  "pt",
  "pa",
  "ro",
  "ru",
  "sco",
  "sr",
  "scn",
  "sk",
  "sl",
  "aze",
  "es",
  "su",
  "sw",
  "sv",
  "tl",
  "tg",
  "th",
  "ta",
  "tt",
  "te",
  "tr",
  "uk",
  "ud",
  "uz",
  "vi",
  "vo",
  "war",
  "cy",
  "fry",
  "pnb",
  "yo",
  "dataset:wikipedia",
  "arxiv:1910.01108",
  "arxiv:1910.09700",
  "license:apache-2.0",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">fill-mask</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: 
- multilingual
- af
- sq
- ar
- an
- hy
- ast
- az
- ba
- eu
- bar
- be
- bn
- inc
- bs
- br
- bg
- my
- ca
- ceb
- ce
- zh
- cv
- hr
- cs
- da
- nl
- en
- et
- fi
- fr
- gl
- ka
- de
- el
- gu
- ht
- he
- hi
- hu
- is
- io
- id
- ga
- it
- ja
- jv
- kn
- kk
- ky
- ko
- la
- lv
- lt
- roa
- nds
- lm
- mk
- mg
- ms
- ml
- mr
- mn
- min
- ne
- new
- nb
- nn
- oc
- fa
- pms
- pl
- pt
- pa
- ro
- ru
- sco
- sr
- hr
- scn
- sk
- sl
- aze
- es
- su
- sw
- sv
- tl
- tg
- th
- ta
- tt
- te
- tr
- uk
- ud
- uz
- vi
- vo
- war
- cy
- fry
- pnb
- yo
license: apache-2.0
datasets:
- wikipedia
---

# Model Card for DistilBERT base multilingual (cased)

# Table of Contents

1. [Model Details](#model-details)
2. [Uses](#uses)
3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)
4. [Training Details](#training-details)
5. [Evaluation](#evaluation)
6. [Environmental Impact](#environmental-impact)
7. [Citation](#citation)
8. [How To Get Started With the Model](#how-to-get-started-with-the-model)

# Model Details

## Model Description

This model is a distilled version of the [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased/). The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is cased: it does make a difference between english and English.

The model is trained on the concatenation of Wikipedia in 104 different languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).
The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).
On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.

We encourage potential users of this model to check out the [BERT base multilingual model card](https://huggingface.co/bert-base-multilingual-cased) to learn more about usage, limitations and potential biases.

- **Developed by:** Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face)
- **Model type:** Transformer-based language model
- **Language(s) (NLP):** 104 languages; see full list [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)
- **License:** Apache 2.0
- **Related Models:** [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased)
- **Resources for more information:** 
  - [GitHub Repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)
  - [Associated Paper](https://arxiv.org/abs/1910.01108)

# Uses

## Direct Use and Downstream Use

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.

## Out of Scope Use

The model should not be used to intentionally create hostile or alienating environments for people. The model was not trained to be factual or true representations of people or events, and therefore using the models to generate such content is out-of-scope for the abilities of this model.

# Bias, Risks, and Limitations

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.

## Recommendations

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.

# Training Details

- The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages
- The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.
- Further information about the training procedure and data is included in the [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) model card.

# Evaluation

The model developers report the following accuracy results for DistilmBERT (see [GitHub Repo](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)): 

&gt; Here are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):

| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |
| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|
| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |
| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |
| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |

# Environmental Impact

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** More information needed
- **Hours used:** More information needed
- **Cloud Provider:** More information needed
- **Compute Region:** More information needed
- **Carbon Emitted:** More information needed

# Citation

```bibtex
@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}
```

APA
- Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

# How to Get Started With the Model

You can use the model directly with a pipeline for masked language modeling: 

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')
&gt;&gt;&gt; unmasker("Hello I'm a [MASK] model.")

[{'score': 0.040800247341394424,
  'sequence': "Hello I'm a virtual model.",
  'token': 37859,
  'token_str': 'virtual'},
 {'score': 0.020015988498926163,
  'sequence': "Hello I'm a big model.",
  'token': 22185,
  'token_str': 'big'},
 {'score': 0.018680453300476074,
  'sequence': "Hello I'm a Hello model.",
  'token': 31178,
  'token_str': 'Hello'},
 {'score': 0.017396586015820503,
  'sequence': "Hello I'm a model model.",
  'token': 13192,
  'token_str': 'model'},
 {'score': 0.014229810796678066,
  'sequence': "Hello I'm a perfect model.",
  'token': 43477,
  'token_str': 'perfect'}]
```
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="22"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai/clip-vit-large-patch14-336</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">openai</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-10-04T09:41:39"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">6,928,591</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">108</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "clip",
  "zero-shot-image-classification",
  "generated_from_keras_callback",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">zero-shot-image-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-04-22T14:57:43"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
tags:
- generated_from_keras_callback
widget:
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png
  candidate_labels: playing music, playing sports
  example_title: Cat &amp; Dog
model-index:
- name: clip-vit-large-patch14-336
  results: []
---

&lt;!-- This model card has been generated automatically according to the information Keras had access to. You should
probably proofread and complete it, then remove this comment. --&gt;

# clip-vit-large-patch14-336

This model was trained from scratch on an unknown dataset.
It achieves the following results on the evaluation set:


## Model description

More information needed

## Intended uses &amp; limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- optimizer: None
- training_precision: float32

### Training results



### Framework versions

- Transformers 4.21.3
- TensorFlow 2.8.2
- Tokenizers 0.12.1
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="23"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">FacebookAI/xlm-roberta-base</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">FacebookAI</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-19T12:48:21"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">6,755,596</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">486</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "onnx",
  "safetensors",
  "xlm-roberta",
  "fill-mask",
  "exbert",
  "multilingual",
  "af",
  "am",
  "ar",
  "as",
  "az",
  "be",
  "bg",
  "bn",
  "br",
  "bs",
  "ca",
  "cs",
  "cy",
  "da",
  "de",
  "el",
  "en",
  "eo",
  "es",
  "et",
  "eu",
  "fa",
  "fi",
  "fr",
  "fy",
  "ga",
  "gd",
  "gl",
  "gu",
  "ha",
  "he",
  "hi",
  "hr",
  "hu",
  "hy",
  "id",
  "is",
  "it",
  "ja",
  "jv",
  "ka",
  "kk",
  "km",
  "kn",
  "ko",
  "ku",
  "ky",
  "la",
  "lo",
  "lt",
  "lv",
  "mg",
  "mk",
  "ml",
  "mn",
  "mr",
  "ms",
  "my",
  "ne",
  "nl",
  "no",
  "om",
  "or",
  "pa",
  "pl",
  "ps",
  "pt",
  "ro",
  "ru",
  "sa",
  "sd",
  "si",
  "sk",
  "sl",
  "so",
  "sq",
  "sr",
  "su",
  "sv",
  "sw",
  "ta",
  "te",
  "th",
  "tl",
  "tr",
  "ug",
  "uk",
  "ur",
  "uz",
  "vi",
  "xh",
  "yi",
  "zh",
  "arxiv:1911.02116",
  "license:mit",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">fill-mask</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:04"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
tags:
- exbert
language: 
- multilingual
- af
- am
- ar
- as
- az
- be
- bg
- bn
- br
- bs
- ca
- cs
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- fy
- ga
- gd
- gl
- gu
- ha
- he
- hi
- hr
- hu
- hy
- id
- is
- it
- ja
- jv
- ka
- kk
- km
- kn
- ko
- ku
- ky
- la
- lo
- lt
- lv
- mg
- mk
- ml
- mn
- mr
- ms
- my
- ne
- nl
- no
- om
- or
- pa
- pl
- ps
- pt
- ro
- ru
- sa
- sd
- si
- sk
- sl
- so
- sq
- sr
- su
- sv
- sw
- ta
- te
- th
- tl
- tr
- ug
- uk
- ur
- uz
- vi
- xh
- yi
- zh
license: mit
---

# XLM-RoBERTa (base-sized model) 

XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). 

Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.

## Model description

XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. 

RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.

More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.

This way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.

## Intended uses &amp; limitations

You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.

## Usage

You can use this model directly with a pipeline for masked language modeling:

```python
&gt;&gt;&gt; from transformers import pipeline
&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='xlm-roberta-base')
&gt;&gt;&gt; unmasker("Hello I'm a &lt;mask&gt; model.")

[{'score': 0.10563907772302628,
  'sequence': "Hello I'm a fashion model.",
  'token': 54543,
  'token_str': 'fashion'},
 {'score': 0.08015287667512894,
  'sequence': "Hello I'm a new model.",
  'token': 3525,
  'token_str': 'new'},
 {'score': 0.033413201570510864,
  'sequence': "Hello I'm a model model.",
  'token': 3299,
  'token_str': 'model'},
 {'score': 0.030217764899134636,
  'sequence': "Hello I'm a French model.",
  'token': 92265,
  'token_str': 'French'},
 {'score': 0.026436051353812218,
  'sequence': "Hello I'm a sexy model.",
  'token': 17473,
  'token_str': 'sexy'}]
```

Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
model = AutoModelForMaskedLM.from_pretrained("xlm-roberta-base")

# prepare input
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')

# forward pass
output = model(**encoded_input)
```

### BibTeX entry and citation info

```bibtex
@article{DBLP:journals/corr/abs-1911-02116,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```

&lt;a href="https://huggingface.co/exbert/?model=xlm-roberta-base"&gt;
	&lt;img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png"&gt;
&lt;/a&gt;
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="24"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pyannote/segmentation-3.0</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pyannote</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-10-04T18:53:59"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">6,481,292</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">108</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pyannote-audio</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "pyannote-audio",
  "pytorch",
  "pyannote",
  "pyannote-audio-model",
  "audio",
  "voice",
  "speech",
  "speaker",
  "speaker-diarization",
  "speaker-change-detection",
  "speaker-segmentation",
  "voice-activity-detection",
  "overlapped-speech-detection",
  "resegmentation",
  "license:mit",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">voice-activity-detection</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-09-22T12:03:10"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
tags:
- pyannote
- pyannote-audio
- pyannote-audio-model
- audio
- voice
- speech
- speaker
- speaker-diarization
- speaker-change-detection
- speaker-segmentation
- voice-activity-detection
- overlapped-speech-detection
- resegmentation
license: mit
inference: false
extra_gated_prompt: "The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers improve it further. Though this model uses MIT license and will always remain open-source, we will occasionnally email you about premium models and paid services around pyannote."
extra_gated_fields:
  Company/university: text
  Website: text
---

Using this open-source model in production?  
Make the most of it thanks to our [consulting services](https://herve.niderb.fr/consulting.html).

# 🎹 "Powerset" speaker segmentation

This model ingests 10 seconds of mono audio sampled at 16kHz and outputs speaker diarization as a (num_frames, num_classes) matrix where the 7 classes are _non-speech_, _speaker #1_, _speaker #2_, _speaker #3_, _speakers #1 and #2_, _speakers #1 and #3_, and _speakers #2 and #3_.

![Example output](example.png)

```python
# waveform (first row)
duration, sample_rate, num_channels = 10, 16000, 1
waveform = torch.randn(batch_size, num_channels, duration * sample_rate 

# powerset multi-class encoding (second row)
powerset_encoding = model(waveform)

# multi-label encoding (third row)
from pyannote.audio.utils.powerset import Powerset
max_speakers_per_chunk, max_speakers_per_frame = 3, 2
to_multilabel = Powerset(
    max_speakers_per_chunk, 
    max_speakers_per_frame).to_multilabel
multilabel_encoding = to_multilabel(powerset_encoding)
```

The various concepts behind this model are described in details in this [paper](https://www.isca-speech.org/archive/interspeech_2023/plaquet23_interspeech.html).

It has been trained by Séverin Baroudi with [pyannote.audio](https://github.com/pyannote/pyannote-audio) `3.0.0` using the combination of the training sets of AISHELL, AliMeeting, AMI, AVA-AVD, DIHARD, Ego4D, MSDWild, REPERE, and VoxConverse.

This [companion repository](https://github.com/FrenchKrab/IS2023-powerset-diarization/) by [Alexis Plaquet](https://frenchkrab.github.io/) also provides instructions on how to train or finetune such a model on your own data.

## Requirements

1. Install [`pyannote.audio`](https://github.com/pyannote/pyannote-audio) `3.0` with `pip install pyannote.audio`
2. Accept [`pyannote/segmentation-3.0`](https://hf.co/pyannote/segmentation-3.0) user conditions
3. Create access token at [`hf.co/settings/tokens`](https://hf.co/settings/tokens).


## Usage

```python
# instantiate the model
from pyannote.audio import Model
model = Model.from_pretrained(
  "pyannote/segmentation-3.0", 
  use_auth_token="HUGGINGFACE_ACCESS_TOKEN_GOES_HERE")
```

### Speaker diarization

This model cannot be used to perform speaker diarization of full recordings on its own (it only processes 10s chunks). 

See [pyannote/speaker-diarization-3.0](https://hf.co/pyannote/speaker-diarization-3.0) pipeline that uses an additional speaker embedding model to perform full recording speaker diarization.

### Voice activity detection

```python
from pyannote.audio.pipelines import VoiceActivityDetection
pipeline = VoiceActivityDetection(segmentation=model)
HYPER_PARAMETERS = {
  # remove speech regions shorter than that many seconds.
  "min_duration_on": 0.0,
  # fill non-speech regions shorter than that many seconds.
  "min_duration_off": 0.0
}
pipeline.instantiate(HYPER_PARAMETERS)
vad = pipeline("audio.wav")
# `vad` is a pyannote.core.Annotation instance containing speech regions
```

### Overlapped speech detection

```python
from pyannote.audio.pipelines import OverlappedSpeechDetection
pipeline = OverlappedSpeechDetection(segmentation=model)
HYPER_PARAMETERS = {
  # remove overlapped speech regions shorter than that many seconds.
  "min_duration_on": 0.0,
  # fill non-overlapped speech regions shorter than that many seconds.
  "min_duration_off": 0.0
}
pipeline.instantiate(HYPER_PARAMETERS)
osd = pipeline("audio.wav")
# `osd` is a pyannote.core.Annotation instance containing overlapped speech regions
```

## Citations

```bibtex
@inproceedings{Plaquet23,
  author={Alexis Plaquet and Hervé Bredin},
  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
```

```bibtex
@inproceedings{Bredin23,
  author={Hervé Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
```
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="25"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">tsmatz/xlm-roberta-ner-japanese</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">tsmatz</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-09-12T00:26:01"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">6,433,134</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">12</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "xlm-roberta",
  "token-classification",
  "generated_from_trainer",
  "ner",
  "bert",
  "ja",
  "base_model:xlm-roberta-base",
  "license:mit",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">token-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-10-24T02:08:37"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language:
- ja
license: mit
tags:
- generated_from_trainer
- ner
- bert
metrics:
- f1
widget:
- text: 鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った
- text: 中国では、中国共産党による一党統治が続く
base_model: xlm-roberta-base
model-index:
- name: xlm-roberta-ner-ja
  results: []
---

&lt;!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. --&gt;

# xlm-roberta-ner-japanese

(Japanese caption : 日本語の固有表現抽出のモデル)

This model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification.

The model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles.&lt;br&gt;
See [here](https://github.com/stockmarkteam/ner-wikipedia-dataset) for the license of this dataset.

Each token is labeled by :

| Label id | Tag | Tag in Widget | Description |
|---|---|---|---|
| 0 | O | (None) | others or nothing |
| 1 | PER | PER | person |
| 2 | ORG | ORG | general corporation organization |
| 3 | ORG-P | P | political organization |
| 4 | ORG-O | O | other organization |
| 5 | LOC | LOC | location |
| 6 | INS | INS | institution, facility |
| 7 | PRD | PRD | product |
| 8 | EVT | EVT | event |

## Intended uses

```python
from transformers import pipeline

model_name = "tsmatz/xlm-roberta-ner-japanese"
classifier = pipeline("token-classification", model=model_name)
result = classifier("鈴木は4月の陽気の良い日に、鈴をつけて熊本県の阿蘇山に登った")
print(result)
```

## Training procedure

You can download the source code for fine-tuning from [here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb).

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 12
- eval_batch_size: 12
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 5

### Training results

| Training Loss | Epoch | Step | Validation Loss | F1     |
|:-------------:|:-----:|:----:|:---------------:|:------:|
| No log        | 1.0   | 446  | 0.1510          | 0.8457 |
| No log        | 2.0   | 892  | 0.0626          | 0.9261 |
| No log        | 3.0   | 1338 | 0.0366          | 0.9580 |
| No log        | 4.0   | 1784 | 0.0196          | 0.9792 |
| No log        | 5.0   | 2230 | 0.0173          | 0.9864 |


### Framework versions

- Transformers 4.23.1
- Pytorch 1.12.1+cu102
- Datasets 2.6.1
- Tokenizers 0.13.1
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="26"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pyannote/wespeaker-voxceleb-resnet34-LM</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pyannote</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-11-16T12:28:25"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">5,684,778</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">20</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">pyannote-audio</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "pyannote-audio",
  "pytorch",
  "pyannote",
  "pyannote-audio-model",
  "wespeaker",
  "audio",
  "voice",
  "speech",
  "speaker",
  "speaker-recognition",
  "speaker-verification",
  "speaker-identification",
  "speaker-embedding",
  "dataset:voxceleb",
  "license:cc-by-4.0",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto"><div class="text-right text-gray-400">null</div></div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-11-13T15:32:31"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
tags:
  - pyannote
  - pyannote-audio
  - pyannote-audio-model
  - wespeaker
  - audio
  - voice
  - speech
  - speaker
  - speaker-recognition
  - speaker-verification
  - speaker-identification
  - speaker-embedding
datasets:
  - voxceleb
license: cc-by-4.0
inference: false
---

Using this open-source model in production?  
Make the most of it thanks to our [consulting services](https://herve.niderb.fr/consulting.html).

# 🎹 Wrapper around wespeaker-voxceleb-resnet34-LM

This model requires `pyannote.audio` version 3.1 or higher.

This is a wrapper around [WeSpeaker](https://github.com/wenet-e2e/wespeaker) `wespeaker-voxceleb-resnet34-LM` pretrained speaker embedding model, for use in `pyannote.audio`.

## Basic usage

```python
# instantiate pretrained model
from pyannote.audio import Model
model = Model.from_pretrained("pyannote/wespeaker-voxceleb-resnet34-LM")
```

```python
from pyannote.audio import Inference
inference = Inference(model, window="whole")
embedding1 = inference("speaker1.wav")
embedding2 = inference("speaker2.wav")
# `embeddingX` is (1 x D) numpy array extracted from the file as a whole.

from scipy.spatial.distance import cdist
distance = cdist(embedding1, embedding2, metric="cosine")[0,0]
# `distance` is a `float` describing how dissimilar speakers 1 and 2 are.
```

## Advanced usage

### Running on GPU

```python
import torch
inference.to(torch.device("cuda"))
embedding = inference("audio.wav")
```

### Extract embedding from an excerpt

```python
from pyannote.audio import Inference
from pyannote.core import Segment
inference = Inference(model, window="whole")
excerpt = Segment(13.37, 19.81)
embedding = inference.crop("audio.wav", excerpt)
# `embedding` is (1 x D) numpy array extracted from the file excerpt.
```

### Extract embeddings using a sliding window

```python
from pyannote.audio import Inference
inference = Inference(model, window="sliding",
                      duration=3.0, step=1.0)
embeddings = inference("audio.wav")
# `embeddings` is a (N x D) pyannote.core.SlidingWindowFeature
# `embeddings[i]` is the embedding of the ith position of the
# sliding window, i.e. from [i * step, i * step + duration].
```

## License

According to [this page](https://github.com/wenet-e2e/wespeaker/blob/master/docs/pretrained.md):

&gt; The pretrained model in WeNet follows the license of it's corresponding dataset. For example, the pretrained model on VoxCeleb follows Creative Commons Attribution 4.0 International License., since it is used as license of the VoxCeleb dataset, see https://mm.kaist.ac.kr/datasets/voxceleb/.

## Citation

```bibtex
@inproceedings{Wang2023,
  title={Wespeaker: A research and production oriented speaker embedding learning toolkit},
  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},
  booktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
```

```bibtex
@inproceedings{Bredin23,
  author={Hervé Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={1983--1987},
  doi={10.21437/Interspeech.2023-105}
}
```
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="27"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">google/vit-base-patch16-224-in21k</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">google</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-02-05T16:37:39"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">5,555,804</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">127</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "safetensors",
  "vit",
  "image-feature-extraction",
  "vision",
  "dataset:imagenet-21k",
  "arxiv:2010.11929",
  "arxiv:2006.03677",
  "license:apache-2.0",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">image-feature-extraction</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
license: apache-2.0
tags:
- vision
datasets:
- imagenet-21k
inference: false
---

# Vision Transformer (base-sized model) 

Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. 

Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.

## Model description

The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. 

Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.

Note that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).

By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.

## Intended uses &amp; limitations

You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for
fine-tuned versions on a task that interests you.

### How to use

Here is how to use this model in PyTorch:

```python
from transformers import ViTImageProcessor, ViTModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')
model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state
```

Here is how to use this model in JAX/Flax:

```python
from transformers import ViTImageProcessor, FlaxViTModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')
model = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')

inputs = processor(images=image, return_tensors="np")
outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state
```

## Training data

The ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes. 

## Training procedure

### Preprocessing

The exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). 

Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).

### Pretraining

The model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.

## Evaluation results

For evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.

### BibTeX entry and citation info

```bibtex
@misc{wu2020visual,
      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, 
      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
      year={2020},
      eprint={2006.03677},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

```bibtex
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
```</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="28"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">laion/CLIP-ViT-B-32-laion2B-s34B-b79K</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">laion</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2024-01-15T20:33:50"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">5,170,478</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">66</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">open_clip</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "open_clip",
  "pytorch",
  "safetensors",
  "clip",
  "zero-shot-image-classification",
  "arxiv:1910.04867",
  "license:mit",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">zero-shot-image-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-09-14T22:49:28"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
license: mit
widget:
- src: &gt;-
    https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png
  candidate_labels: playing music, playing sports
  example_title: Cat &amp; Dog
pipeline_tag: zero-shot-image-classification
---
# Model Card for CLIP ViT-B/32 - LAION-2B

#  Table of Contents

1. [Model Details](#model-details)
2. [Uses](#uses)
3. [Training Details](#training-details)
4. [Evaluation](#evaluation)
5. [Acknowledgements](#acknowledgements)
6. [Citation](#citation)
7. [How To Get Started With the Model](#how-to-get-started-with-the-model)


# Model Details

## Model Description

A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).

Model training done by Romain Beaumont on the [stability.ai](https://stability.ai/) cluster. 

# Uses

As per the original [OpenAI CLIP model card](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/model-card.md), this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. 

The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. 

## Direct Use

Zero-shot image classification, image and text retrieval, among others.

## Downstream Use

Image classification and other image task fine-tuning, linear probe image classification, image generation guiding and conditioning, among others.

## Out-of-Scope Use

As per the OpenAI models,

**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. 

Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.

Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.

Further the above notice, the LAION-5B dataset used in training of these models has additional considerations, see below.

# Training Details

## Training Data

This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/).

**IMPORTANT NOTE:** The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.

## Training Procedure

Please see [training notes](https://docs.google.com/document/d/1EFbMLRWSSV0LUf9Du1pWzWqgeiIRPwEWX2s1C6mAk5c) and [wandb logs](https://wandb.ai/rom1504/eval_openclip/reports/B-32-2B--VmlldzoyNDkwNDMy).

# Evaluation

Evaluation done with code in the [LAION CLIP Benchmark suite](https://github.com/LAION-AI/CLIP_benchmark).

## Testing Data, Factors &amp; Metrics

### Testing Data

The testing is performed with VTAB+ (A combination of VTAB (https://arxiv.org/abs/1910.04867) w/ additional robustness datasets) for classification and COCO and Flickr for retrieval.

**TODO** - more detail

## Results

The model achieves a 66.6 zero-shot top-1 accuracy on ImageNet-1k.

An initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb

**TODO** - create table for just this model's metrics.

# Acknowledgements

Acknowledging [stability.ai](https://stability.ai/) for the compute used to train this model.

# Citation

**BibTeX:**

In addition to forthcoming LAION-5B (https://laion.ai/blog/laion-5b/) paper, please cite:

OpenAI CLIP paper
```
@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={ICML},
  year={2021}
}
```

OpenCLIP software
```
@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}
```

# How to Get Started with the Model

Use the code below to get started with the model.

** TODO ** - Hugging Face transformers, OpenCLIP, and timm getting started snippets</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="29"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">microsoft/deberta-base</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">microsoft</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-09-26T08:50:43"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">4,985,065</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">60</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "rust",
  "deberta",
  "deberta-v1",
  "fill-mask",
  "en",
  "arxiv:2006.03654",
  "license:mit",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">fill-mask</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
language: en
tags: 
- deberta-v1
- fill-mask
thumbnail: https://huggingface.co/front/thumbnails/microsoft.png
license: mit
---

## DeBERTa: Decoding-enhanced BERT with Disentangled Attention

[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. 

Please check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.


#### Fine-tuning on NLU tasks

We present the dev results on SQuAD 1.1/2.0 and MNLI tasks.

| Model             | SQuAD 1.1 | SQuAD 2.0 | MNLI-m |
|-------------------|-----------|-----------|--------|
| RoBERTa-base      | 91.5/84.6 | 83.7/80.5 | 87.6   |
| XLNet-Large       | -/-       | -/80.2    | 86.8   |
| **DeBERTa-base**  | 93.1/87.2 | 86.2/83.1 | 88.8   |

### Citation

If you find DeBERTa useful for your work, please cite the following paper:

``` latex
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}
```
</div></div></td> </tr><tr class="group cursor-pointer space-x-4 divide-x border-b outline-offset-[-2px] last:border-none odd:bg-gray-50 hover:bg-gray-100 dark:odd:bg-gray-925 dark:hover:bg-gray-850 " tabindex="0" data-row-idx="30"><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">google/vit-base-patch16-224</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">google</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2023-09-05T15:27:12"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">4,944,301</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-right" dir="auto">498</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">transformers</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">[
  "transformers",
  "pytorch",
  "tf",
  "jax",
  "safetensors",
  "vit",
  "image-classification",
  "vision",
  "dataset:imagenet-1k",
  "dataset:imagenet-21k",
  "arxiv:2010.11929",
  "arxiv:2006.03677",
  "license:apache-2.0",
  "autotrain_compatible",
  "endpoints_compatible",
  "has_space",
  "region:us"
]</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">image-classification</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="text-left" dir="auto">"2022-03-02T23:29:05"</div></div></td><td class="min-w-fit max-w-sm break-words p-2 "><div class="line-clamp-2 "><div class="" dir="auto">---
license: apache-2.0
tags:
- vision
- image-classification
datasets:
- imagenet-1k
- imagenet-21k
widget:
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg
  example_title: Tiger
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg
  example_title: Teapot
- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg
  example_title: Palace
---

# Vision Transformer (base-sized model) 

Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. 

Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.

## Model description

The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.

Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.

By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.

## Intended uses &amp; limitations

You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for
fine-tuned versions on a task that interests you.

### How to use

Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:

```python
from transformers import ViTImageProcessor, ViTForImageClassification
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')

inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])
```

For more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).

## Training data

The ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. 

## Training procedure

### Preprocessing

The exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). 

Images are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).

### Pretraining

The model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.

## Evaluation results

For evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.

### BibTeX entry and citation info

```bibtex
@misc{wu2020visual,
      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, 
      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
      year={2020},
      eprint={2006.03677},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

```bibtex
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
```</div></div></td> </tr></tbody></table> <div class="border-t border-dashed border-gray-300 bg-gradient-to-b from-gray-100 to-white py-3 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900">End of preview. <a href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/default/train" class="group"><span class="underline decoration-gray-300 group-hover:decoration-gray-400 dark:decoration-gray-500 dark:group-hover:decoration-gray-300">Expand</span>
						in <svg class="text-xs mr-0.5 inline -translate-y-px text-red-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-primary" d="M21 22H3a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1zM4 20h16V4H4z" fill="currentColor"></path><path class="uim-primary" d="M9 22a1 1 0 0 1-1-1V3a1 1 0 0 1 2 0v18a1 1 0 0 1-1 1zm6 0a1 1 0 0 1-1-1V3a1 1 0 0 1 2 0v18a1 1 0 0 1-1 1z" fill="currentColor"></path><path class="uim-primary" d="M21 10H3a1 1 0 0 1 0-2h18a1 1 0 0 1 0 2zm0 6H3a1 1 0 0 1 0-2h18a1 1 0 0 1 0 2z" fill="currentColor"></path></svg>Dataset Viewer.</a></div></div> <nav><ul class="flex select-none items-center justify-between space-x-2 text-gray-700 sm:justify-center border-t border-dashed border-gray-300 bg-gradient-to-b from-gray-100 to-white py-1 text-center font-mono text-xs dark:border-gray-700 dark:from-gray-950 dark:to-gray-900 rounded-b-lg"><li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 pointer-events-none cursor-default text-gray-400 hover:text-gray-700" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg> Previous</a></li> <li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1 bg-gray-50 font-semibold ring-1 ring-inset ring-gray-200 dark:bg-gray-900 dark:text-yellow-500 dark:ring-gray-900 hover:bg-gray-50 dark:hover:bg-gray-800" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/default/train?p=0">1</a></li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/default/train?p=1">2</a></li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/default/train?p=2">3</a></li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  pointer-events-none cursor-default" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#">...</a></li><li class="hidden sm:block"><a class="rounded-lg px-2.5 py-1  hover:bg-gray-50 dark:hover:bg-gray-800" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/default/train?p=5813">5,814</a></li> <li><a class="flex items-center rounded-lg px-2.5 py-1 hover:bg-gray-50 dark:hover:bg-gray-800 " href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/viewer/default/train?p=1">Next <svg class="ml-1.5 transform rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M10 16L20 6l1.4 1.4l-8.6 8.6l8.6 8.6L20 26z" fill="currentColor"></path></svg></a></li></ul></nav></div></div></div>
				
					<div class="SVELTE_HYDRATER contents" data-target="RepoCodeCopy" data-props="{}"><div></div></div>
					

					<div class="SVELTE_HYDRATER contents" data-target="SideNavigation" data-props="{&quot;titleTree&quot;:[{&quot;id&quot;:&quot;dataset-details&quot;,&quot;label&quot;:&quot;Dataset Details&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Details&quot;},{&quot;id&quot;:&quot;uses&quot;,&quot;label&quot;:&quot;Uses&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;out-of-scope-use&quot;,&quot;label&quot;:&quot;Out-of-Scope Use&quot;,&quot;children&quot;:[],&quot;isValid&quot;:false,&quot;title&quot;:&quot;Out-of-Scope Use&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Uses&quot;},{&quot;id&quot;:&quot;dataset-structure&quot;,&quot;label&quot;:&quot;Dataset Structure&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Structure&quot;},{&quot;id&quot;:&quot;dataset-creation&quot;,&quot;label&quot;:&quot;Dataset Creation&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;curation-rationale&quot;,&quot;label&quot;:&quot;Curation Rationale&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Curation Rationale&quot;},{&quot;id&quot;:&quot;source-data&quot;,&quot;label&quot;:&quot;Source Data&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Source Data&quot;},{&quot;id&quot;:&quot;annotations-optional&quot;,&quot;label&quot;:&quot;Annotations [optional]&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Annotations [optional]&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Creation&quot;},{&quot;id&quot;:&quot;bias-risks-and-limitations&quot;,&quot;label&quot;:&quot;Bias, Risks, and Limitations&quot;,&quot;children&quot;:[{&quot;id&quot;:&quot;recommendations&quot;,&quot;label&quot;:&quot;Recommendations&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Recommendations&quot;}],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Bias, Risks, and Limitations&quot;},{&quot;id&quot;:&quot;citation&quot;,&quot;label&quot;:&quot;Citation&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Citation&quot;},{&quot;id&quot;:&quot;dataset-card-authors&quot;,&quot;label&quot;:&quot;Dataset Card Authors&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Card Authors&quot;},{&quot;id&quot;:&quot;dataset-card-contact&quot;,&quot;label&quot;:&quot;Dataset Card Contact&quot;,&quot;children&quot;:[],&quot;isValid&quot;:true,&quot;title&quot;:&quot;Dataset Card Contact&quot;}],&quot;classNames&quot;:&quot;top-6&quot;}"><div class="absolute -left-12 z-10 h-full top-6"><div class="sticky top-4 flex"><div class="pt-[0.175rem]"><span class="peer" tabindex="0"><button class="select-none hover:cursor-pointer"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-lg text-gray-400/60 dark:text-gray-500 hover:text-gray-800 dark:hover:text-gray-400" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg></button></span> <div class="invisible w-0 -translate-x-24 -translate-y-6 overflow-hidden rounded-xl border bg-white transition-transform hover:visible hover:w-52 hover:translate-x-0 peer-focus-within:visible peer-focus-within:w-52 peer-focus-within:translate-x-0"><nav aria-label="Secondary" class="max-h-[550px] overflow-y-auto p-3"><ul><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-details" title="Dataset Details">Dataset Details</a> <ul class="pl-1"></ul> </li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#uses" title="Uses">Uses</a> <ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-300 dark:text-gray-700" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#out-of-scope-use" title="Out-of-Scope Use">Out-of-Scope Use</a> <ul class="pl-2"></ul> </li></ul> </li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-structure" title="Dataset Structure">Dataset Structure</a> <ul class="pl-1"></ul> </li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-creation" title="Dataset Creation">Dataset Creation</a> <ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#curation-rationale" title="Curation Rationale">Curation Rationale</a> <ul class="pl-2"></ul> </li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#source-data" title="Source Data">Source Data</a> <ul class="pl-2"></ul> </li><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#annotations-optional" title="Annotations [optional]">Annotations [optional]</a> <ul class="pl-2"></ul> </li></ul> </li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#bias-risks-and-limitations" title="Bias, Risks, and Limitations">Bias, Risks, and Limitations</a> <ul class="pl-1"><li><a class="mb-0.5 block break-words hover:underline active:text-gray-700 dark:active:text-gray-300 text-gray-500" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#recommendations" title="Recommendations">Recommendations</a> <ul class="pl-2"></ul> </li></ul> </li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#citation" title="Citation">Citation</a> <ul class="pl-1"></ul> </li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-card-authors" title="Dataset Card Authors">Dataset Card Authors</a> <ul class="pl-1"></ul> </li><li class="mb-3 text-sm last:mb-0"><a class="mb-1 block break-words font-semibold text-gray-700 hover:underline active:text-gray-900 dark:active:text-gray-200 [&amp;&gt;*]:break-words" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-card-contact" title="Dataset Card Contact">Dataset Card Contact</a> <ul class="pl-1"></ul> </li></ul></nav></div></div></div></div></div>
					<div class="2xl:pr-6"><div class="prose pl-6 -ml-6 hf-sanitized hf-sanitized-tBoApXp5eRORoOnWItYae">
	<!-- HTML_TAG_START --><h1 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-card-for-hugging-face-hub-model-cards" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-card-for-hugging-face-hub-model-cards">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Card for Hugging Face Hub Model Cards
	</span>
</h1>
<p>This datasets consists of <a rel="nofollow" href="https://huggingface.co/docs/hub/model-cards">model cards</a> for models hosted on the Hugging Face Hub. The model cards are created by the community and provide information about the model, its performance, its intended uses, and more. 
This dataset is updated on a daily basis and includes publicly available models on the Hugging Face Hub.</p>
<p>This dataset is made available to help support users wanting to work with a large number of Model Cards from the Hub. We hope that this dataset will help support research in the area of Model Cards and their use but the format of this dataset may not be useful for all use cases. If there are other features that you would like to see included in this dataset, please open a new <a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/discussions/new">discussion</a>. </p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-details" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-details">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Details
	</span>
</h2>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#uses" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="uses">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Uses
	</span>
</h2>
<p>There are a number of potential uses for this dataset including:</p>
<ul>
<li>text mining to find common themes in model cards</li>
<li>analysis of the model card format/content</li>
<li>topic modelling of model cards</li>
<li>analysis of the model card metadata</li>
<li>training language models on model cards</li>
</ul>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#out-of-scope-use" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="out-of-scope-use">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Out-of-Scope Use
	</span>
</h3>
<p>[More Information Needed]</p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-structure" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-structure">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Structure
	</span>
</h2>
<p>This dataset has a single split.</p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-creation" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-creation">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Creation
	</span>
</h2>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#curation-rationale" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="curation-rationale">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Curation Rationale
	</span>
</h3>


<p>The dataset was created to assist people in working with model cards. In particular it was created to support research in the area of model cards and their use. It is possible to use the Hugging Face Hub API or client library to download model cards and this option may be preferable if you have a very specific use case or require a different format.</p>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#source-data" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="source-data">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Source Data
	</span>
</h3>
<p>The source data is <code>README.md</code> files for models hosted on the Hugging Face Hub. We do not include any other supplementary files that may be included in the model card directory. </p>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#data-collection-and-processing" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="data-collection-and-processing">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Data Collection and Processing
	</span>
</h4>


<p>The data is downloaded using a CRON job on a daily basis. </p>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#who-are-the-source-data-producers" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="who-are-the-source-data-producers">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Who are the source data producers?
	</span>
</h4>
<p>The source data producers are the creators of the model cards on the Hugging Face Hub. This includes a broad variety of people from the community ranging from large companies to individual researchers. We do not gather any information about who created the model card in this repository although this information can be gathered from the Hugging Face Hub API.</p>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#annotations-optional" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="annotations-optional">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Annotations [optional]
	</span>
</h3>
<p>There are no additional annotations in this dataset beyond the model card content.</p>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#annotation-process" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="annotation-process">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Annotation process
	</span>
</h4>
<p>N/A</p>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#who-are-the-annotators" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="who-are-the-annotators">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Who are the annotators?
	</span>
</h4>


<p>N/A</p>
<h4 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#personal-and-sensitive-information" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="personal-and-sensitive-information">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Personal and Sensitive Information
	</span>
</h4>


<p>We make no effort to anonymize the data. Whilst we don't expect the majority of model cards to contain personal or sensitive information, it is possible that some model cards may contain this information. Model cards may also link to websites or email addresses. </p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#bias-risks-and-limitations" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="bias-risks-and-limitations">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Bias, Risks, and Limitations
	</span>
</h2>


<p>Model cards are created by the community and we do not have any control over the content of the model cards. We do not review the content of the model cards and we do not make any claims about the accuracy of the information in the model cards. 
Some model cards will themselves discuss bias and sometimes this is done by providing examples of bias in either the training data or the responses provided by the model. As a result this dataset may contain examples of bias. </p>
<p>Whilst we do not directly download any images linked to in the model cards, some model cards may include images. Some of these images may not be suitable for all audiences.</p>
<h3 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#recommendations" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="recommendations">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Recommendations
	</span>
</h3>


<p>Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.</p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#citation" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="citation">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Citation
	</span>
</h2>
<p>No formal citation is required for this dataset but if you use this dataset in your work, please include a link to this dataset page.</p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-card-authors" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-card-authors">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Card Authors
	</span>
</h2>
<p><a rel="nofollow" href="https://huggingface.co/davanstrien">@davanstrien</a></p>
<h2 class="relative group flex items-center">
	<a rel="nofollow" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata#dataset-card-contact" class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" id="dataset-card-contact">
		<span class="header-link"><svg viewBox="0 0 256 256" preserveAspectRatio="xMidYMid meet" height="1em" width="1em" role="img" aria-hidden="true" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4"><path fill="currentColor" d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z"></path></svg></span>
	</a>
	<span>
		Dataset Card Contact
	</span>
</h2>
<p><a rel="nofollow" href="https://huggingface.co/davanstrien">@davanstrien</a></p>
<!-- HTML_TAG_END --></div>
</div></section>
			
			<section class="pt-6 border-gray-100 md:pb-24 md:pl-6 md:w-64 lg:w-80 xl:w-96 flex-none order-first md:order-none md:border-l !pt-3 md:!pt-6"><dl class="flex items-baseline justify-between"><dt class="text-sm text-gray-500">Downloads last month</dt><div class="mx-4 flex-1 border-b border-dotted"></div><dd class="font-semibold">104</dd></dl>
				<div class="divider-column-vertical"></div>
				<div class="grid grid-cols-2 gap-x-2 md:flex md:flex-row md:flex-wrap"><div class="SVELTE_HYDRATER contents" data-target="DatasetAndModelActionsDropdown" data-props="{&quot;classNames&quot;:&quot;order-last&quot;,&quot;discussionsDisabled&quot;:false,&quot;csrf&quot;:&quot;eyJkYXRhIjp7ImV4cGlyYXRpb24iOjE3MTI3NDE3Mjc1MzEsInVzZXJJZCI6IjYzOTg2Y2M3OTgyMzRjYTIyZjg3MGU3ZCJ9LCJzaWduYXR1cmUiOiIwNzNiYTczMGU2ZDkwOGUxM2VhMjFlOTdmNmZmMjNkNGYxNWUxNGI3ZDgwOTZjZDhiZGRiMmNhYTUyOTZjMDQ0In0=&quot;,&quot;repo&quot;:{&quot;type&quot;:&quot;dataset&quot;,&quot;name&quot;:&quot;librarian-bots/model_cards_with_metadata&quot;},&quot;canWrite&quot;:false,&quot;canDisable&quot;:false,&quot;repoIsPrivate&quot;:false,&quot;repoIsGated&quot;:false,&quot;repoIsAdminFlaggedNFAA&quot;:false,&quot;watched&quot;:false,&quot;muted&quot;:false,&quot;orgs&quot;:[],&quot;user&quot;:&quot;nelson2424&quot;}"><div class="order-last"><div class="relative "><button class="btn px-1.5 py-1.5 " type="button"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="p-0.5" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><circle cx="16" cy="7" r="3" fill="currentColor"></circle><circle cx="16" cy="16" r="3" fill="currentColor"></circle><circle cx="16" cy="25" r="3" fill="currentColor"></circle></svg> </button> </div></div>    </div>
					<div class="SVELTE_HYDRATER contents" data-target="DatasetLibraryModal" data-props="{&quot;classNames&quot;:&quot;md:w-full xl:w-auto xl:flex-none&quot;,&quot;dataset&quot;:{&quot;author&quot;:&quot;librarian-bots&quot;,&quot;cardData&quot;:{&quot;size_categories&quot;:[&quot;100K&lt;n&lt;1M&quot;],&quot;task_categories&quot;:[&quot;text-retrieval&quot;],&quot;pretty_name&quot;:&quot;Hugging Face Hub Model Cards&quot;,&quot;dataset_info&quot;:{&quot;features&quot;:[{&quot;name&quot;:&quot;modelId&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;author&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;last_modified&quot;,&quot;dtype&quot;:&quot;timestamp[us, tz=UTC]&quot;},{&quot;name&quot;:&quot;downloads&quot;,&quot;dtype&quot;:&quot;int64&quot;},{&quot;name&quot;:&quot;likes&quot;,&quot;dtype&quot;:&quot;int64&quot;},{&quot;name&quot;:&quot;library_name&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;tags&quot;,&quot;sequence&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;pipeline_tag&quot;,&quot;dtype&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;createdAt&quot;,&quot;dtype&quot;:&quot;timestamp[us, tz=UTC]&quot;},{&quot;name&quot;:&quot;card&quot;,&quot;dtype&quot;:&quot;string&quot;}],&quot;splits&quot;:[{&quot;name&quot;:&quot;train&quot;,&quot;num_bytes&quot;:1070235362,&quot;num_examples&quot;:581390}],&quot;download_size&quot;:323296504,&quot;dataset_size&quot;:1070235362},&quot;configs&quot;:[{&quot;config_name&quot;:&quot;default&quot;,&quot;data_files&quot;:[{&quot;split&quot;:&quot;train&quot;,&quot;path&quot;:&quot;data/train-*&quot;}]}],&quot;tags&quot;:[&quot;ethics&quot;]},&quot;cardExists&quot;:true,&quot;description&quot;:&quot;\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Hugging Face Hub Model Cards\n\t\n\nThis datasets consists of model cards for models hosted on the Hugging Face Hub. The model cards are created by the community and provide information about the model, its performance, its intended uses, and more. \nThis dataset is updated on a daily basis and includes publicly available models on the Hugging Face Hub.\nThis dataset is made available to help support users wanting to work with a large number of Model Cards from the Hub.… See the full description on the dataset page: https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata.&quot;,&quot;downloads&quot;:104,&quot;downloadsAllTime&quot;:435,&quot;id&quot;:&quot;librarian-bots/model_cards_with_metadata&quot;,&quot;isLikedByUser&quot;:true,&quot;isMutedByUser&quot;:false,&quot;isWatchedByUser&quot;:false,&quot;lastModified&quot;:&quot;2024-04-05T07:02:39.000Z&quot;,&quot;likes&quot;:7,&quot;datasetsServerInfo&quot;:{&quot;viewer&quot;:&quot;viewer&quot;,&quot;numRows&quot;:581390,&quot;tags&quot;:[&quot;croissant&quot;],&quot;libraries&quot;:[&quot;datasets&quot;,&quot;dask&quot;,&quot;mlcroissant&quot;]},&quot;discussionsDisabled&quot;:false,&quot;repoType&quot;:&quot;dataset&quot;,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;tags&quot;:[&quot;task_categories:text-retrieval&quot;,&quot;size_categories:100K&lt;n&lt;1M&quot;,&quot;ethics&quot;,&quot;croissant&quot;,&quot;region:us&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;task_categories:text-retrieval&quot;,&quot;label&quot;:&quot;text-retrieval&quot;,&quot;type&quot;:&quot;task_categories&quot;,&quot;subType&quot;:&quot;nlp&quot;},{&quot;id&quot;:&quot;size_categories:100K&lt;n&lt;1M&quot;,&quot;label&quot;:&quot;100K&lt;n&lt;1M&quot;,&quot;type&quot;:&quot;size_categories&quot;},{&quot;id&quot;:&quot;ethics&quot;,&quot;label&quot;:&quot;ethics&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;croissant&quot;,&quot;label&quot;:&quot;Croissant&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;type&quot;:&quot;region&quot;,&quot;label&quot;:&quot;🇺🇸 Region: US&quot;,&quot;id&quot;:&quot;region:us&quot;}]}}"><div class="mb-2 md:w-full xl:w-auto xl:flex-none"><button class="w-full cursor-pointer btn text-sm" type="button"><svg class="mr-1.5 " xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" style="transform: rotate(360deg);"><path d="M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z" fill="currentColor"></path><path d="M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z" fill="currentColor"></path><path d="M12.419 25.484L17.639 6l1.932.518L14.35 26z" fill="currentColor"></path></svg> Use in Datasets library</button> </div></div>
					<a class="btn mb-2 text-sm md:w-full xl:w-auto xl:flex-none" href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/edit/main/README.md"><svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M2 26h28v2H2z" fill="currentColor"></path><path d="M25.4 9c.8-.8.8-2 0-2.8l-3.6-3.6c-.8-.8-2-.8-2.8 0l-15 15V24h6.4l15-15zm-5-5L24 7.6l-3 3L17.4 7l3-3zM6 22v-3.6l10-10l3.6 3.6l-10 10H6z" fill="currentColor"></path></svg>
							Edit dataset card
						</a>
					</div>
				<div class="divider-column-vertical"></div>
					<div class="flex flex-col flex-wrap xl:flex-row"><a class="group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 bg-gradient-to-r from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Curated by:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->Daniel van Strien<!-- HTML_TAG_END --></div></a><a class="group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 bg-gradient-to-r from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Language(s) (NLP):</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->Model cards on the Hugging Face Hub are predominantly in English but may include other languages.<!-- HTML_TAG_END --></div></a><a class="group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 bg-gradient-to-r from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of downloaded dataset files:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->323 MB<!-- HTML_TAG_END --></div></a><a class="group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 bg-gradient-to-r from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 " href="https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata/tree/refs%2Fconvert%2Fparquet/" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Size of the auto-converted Parquet files:</div>
		<div class="truncate text-sm group-hover:underline">
			<!-- HTML_TAG_START -->323 MB<!-- HTML_TAG_END --></div></a><a class="group mb-1.5 flex max-w-full flex-col overflow-hidden rounded-lg border border-gray-100 bg-gradient-to-r from-white via-white to-white px-2 py-1 hover:from-gray-50 dark:from-gray-900 dark:to-gray-925 dark:hover:to-gray-900 md:mr-1.5 pointer-events-none" rel="nofollow" target="_blank"><div class="truncate text-xs text-gray-400">Number of rows:</div>
		<div class="truncate text-sm ">
			<!-- HTML_TAG_START -->581,390<!-- HTML_TAG_END --></div></a></div>
				
				
				

				<div class="divider-column-vertical"></div>
					<h2 class="mb-5 flex items-baseline overflow-hidden whitespace-nowrap text-smd font-semibold text-gray-800"><svg class="mr-1 inline self-center flex-none" width="1em" height="1em" aria-hidden="true" focusable="false" role="img" viewBox="0 0 
	12 13" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><rect x="2" y="2.49902" width="8" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.4"></rect><rect x="6.21875" y="6.7334" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.7"></rect><rect x="2" y="6.73438" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.5"></rect></svg>
						Collection including
						<span class="ml-1 truncate font-mono text-[0.86rem] font-medium">librarian-bots/model_cards_with_metadata</span></h2>
					<div class="flex flex-col gap-3">

<div class="flex flex-col"><article class="overview-card-wrapper group/collection relative !rounded-md !from-white !to-white dark:!from-gray-900 dark:!to-gray-900  peer"><a href="https://huggingface.co/collections/librarian-bots/hugging-face-hub-model-cards-and-dataset-cards-65311589600f5d88eb7f6a22" class="block p-2"><header class="mb-0.5 flex items-center" title="Hugging Face Hub Model Cards and Dataset Cards"><h4 class="text-md truncate text-smd font-semibold group-hover/collection:underline">Hugging Face Hub Model Cards and Dataset Cards</h4>
				<div class="ml-2 flex items-center rounded sm:ml-2.5 bg-purple-500/10 py-0.5 pl-1 pr-1.5 text-xs leading-none text-gray-700"><svg class="mr-0.5 flex-none text-purple-700" width="1em" height="1em" aria-hidden="true" focusable="false" role="img" viewBox="0 0 
	12 13" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><rect x="2" y="2.49902" width="8" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.4"></rect><rect x="6.21875" y="6.7334" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.7"></rect><rect x="2" y="6.73438" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.5"></rect></svg>
					Collection
				</div></header>

			<div class="mr-1 flex items-center overflow-hidden whitespace-nowrap text-sm leading-tight text-gray-400">
				<span>1 item</span>
				<span class="px-1.5 text-gray-300">• </span>
				<span class="truncate">Updated
					<time datetime="2023-10-19T11:39:53" title="2023-10-19T11:39:53.049Z">Oct 19, 2023</time></span>
				</div></a></article>
	</div></div>
				<div class="divider-column-vertical md:hidden"></div></section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container flex flex-col justify-between space-y-2 py-6 text-gray-500 md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="font-semibold text-black md:hidden">Company</div>
		<div class="order-last pt-6 text-gray-400 md:order-none md:pt-0" href="Terms">© Hugging Face</div>
		<a class="hover:underline" href="https://huggingface.co/terms-of-service">TOS</a>
		<a class="hover:underline" href="https://huggingface.co/privacy">Privacy</a>
		<a class="hover:underline" href="https://huggingface.co/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="https://huggingface.co/" class="group order-first flex-none pb-6 md:order-none md:pb-0"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="pt-6 font-semibold text-black md:hidden md:pt-0">Website</div>

		<a class="hover:underline" href="https://huggingface.co/models">Models</a>
		<a class="hover:underline" href="https://huggingface.co/datasets">Datasets</a>
		<a class="hover:underline" href="https://huggingface.co/spaces">Spaces</a>
		<a class="hover:underline" href="https://huggingface.co/pricing">Pricing</a>
		<a class="hover:underline" href="https://huggingface.co/docs">Docs</a></nav></footer></div>

		<script>
			import("/front/build/kube-02525e9/index.js");
			window.moonSha = "kube-02525e9/";
			window.hubConfig = JSON.parse(`{"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https://huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https://datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)"}`);
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>

		<!-- Google analytics v4 -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://www.googletagmanager.com/gtag/js?id=G-8Q63TH4CSL";
				script.async = true;
				document.head.appendChild(script);

				window.dataLayer = window.dataLayer || [];
				function gtag() {
					if (window.dataLayer !== undefined) {
						window.dataLayer.push(arguments);
					}
				}
				gtag("js", new Date());
				gtag("config", "G-8Q63TH4CSL", { page_path: "/datasets/librarian-bots/model_cards_with_metadata" });
				/// ^ See https://developers.google.com/analytics/devguides/collection/gtagjs/pages
				gtag("consent", "default", { ad_storage: "denied", analytics_storage: "denied" });
				/// ^ See https://developers.google.com/tag-platform/gtagjs/reference#consent
				/// TODO: ask the user for their consent and update this with gtag('consent', 'update')
			}
		</script>
	

<iframe name="__privateStripeMetricsController2560" frameborder="0" allowtransparency="true" scrolling="no" role="presentation" allow="payment *" src="./librarian-bots_model_cards_with_metadata · Datasets at Hugging Face_files/m-outer-3437aaddcdf6922d623e172c2d6f9278.html" aria-hidden="true" tabindex="-1" style="border: none !important; margin: 0px !important; padding: 0px !important; width: 1px !important; min-width: 100% !important; overflow: hidden !important; display: block !important; visibility: hidden !important; position: fixed !important; height: 1px !important; pointer-events: none !important; user-select: none !important;"></iframe><div id="dc972c2f-992c-51c6-a634-8f72aeb33ee7"></div></body><scispace-div id="scispace-extension-root"><template shadowrootmode="open"><style>
    :host {
      all: initial;
      font-size: initial ;
    }
  </style><link rel="stylesheet" type="text/css" href="chrome-extension://cipccbpjpemcnijhjcdjmkjhmhniiick/static/styles/styles-760a7b596bc0696605d9.css"><div class="container"><div id="scispace-extension-app-root"><div class="box-border h-full w-full"><div data-plugin-state="COLLAPSED" data-plugin-context-id="aade2726-f378-42b1-8c57-66393bf3dd19"></div><div class="relative"><div id="draggableOverlay" class="pointer-events-none fixed inset-0 z-extension box-border h-full w-full bg-transparent p-3"><div class="pointer-events-auto fixed right-0 top-0 z-extension mt-14 h-7 w-max rounded-l-full p-0.5 scispace-extension-draggable" style="transform: translate(0px, 308px);"><div class="group/copilot-pill relative flex w-7 items-center rounded-l-full border border-r-0 border-solid border-neutral-100/60 bg-white/60 pr-1.5 shadow-pill transition-all hover:w-11 hover:border-neutral-100 hover:bg-white aria-pressed:w-11 aria-pressed:border-neutral-100 aria-pressed:bg-white/60" aria-pressed="false"><button class="flex cursor-pointer items-center justify-center gap-2 font-sans text-sm font-semibold
  focus-visible:outline-none disabled:pointer-events-none disabled:opacity-40 disabled:hover:pointer-events-auto border-0 bg-transparent !p-0 bg-primary-500 px-2 py-1 rounded-full peer/disable-quick-access absolute -left-2 -top-2 hidden h-4 w-4 items-center justify-center !bg-neutral-800/60 hover:!bg-neutral-800 group-hover/copilot-pill:flex peer-hover:!bg-neutral-800/60 aria-pressed:!hidden" aria-pressed="false"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark text-white" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M342.6 150.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192 210.7 86.6 105.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L146.7 256 41.4 361.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192 301.3 297.4 406.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.3 256 342.6 150.6z"></path></svg></button><span data-state="closed"><span><button class="flex cursor-pointer items-center justify-center gap-2 font-sans text-sm font-semibold
  focus-visible:outline-none disabled:pointer-events-none disabled:opacity-40 disabled:hover:pointer-events-auto border-0 bg-transparent !p-0 bg-primary-500 px-2 py-1 !p-0 rounded-md !cursor-pointer !p-1.5 !pl-3 opacity-60 group-hover/copilot-pill:opacity-100 aria-pressed:opacity-60" aria-pressed="false"><svg xmlns="http://www.w3.org/2000/svg" width="17" height="18" fill="none" viewBox="0 0 17 18"><path fill="url(#paint0_linear_8778_17378)" fill-rule="evenodd" d="M7.361.43C5.4 1.558 3.831 2.324 1.396 3.797.835 4.136.532 4.7.875 5.322c.77 1.393 1.846 3.433 2.815 5.183a.09.09 0 01-.037.123c-.405.219-1.669.907-2.585 1.446-.99.582-.53 1.59.2 1.974.73.383 1.116.645 1.836 1.052 1.907 1.08 3.257 1.683 4.795 2.509 1.533.687 2.132.398 3.276-.32 2.65-1.666 4.289-2.823 5.02-3.321.688-.469 1.07-1.277.593-2.056-.046-.09-.355-.609-1.24-2.046l-1.663-2.698a.089.089 0 01.03-.122c.757-.485 1.776-1.164 2.28-1.493.522-.34.545-1.329-.112-1.756-1.535-1-3.702-2.154-6.376-3.575-.704-.375-1.54-.255-2.346.208zm4.882 2.272c1.584.936 1.931 1.155 2.425 1.416.639.339.734.654.14 1L9.598 8.2c-.464.275-.884.273-1.349 0l-5.27-2.879c-.543-.303-.921-.848 0-1.319l4.845-2.61c.541-.342 1.202-.462 1.951-.022.625.366 1.257.614 2.47 1.331zM4.28 7.67c1.405.8 1.97 1.168 3.62 2.031 1.088.569 1.636 1.013 2.76.276.542-.356 1.22-.779 2.328-1.383a.099.099 0 01.13.031c.242.375 1.824 2.522 2.347 3.4.325.544.136.82-.263 1.06-.544.327-1.573.998-2.567 1.627-.892.565-1.135.76-2.247 1.443-.7.43-1.486.15-1.947-.575a268.435 268.435 0 01-4.706-7.672c-.798-1.353.01-.543.545-.238zm1.411 6.107c1.964 3.137-2.05.359-3.259-.394-.489-.304-.827-.45-.217-.793.391-.22 1.59-.9 1.912-1.083a.098.098 0 01.13.03c.149.223.626.95 1.434 2.24z" clip-rule="evenodd"></path><defs><lineargradient id="paint0_linear_8778_17378" x1="0.5" x2="17" y1="9" y2="9" gradientUnits="userSpaceOnUse"><stop stop-color="#FA6400"></stop><stop offset="0.536" stop-color="#28AD9D"></stop><stop offset="1" stop-color="#047BB2"></stop></lineargradient></defs></svg></button></span></span><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="grip-vertical" class="svg-inline--fa fa-grip-vertical ml-0.5 hidden cursor-move text-neutral-800 group-hover/copilot-pill:block peer-hover/disable-quick-access:text-neutral-800/60 aria-pressed:block aria-pressed:text-neutral-800/60" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512" id="drag-handler" aria-pressed="false"><path fill="currentColor" d="M40 352l48 0c22.1 0 40 17.9 40 40l0 48c0 22.1-17.9 40-40 40l-48 0c-22.1 0-40-17.9-40-40l0-48c0-22.1 17.9-40 40-40zm192 0l48 0c22.1 0 40 17.9 40 40l0 48c0 22.1-17.9 40-40 40l-48 0c-22.1 0-40-17.9-40-40l0-48c0-22.1 17.9-40 40-40zM40 320c-22.1 0-40-17.9-40-40l0-48c0-22.1 17.9-40 40-40l48 0c22.1 0 40 17.9 40 40l0 48c0 22.1-17.9 40-40 40l-48 0zM232 192l48 0c22.1 0 40 17.9 40 40l0 48c0 22.1-17.9 40-40 40l-48 0c-22.1 0-40-17.9-40-40l0-48c0-22.1 17.9-40 40-40zM40 160c-22.1 0-40-17.9-40-40L0 72C0 49.9 17.9 32 40 32l48 0c22.1 0 40 17.9 40 40l0 48c0 22.1-17.9 40-40 40l-48 0zM232 32l48 0c22.1 0 40 17.9 40 40l0 48c0 22.1-17.9 40-40 40l-48 0c-22.1 0-40-17.9-40-40l0-48c0-22.1 17.9-40 40-40z"></path></svg></div></div></div></div></div></div></div></template></scispace-div><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>