{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to extract data from HF repository\n",
    "\n",
    "The idea is to get familiarize with the HF API to extract data from ML repositories.\n",
    "Is important to realize that relevant metadata basically can be found in the readme.MD file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a specific file\n",
    "\n",
    "In the quickstart guide of the [Hugging Face Hub API](https://huggingface.co/docs/huggingface_hub/v0.8.0/en/quick-start), there is an example on how to retrieve particular documents from the ML repository.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/vscode/.cache/huggingface/hub/models--google--pegasus-xsum/snapshots/8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b/README.md'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"google/pegasus-xsum\", filename=\"README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to pick where is the document being downloaded. Even check if we can bring it to memory right away.\n",
    "\n",
    "For the first problem we have the following [documentation](https://huggingface.co/docs/huggingface_hub/main/en/guides/download). We can use the `local_dir=\"path/to/folder\"` attribute of the `hf_hub_download` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vscode/README_files/README.md'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_hub_download(repo_id=\"google/pegasus-xsum\", filename=\"README.md\",local_dir=\"/home/vscode/README_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model cards\n",
    "Even better than the above, there are ways to only query the Model card metadata that is usually on the README.md file, using the [ModelCard](https://huggingface.co/docs/huggingface_hub/main/en/guides/model-cards) function. \n",
    "\n",
    "Now the problem with this approach is clear, not all the metadata can be found on the Model card, a lot of useful information can be extracted from the README.md file.\n",
    "\n",
    "Even with the previous setback in can said that this functionality really helps to proccess the data better. \n",
    "\n",
    "Now I don't know if it is worth to spend one of the 10k limited requests we have every minute on a preprocessing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "language: en\n",
      "tags:\n",
      "- summarization\n",
      "model-index:\n",
      "- name: google/pegasus-xsum\n",
      "  results:\n",
      "  - task:\n",
      "      type: summarization\n",
      "      name: Summarization\n",
      "    dataset:\n",
      "      name: samsum\n",
      "      type: samsum\n",
      "      config: samsum\n",
      "      split: train\n",
      "    metrics:\n",
      "    - type: rouge\n",
      "      value: 21.8096\n",
      "      name: ROUGE-1\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 4.2525\n",
      "      name: ROUGE-2\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 17.4469\n",
      "      name: ROUGE-L\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 18.8907\n",
      "      name: ROUGE-LSUM\n",
      "      verified: true\n",
      "    - type: loss\n",
      "      value: 3.0317161083221436\n",
      "      name: loss\n",
      "      verified: true\n",
      "    - type: gen_len\n",
      "      value: 20.3122\n",
      "      name: gen_len\n",
      "      verified: true\n",
      "  - task:\n",
      "      type: summarization\n",
      "      name: Summarization\n",
      "    dataset:\n",
      "      name: xsum\n",
      "      type: xsum\n",
      "      config: default\n",
      "      split: test\n",
      "    metrics:\n",
      "    - type: rouge\n",
      "      value: 46.8623\n",
      "      name: ROUGE-1\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 24.4533\n",
      "      name: ROUGE-2\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 39.0548\n",
      "      name: ROUGE-L\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 39.0994\n",
      "      name: ROUGE-LSUM\n",
      "      verified: true\n",
      "    - type: loss\n",
      "      value: 1.5717021226882935\n",
      "      name: loss\n",
      "      verified: true\n",
      "    - type: gen_len\n",
      "      value: 22.8821\n",
      "      name: gen_len\n",
      "      verified: true\n",
      "  - task:\n",
      "      type: summarization\n",
      "      name: Summarization\n",
      "    dataset:\n",
      "      name: cnn_dailymail\n",
      "      type: cnn_dailymail\n",
      "      config: 3.0.0\n",
      "      split: test\n",
      "    metrics:\n",
      "    - type: rouge\n",
      "      value: 22.2062\n",
      "      name: ROUGE-1\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 7.6701\n",
      "      name: ROUGE-2\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 15.4046\n",
      "      name: ROUGE-L\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 19.2182\n",
      "      name: ROUGE-LSUM\n",
      "      verified: true\n",
      "    - type: loss\n",
      "      value: 2.681241273880005\n",
      "      name: loss\n",
      "      verified: true\n",
      "    - type: gen_len\n",
      "      value: 25.0234\n",
      "      name: gen_len\n",
      "      verified: true\n",
      "---\n",
      "\n",
      "### Pegasus Models\n",
      "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html)\n",
      "\n",
      "Original TF 1 code [here](https://github.com/google-research/pegasus)\n",
      "\n",
      "Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019\n",
      "\n",
      "Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)\n",
      "\n",
      "Task: Summarization\n",
      "\n",
      "The following is copied from the authors' README.\n",
      "\n",
      "# Mixed & Stochastic Checkpoints\n",
      "\n",
      "We train a pegasus model with sampled gap sentence ratios on both C4 and HugeNews, and stochastically sample important sentences. The updated the results are reported in this table.\n",
      "\n",
      "| dataset | C4 | HugeNews | Mixed & Stochastic|\n",
      "| ---- | ---- | ---- | ----|\n",
      "| xsum | 45.20/22.06/36.99 | 47.21/24.56/39.25 | 47.60/24.83/39.64|\n",
      "| cnn_dailymail | 43.90/21.20/40.76 | 44.17/21.47/41.11 | 44.16/21.56/41.30|\n",
      "| newsroom | 45.07/33.39/41.28 | 45.15/33.51/41.33 | 45.98/34.20/42.18|\n",
      "| multi_news | 46.74/17.95/24.26 | 47.52/18.72/24.91 | 47.65/18.75/24.95|\n",
      "| gigaword | 38.75/19.96/36.14 | 39.12/19.86/36.24 | 39.65/20.47/36.76|\n",
      "| wikihow | 43.07/19.70/34.79 | 41.35/18.51/33.42 | 46.39/22.12/38.41 *|\n",
      "| reddit_tifu | 26.54/8.94/21.64 | 26.63/9.01/21.60 | 27.99/9.81/22.94|\n",
      "| big_patent | 53.63/33.16/42.25 | 53.41/32.89/42.07 | 52.29/33.08/41.66 *|\n",
      "| arxiv | 44.70/17.27/25.80 | 44.67/17.18/25.73 | 44.21/16.95/25.67|\n",
      "| pubmed | 45.49/19.90/27.69 | 45.09/19.56/27.42 | 45.97/20.15/28.25|\n",
      "| aeslc | 37.69/21.85/36.84 | 37.40/21.22/36.45 | 37.68/21.25/36.51|\n",
      "| billsum | 57.20/39.56/45.80 | 57.31/40.19/45.82 | 59.67/41.58/47.59|\n",
      "\n",
      "The \"Mixed & Stochastic\" model has the following changes:\n",
      "- trained on both C4 and HugeNews (dataset mixture is weighted by their number of examples). \n",
      "- trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).\n",
      "- the model uniformly sample a gap sentence ratio between 15% and 45%.\n",
      "- importance sentences are sampled using a 20% uniform noise to importance scores.\n",
      "- the sentencepiece tokenizer is updated to be able to encode newline character.\n",
      "\n",
      "\n",
      "(*) the numbers of wikihow and big_patent datasets are not comparable because of change in tokenization and data:\n",
      "- wikihow dataset contains newline characters which is useful for paragraph segmentation, the C4 and HugeNews model's sentencepiece tokenizer doesn't encode newline and loose this information.\n",
      "- we update the BigPatent dataset to preserve casing, some format cleanings are also changed, please refer to change in TFDS.\n",
      "\n",
      "\n",
      "The \"Mixed & Stochastic\" model has the following changes (from pegasus-large in the paper):\n",
      "\n",
      "\n",
      "trained on both C4 and HugeNews (dataset mixture is weighted by their number of examples).\n",
      "trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).\n",
      "the model uniformly sample a gap sentence ratio between 15% and 45%.\n",
      "importance sentences are sampled using a 20% uniform noise to importance scores.\n",
      "the sentencepiece tokenizer is updated to be able to encode newline character.\n",
      "\n",
      "\n",
      "Citation\n",
      "```\n",
      "\n",
      "\n",
      "@misc{zhang2019pegasus,\n",
      "    title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},\n",
      "    author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},\n",
      "    year={2019},\n",
      "    eprint={1912.08777},\n",
      "    archivePrefix={arXiv},\n",
      "    primaryClass={cs.CL}\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import ModelCard\n",
    "\n",
    "card = ModelCard.load('google/pegasus-xsum')\n",
    "\n",
    "print(card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` card.data ` : Returns a ModelCardData instance with the model cardâ€™s metadata. \n",
    "Call `.to_dict()` on this instance to get the representation as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'en', 'tags': ['summarization'], 'model-index': [{'name': 'google/pegasus-xsum', 'results': [{'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'samsum', 'type': 'samsum', 'config': 'samsum', 'split': 'train'}, 'metrics': [{'type': 'rouge', 'value': 21.8096, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 4.2525, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 17.4469, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 18.8907, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 3.0317161083221436, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 20.3122, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'xsum', 'type': 'xsum', 'config': 'default', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 46.8623, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 24.4533, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 39.0548, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 39.0994, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 1.5717021226882935, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 22.8821, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'cnn_dailymail', 'type': 'cnn_dailymail', 'config': '3.0.0', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 22.2062, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 7.6701, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 15.4046, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 19.2182, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 2.681241273880005, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 25.0234, 'name': 'gen_len', 'verified': True}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "print(card.data.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models\n",
    "\n",
    "Now the next step is how to get the names of the model repositories. After that, it would be nice to figure out a way to get the names of models based on a time period.\n",
    "\n",
    "Let's explore the [list_models](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.list_models) function.\n",
    "\n",
    "We can access relevant information of the model by using the properties of the [ModelInfo](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.hf_api.ModelInfo) Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert/albert-base-v1 2022-03-02 23:29:04+00:00\n",
      "albert/albert-base-v2 2022-03-02 23:29:04+00:00\n",
      "albert/albert-large-v1 2022-03-02 23:29:04+00:00\n",
      "albert/albert-large-v2 2022-03-02 23:29:04+00:00\n",
      "albert/albert-xlarge-v1 2022-03-02 23:29:04+00:00\n",
      "albert/albert-xlarge-v2 2022-03-02 23:29:04+00:00\n",
      "albert/albert-xxlarge-v1 2022-03-02 23:29:04+00:00\n",
      "albert/albert-xxlarge-v2 2022-03-02 23:29:04+00:00\n",
      "bert-base-cased-finetuned-mrpc 2022-03-02 23:29:04+00:00\n",
      "bert-base-cased 2022-03-02 23:29:04+00:00\n",
      "bert-base-chinese 2022-03-02 23:29:04+00:00\n",
      "bert-base-german-cased 2022-03-02 23:29:04+00:00\n",
      "bert-base-german-dbmdz-cased 2022-03-02 23:29:04+00:00\n",
      "bert-base-german-dbmdz-uncased 2022-03-02 23:29:04+00:00\n",
      "bert-base-multilingual-cased 2022-03-02 23:29:04+00:00\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# List all models\n",
    "model_list = api.list_models()\n",
    "\n",
    "i = 15\n",
    "\n",
    "for model in model_list:\n",
    "    if(i==0):\n",
    "        break\n",
    "    i-=1\n",
    "    print(model.id ,model.created_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After around 30000 of the initial models, different dates start to appear, and they look to be orderd in an accedent pattern.\n",
    "The initial date appears to be 2022-03-02 23:29:04+00:00, and then another batch with 2022-03-02 23:29:05+00:00 appears as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc), datetime.datetime(2022, 3, 2, 23, 29, 5, tzinfo=datetime.timezone.utc)}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "model_list = api.list_models()\n",
    "i = 30000\n",
    "unique_dates = set()\n",
    "for model in model_list:\n",
    "    if(i==0):\n",
    "        break\n",
    "    i-=1\n",
    "    unique_dates.add(model.created_at)\n",
    "\n",
    "print(unique_dates)\n",
    "print(len(unique_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892\n",
      "['2022-03-09 11:52:08+00:00', '2022-03-09 13:30:22+00:00', '2022-03-03 15:53:14+00:00', '2022-03-04 12:49:07+00:00', '2022-03-07 00:22:21+00:00', '2022-03-08 14:22:06+00:00', '2022-03-03 04:53:15+00:00', '2022-03-06 20:35:13+00:00', '2022-03-05 20:52:57+00:00', '2022-03-07 09:51:17+00:00']\n"
     ]
    }
   ],
   "source": [
    "model_list = api.list_models()\n",
    "i = 31000\n",
    "unique_dates = set()\n",
    "for model in model_list:\n",
    "    if(i==0):\n",
    "        break\n",
    "    i-=1\n",
    "    unique_dates.add(model.created_at)\n",
    "\n",
    "print(len(unique_dates))\n",
    "last_10_dates = list(unique_dates)[-10:]\n",
    "print([str(x) for x in last_10_dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster downloads\n",
    "If you are running on a machine with high bandwidth, you can increase your download speed with [hf_transfer](https://github.com/huggingface/hf_transfer), a Rust-based library developed to speed up file transfers with the Hub. To enable it:\n",
    "\n",
    "- Specify the hf_transfer extra when installing huggingface_hub (e.g. pip install huggingface_hub[hf_transfer]).\n",
    "- Set HF_HUB_ENABLE_HF_TRANSFER=1 as an environment variable.\n",
    "\n",
    "Test will have to be made to actually determine how worth it it is to use this feature, given that the README.md files weight between 3KB to 10KB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
