[
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"MIT\/ast-finetuned-audioset-10-10-0.4593",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"MIT",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-11-14 18:41:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "audio classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"Audio Spectrogram Transformer",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000012
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"Audio Spectrogram Transformer",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001132961
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"several audio classification benchmarks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000001786
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"several audio classification benchmarks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000154
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"Vision Transformer",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000005
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"Hugging Face team",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000006
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "2104.01778"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"audio",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000271278
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "bsd-3-clause"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"Gong et al.",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.6164907217
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"AudioSet classes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001380682
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"classifying audio into one of the AudioSet classes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2491736114
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"several audio classification benchmarks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000010609
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"classifying audio into one of the AudioSet classes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0030048019
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"audio",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000017
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Gong et al.",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1902675182
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"audio",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000289
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2023-09-06 14:49:15+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"The team releasing Audio Spectrogram Transformer did not write a model card",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000002589
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"Audio Spectrogram Transformer",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000003117
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"0.693 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlicense: bsd-3-clause\ntags:\n- audio-classification\n---\n\n# Audio Spectrogram Transformer (fine-tuned on AudioSet) \n\nAudio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper [AST: Audio Spectrogram Transformer](https:\/\/arxiv.org\/abs\/2104.01778) by Gong et al. and first released in [this repository](https:\/\/github.com\/YuanGongND\/ast). \n\nDisclaimer: The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Audio Spectrogram Transformer is equivalent to [ViT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\n\n## Usage\n\nYou can use the raw model for classifying audio into one of the AudioSet classes. See the [documentation](https:\/\/huggingface.co\/docs\/transformers\/main\/en\/model_doc\/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"openai\/clip-vit-large-patch14",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "zero shot image classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"~93% for racial classification and ~63%",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1176930815
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"ViT-L\/14 Transformer architecture",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3469974101
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"linear probes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1654023975
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0684200153
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"zero-shot",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000466
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"accuracy",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0066551897
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "2103.00020",
                    "1908.04913"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"within",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0283979475
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"researchers at OpenAI",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.102127552
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"pre-existing image datasets",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0449205376
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2787895799
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0964859426
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"The model card is taken and modified from the official CLIP repository",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.025931837
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0114825713
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"researchers at OpenAI",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0020058858
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0568581484
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2023-09-15 15:49:35+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"The model is intended as a research output for research communities",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000036503
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"gle\/Uv7afRH5dvY34ZEs9",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000165806
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"6.847 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\ntags:\n- vision\nwidget:\n- src: https:\/\/huggingface.co\/datasets\/mishig\/sample_images\/resolve\/main\/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https:\/\/github.com\/openai\/CLIP\/blob\/main\/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L\/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https:\/\/openai.com\/blog\/clip\/)\n- [CLIP Paper](https:\/\/arxiv.org\/abs\/2103.00020)\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai\/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai\/clip-vit-large-patch14\")\n\nurl = \"http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http:\/\/projects.dfki.uni-kl.de\/yfcc100m\/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https:\/\/arxiv.org\/abs\/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement\/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https:\/\/forms.gle\/Uv7afRH5dvY34ZEs9)",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"jonatasgrosman\/wav2vec2-large-xlsr-53-english",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"jonatasgrosman",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "automatic speech recognition"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "common_voice",
                    "mozilla-foundation\/common_voice_6_0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"Common Voice 6.1",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2762574852
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"XLSR-53",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0021992121
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"en\n    metrics",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000848022
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"## Evaluation\n\n1",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.025854513
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"16kHz",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000003494
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"input_values, attention_mask=inputs",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000018767
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"directly",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000193285
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"jonatasgrosman",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.6643454432
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"The model can be used directly (without a language model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0108902687
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"speech recognition",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1605238169
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"english --dataset mozilla-foundation",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000305167
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"directly",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2019733042
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"Wav2Vec2Processor",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.5176950693
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Grosman, Jonatas",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.7634411454
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"without a language model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0002791466
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2023-03-25 10:56:55+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"fine-tuned",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000148449
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"2021",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000155617
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"4.656 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\ndatasets:\n- common_voice\n- mozilla-foundation\/common_voice_6_0\nmetrics:\n- wer\n- cer\ntags:\n- audio\n- automatic-speech-recognition\n- en\n- hf-asr-leaderboard\n- mozilla-foundation\/common_voice_6_0\n- robust-speech-event\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 English by Jonatas Grosman\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice en\n      type: common_voice\n      args: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 19.06\n    - name: Test CER\n      type: cer\n      value: 7.69\n    - name: Test WER (+LM)\n      type: wer\n      value: 14.81\n    - name: Test CER (+LM)\n      type: cer\n      value: 6.84\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2\/dev_data\n      args: en\n    metrics:\n    - name: Dev WER\n      type: wer\n      value: 27.72\n    - name: Dev CER\n      type: cer\n      value: 11.65\n    - name: Dev WER (+LM)\n      type: wer\n      value: 20.85\n    - name: Dev CER (+LM)\n      type: cer\n      value: 11.01\n---\n\n# Fine-tuned XLSR-53 large model for speech recognition in English\n\nFine-tuned [facebook\/wav2vec2-large-xlsr-53](https:\/\/huggingface.co\/facebook\/wav2vec2-large-xlsr-53) on English using the train and validation splits of [Common Voice 6.1](https:\/\/huggingface.co\/datasets\/common_voice).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https:\/\/www.ovhcloud.com\/en\/public-cloud\/ai-training\/) :)\n\nThe script used for training can be found here: https:\/\/github.com\/jonatasgrosman\/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https:\/\/github.com\/jonatasgrosman\/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman\/wav2vec2-large-xlsr-53-english\")\naudio_paths = [\"\/path\/to\/file.mp3\", \"\/path\/to\/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"en\"\nMODEL_ID = \"jonatasgrosman\/wav2vec2-large-xlsr-53-english\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \"SHE'LL BE ALL RIGHT.\" | SHE'LL BE ALL RIGHT |\n| SIX | SIX |\n| \"ALL'S WELL THAT ENDS WELL.\" | ALL AS WELL THAT ENDS WELL |\n| DO YOU MEAN IT? | DO YOU MEAN IT |\n| THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE, BUT STILL CAUSES REGRESSIONS. | THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE BUT STILL CAUSES REGRESSION |\n| HOW IS MOZILLA GOING TO HANDLE AMBIGUITIES LIKE QUEUE AND CUE? | HOW IS MOSLILLAR GOING TO HANDLE ANDBEWOOTH HIS LIKE Q AND Q |\n| \"I GUESS YOU MUST THINK I'M KINDA BATTY.\" | RUSTIAN WASTIN PAN ONTE BATTLY |\n| NO ONE NEAR THE REMOTE MACHINE YOU COULD RING? | NO ONE NEAR THE REMOTE MACHINE YOU COULD RING |\n| SAUCE FOR THE GOOSE IS SAUCE FOR THE GANDER. | SAUCE FOR THE GUICE IS SAUCE FOR THE GONDER |\n| GROVES STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD. | GRAFS STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD |\n\n## Evaluation\n\n1. To evaluate on `mozilla-foundation\/common_voice_6_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman\/wav2vec2-large-xlsr-53-english --dataset mozilla-foundation\/common_voice_6_0 --config en --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2\/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman\/wav2vec2-large-xlsr-53-english --dataset speech-recognition-community-v2\/dev_data --config en --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-english,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {E}nglish},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english}},\n  year={2021}\n}\n```",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"google-bert\/bert-base-uncased",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"google-bert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2309308946
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2291464806
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1586026549
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2145729214
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2338194549
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2326330245
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "1810.04805"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1862080246
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "rust",
                    "onnx",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1072525531
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2166174054
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"to\nbe fine-tuned on a downstream task",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2647350132
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2058063149
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2048392296
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2213628292
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3222407103
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2484121919
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-02-19 11:06:12+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.195142746
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2265984267
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"3.454 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https:\/\/arxiv.org\/abs\/1810.04805) and first released in\n[this repository](https:\/\/github.com\/google-research\/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research\/bert readme](https:\/\/github.com\/google-research\/bert\/blob\/master\/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https:\/\/huggingface.co\/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https:\/\/huggingface.co\/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https:\/\/huggingface.co\/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https:\/\/huggingface.co\/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https:\/\/huggingface.co\/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https:\/\/huggingface.co\/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https:\/\/huggingface.co\/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https:\/\/huggingface.co\/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https:\/\/huggingface.co\/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https:\/\/yknzhu.wixsite.com\/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https:\/\/en.wikipedia.org\/wiki\/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m\/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6\/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals\/corr\/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs\/1810.04805},\n  year      = {2018},\n  url       = {http:\/\/arxiv.org\/abs\/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https:\/\/dblp.org\/rec\/journals\/corr\/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https:\/\/dblp.org}\n}\n```\n\n<a href=\"https:\/\/huggingface.co\/exbert\/?model=bert-base-uncased\">\n\t<img width=\"300px\" src=\"https:\/\/cdn-media.huggingface.co\/exbert\/button.png\">\n<\/a>\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"CAMeL-Lab\/bert-base-arabic-camelbert-mix-pos-egy",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"CAMeL-Lab",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "token classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"12",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2043287605
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"CAMeLBERT-Mix POS-EGY",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.001823682
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"12 datasets",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0240782257
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"more important than the pre-training data size",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000172436
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"Variant, Size, and Task Type",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0020815732
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"Variant, Size, and Task Type",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0270055942
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "2103.06678"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"scaled-down set of the MSA variant",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0009599383
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "ar"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0227857325
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"ARZTB",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1008567661
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"as part of the transformers pipeline",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3484931588
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"variant proximity of pre-training data to fine-tuning data is more important",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0535663776
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"scaled-down set of the MSA variant",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0954435542
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"transformers pipeline",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000001624
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.5144913197
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"optimized system selection model for the studied tasks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000023651
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2021-10-18 10:15:57+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"scaled-down set of the MSA variant",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0009104831
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"fourth language model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000132211
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"1.742 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: \n- ar\nlicense: apache-2.0\nwidget:\n - text: '\u0639\u0627\u0645\u0644 \u0627\u064a\u0647 \u061f'\n---\n# CAMeLBERT-Mix POS-EGY Model\n## Model description\n**CAMeLBERT-Mix POS-EGY Model** is a Egyptian Arabic POS tagging model that was built by fine-tuning the [CAMeLBERT-Mix](https:\/\/huggingface.co\/CAMeL-Lab\/bert-base-arabic-camelbert-mix\/) model.\nFor the fine-tuning, we used the ARZTB dataset .\nOur fine-tuning procedure and the hyperparameters we used can be found in our paper *\"[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https:\/\/arxiv.org\/abs\/2103.06678).\"* Our fine-tuning code can be found [here](https:\/\/github.com\/CAMeL-Lab\/CAMeLBERT).\n\n## Intended uses\nYou can use the CAMeLBERT-Mix POS-EGY model as part of the transformers pipeline.\nThis model will also be available in [CAMeL Tools](https:\/\/github.com\/CAMeL-Lab\/camel_tools) soon.\n\n#### How to use\nTo use the model with a transformers pipeline:\n```python\n>>> from transformers import pipeline\n>>> pos = pipeline('token-classification', model='CAMeL-Lab\/bert-base-arabic-camelbert-mix-pos-egy')\n>>> text = '\u0639\u0627\u0645\u0644 \u0627\u064a\u0647 \u061f'\n>>> pos(text)\n[{'entity': 'adj', 'score': 0.9972628, 'index': 1, 'word': '\u0639\u0627\u0645\u0644', 'start': 0, 'end': 4}, {'entity': 'pron_interrog', 'score': 0.9525163, 'index': 2, 'word': '\u0627\u064a\u0647', 'start': 5, 'end': 8}, {'entity': 'punc', 'score': 0.99869114, 'index': 3, 'word': '\u061f', 'start': 9, 'end': 10}]\n```\n*Note*: to download our models, you would need `transformers>=3.5.0`.\nOtherwise, you could download the models manually.\n## Citation\n```bibtex\n@inproceedings{inoue-etal-2021-interplay,\n    title = \"The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models\",\n    author = \"Inoue, Go  and\n      Alhafni, Bashar  and\n      Baimukan, Nurpeiis  and\n      Bouamor, Houda  and\n      Habash, Nizar\",\n    booktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\n    month = apr,\n    year = \"2021\",\n    address = \"Kyiv, Ukraine (Online)\",\n    publisher = \"Association for Computational Linguistics\",\n    abstract = \"In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.\",\n}\n```",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"tohoku-nlp\/bert-base-japanese",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"tohoku-nlp",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"WordPiece algorithm",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0397738144
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"BERT base model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4635543823
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"1M training steps",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000009218
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"12 layers, 768 dimensions of hidden states, and 12 attention heads",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000016511
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"1M",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0004916486
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"12 layers, 768 dimensions of hidden states, and 12 attention heads",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000005334
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"Japanese Wikipedia",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0235785451
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "cc-by-sa-4.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "ja"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"BERT base model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.001168997
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"WordPiece subword tokenization",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0002730886
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"to extract plain texts from a dump file of Wikipedia articles",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1391654462
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"12 layers, 768 dimensions of hidden states, and 12 attention heads",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000098053
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"input texts with word-level tokenization based on the IPA dictionary",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0036942891
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"1M training steps",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000084197
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"BERT base model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000011459
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"The codes for the pretraining",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000117722
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-02-22 00:57:00+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"To generate the training corpus",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000008523
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"BERT base model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000038746
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"1.433 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: ja\nlicense: cc-by-sa-4.0\ndatasets:\n- wikipedia\nwidget:\n- text: \u6771\u5317\u5927\u5b66\u3067[MASK]\u306e\u7814\u7a76\u3092\u3057\u3066\u3044\u307e\u3059\u3002\n---\n\n# BERT base Japanese (IPA dictionary)\n\nThis is a [BERT](https:\/\/github.com\/google-research\/bert) model pretrained on texts in the Japanese language.\n\nThis version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.\n\nThe codes for the pretraining are available at [cl-tohoku\/bert-japanese](https:\/\/github.com\/cl-tohoku\/bert-japanese\/tree\/v1.0).\n\n## Model architecture\n\nThe model architecture is the same as the original BERT base model; 12 layers, 768 dimensions of hidden states, and 12 attention heads.\n\n## Training Data\n\nThe model is trained on Japanese Wikipedia as of September 1, 2019.\nTo generate the training corpus, [WikiExtractor](https:\/\/github.com\/attardi\/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.\nThe text files used for the training are 2.6GB in size, consisting of approximately 17M sentences.\n\n## Tokenization\n\nThe texts are first tokenized by [MeCab](https:\/\/taku910.github.io\/mecab\/) morphological parser with the IPA dictionary and then split into subwords by the WordPiece algorithm.\nThe vocabulary size is 32000.\n\n## Training\n\nThe model is trained with the same configuration as the original BERT; 512 tokens per instance, 256 instances per batch, and 1M training steps.\n\n## Licenses\n\nThe pretrained models are distributed under the terms of the [Creative Commons Attribution-ShareAlike 3.0](https:\/\/creativecommons.org\/licenses\/by-sa\/3.0\/).\n\n## Acknowledgments\n\nFor training models, we used Cloud TPUs provided by [TensorFlow Research Cloud](https:\/\/www.tensorflow.org\/tfrc\/) program.\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"cardiffnlp\/twitter-roberta-base-sentiment-latest",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"cardiffnlp",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-15 01:21:58+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "text classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "tweet_eval"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"t = '@user'",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0057952991
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"RoBERTa-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0323258601
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"System Demonstrations",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000164255
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"251--260",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0310560744
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"124M",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000044645
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"251--260",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000353107
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "2202.03829"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"Dublin, Ireland",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0015904817
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0062449295
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"booktitle",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0003075268
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"System Demonstrations",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0207979754
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"System Demonstrations\",\n    month = may,\n    year = \"2022\",",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000167195
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"System Demonstrations",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0008730707
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"251--260",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0180039667
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Camacho-collados, Jose  and\n      Rezaee",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2177228332
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"month = may,\n    year = \"2022\"",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.005611592
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2023-05-28 05:45:10+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"publisher = \"Association for Computational Linguistics\"",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000559
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"2022",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000022444
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"1.001 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\nwidget:\n- text: Covid cases are increasing fast!\ndatasets:\n- tweet_eval\n---\n\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https:\/\/huggingface.co\/cardiffnlp\/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https:\/\/github.com\/cardiffnlp\/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https:\/\/arxiv.org\/abs\/2202.03829). \n- Git Repo: [TimeLMs official repository](https:\/\/github.com\/cardiffnlp\/timelms).\n\n<b>Labels<\/b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https:\/\/github.com\/cardiffnlp\/tweetnlp). You can access the demo [here](https:\/\/tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"Covid cases are increasing fast!\")\n```\n```\n[{'label': 'Negative', 'score': 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\nMODEL = f\"cardiffnlp\/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \"Covid cases are increasing fast!\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Covid cases are increasing fast!\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https:\/\/aclanthology.org\/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\n    author = \"Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https:\/\/aclanthology.org\/2022.acl-demo.25\",\n    doi = \"10.18653\/v1\/2022.acl-demo.25\",\n    pages = \"251--260\"\n}\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"sentence-transformers\/all-MiniLM-L6-v2",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "feature extraction",
                    "sentence similarity"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"128 tokens",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000141292
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"Sentence-Transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0015394479
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0049389293
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"automated evaluation of this model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000097561
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"TPU v3-8",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0011639487
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"TPU v3-8",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000022764
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "1904.06472",
                    "2102.07033",
                    "2104.08727",
                    "1704.05179",
                    "1810.09305"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"384 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001929416
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "sentence-transformers",
                    "pytorch",
                    "rust",
                    "safetensors",
                    "transformers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"Hugging Face",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3940327764
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"trivia_qa",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1737683713
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4735915959
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001389649
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3852038085
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"It maps sentences & paragraphs to a 384 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000810265
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Googles Flax, JAX, and Cloud team member",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000028655
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"It maps sentences & paragraphs to a 384 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000415065
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-03-27 09:43:07+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"self-supervised \ncontrastive learning objective",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000397042
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"2020",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000007232
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"0.455 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings\/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data\/sentence-compression\n- embedding-data\/flickr30k-captions\n- embedding-data\/altlex\n- embedding-data\/simple-wiki\n- embedding-data\/QQP\n- embedding-data\/SPECTER\n- embedding-data\/PAQ_pairs\n- embedding-data\/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https:\/\/www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https:\/\/www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers\/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https:\/\/www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers\/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https:\/\/seb.sbert.net](https:\/\/seb.sbert.net?model_name=sentence-transformers\/all-MiniLM-L6-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers\/MiniLM-L6-H384-uncased`](https:\/\/huggingface.co\/nreimers\/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX\/Flax for NLP & CV](https:\/\/discuss.huggingface.co\/t\/open-to-the-community-community-week-using-jax-flax-for-nlp-cv\/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https:\/\/discuss.huggingface.co\/t\/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs\/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers\/MiniLM-L6-H384-uncased`](https:\/\/huggingface.co\/nreimers\/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https:\/\/github.com\/PolyAI-LDN\/conversational-datasets\/tree\/master\/reddit) | [paper](https:\/\/arxiv.org\/abs\/1904.06472) | 726,484,430 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Abstracts) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 116,288,806 |\n| [WikiAnswers](https:\/\/github.com\/afader\/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https:\/\/doi.org\/10.1145\/2623330.2623677) | 77,427,422 |\n| [PAQ](https:\/\/github.com\/facebookresearch\/PAQ) (Question, Answer) pairs | [paper](https:\/\/arxiv.org\/abs\/2102.07033) | 64,371,441 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Titles) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 52,603,982 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) (Title, Abstract) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 41,769,185 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https:\/\/microsoft.github.io\/msmarco\/) triplets | [paper](https:\/\/doi.org\/10.1145\/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https:\/\/github.com\/allenai\/gooaq) | [paper](https:\/\/arxiv.org\/pdf\/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https:\/\/huggingface.co\/datasets\/code_search_net) | - | 1,151,414 |\n| [COCO](https:\/\/cocodataset.org\/#home) Image captions | [paper](https:\/\/link.springer.com\/chapter\/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https:\/\/github.com\/allenai\/specter) citation triplets | [paper](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Question, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Question) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https:\/\/huggingface.co\/datasets\/search_qa) | [paper](https:\/\/arxiv.org\/abs\/1704.05179) | 582,261 |\n| [Eli5](https:\/\/huggingface.co\/datasets\/eli5) | [paper](https:\/\/doi.org\/10.18653\/v1\/p19-1346) | 325,475 |\n| [Flickr 30k](https:\/\/shannon.cs.illinois.edu\/DenotationGraph\/) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/229\/33) | 317,695 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/) and [MultiNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) | [paper SNLI](https:\/\/doi.org\/10.18653\/v1\/d15-1075), [paper MultiNLI](https:\/\/doi.org\/10.18653\/v1\/n18-1101) | 277,230 | \n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https:\/\/github.com\/google-research-datasets\/sentence-compression) | [paper](https:\/\/www.aclweb.org\/anthology\/D13-1155\/) | 180,000 |\n| [Wikihow](https:\/\/github.com\/pvl\/wikihow_pairs_dataset) | [paper](https:\/\/arxiv.org\/abs\/1810.09305) | 128,542 |\n| [Altlex](https:\/\/github.com\/chridey\/altlex\/) | [paper](https:\/\/aclanthology.org\/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https:\/\/quoradata.quora.com\/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https:\/\/cs.pomona.edu\/~dkauchak\/simplification\/) | [paper](https:\/\/www.aclweb.org\/anthology\/P11-2117\/) | 102,225 |\n| [Natural Questions (NQ)](https:\/\/ai.google.com\/research\/NaturalQuestions) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/1455) | 100,231 |\n| [SQuAD2.0](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/) | [paper](https:\/\/aclanthology.org\/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https:\/\/huggingface.co\/datasets\/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"mrm8488\/distilroberta-finetuned-financial-news-sentiment-analysis",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"mrm8488",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "text classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "financial_phrasebank"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"agreement rate of 5-8 annotators",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.604581356
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"RoBERTa-base model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0788627937
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"hyperparameters",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0967469364
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"1.12.1\n- Tokenizers 0.10.3",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1959497184
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"The following hyperparameters",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.070288308
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"hyperparameters were used during training",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0008799654
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"4.10.2\n- Pytorch",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000108025
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "tensorboard",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"4.10.2\n- Pytorch",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0529571474
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"5-8 annotators",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0469943695
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"automatically according to the information the Trainer had access to",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000023749
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"Accuracy",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000047942
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"automatically according to the information the Trainer had access to",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0018882082
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"Framework versions\n\n- Transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0215090048
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Transformers 4.10.2\n- Pytorch",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000011827
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0085541168
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-01-21 15:17:58+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"Operating profit totaled EUR 9.4 mn",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000106973
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"Transformers 4.10.2\n- Pytorch",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0044604912
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"0.660 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlicense: apache-2.0\nthumbnail: https:\/\/huggingface.co\/mrm8488\/distilroberta-finetuned-financial-news-sentiment-analysis\/resolve\/main\/logo_no_bg.png\ntags:\n- generated_from_trainer\n- financial\n- stocks\n- sentiment\nwidget:\n- text: \"Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .\"\ndatasets:\n- financial_phrasebank\nmetrics:\n- accuracy\nmodel-index:\n- name: distilRoberta-financial-sentiment\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: financial_phrasebank\n      type: financial_phrasebank\n      args: sentences_allagree\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9823008849557522\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n<div style=\"text-align:center;width:250px;height:250px;\">\n    <img src=\"https:\/\/huggingface.co\/mrm8488\/distilroberta-finetuned-financial-news-sentiment-analysis\/resolve\/main\/logo_no_bg.png\" alt=\"logo\">\n<\/div>\n\n\n# DistilRoberta-financial-sentiment\n\n\nThis model is a fine-tuned version of [distilroberta-base](https:\/\/huggingface.co\/distilroberta-base) on the financial_phrasebank dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1116\n- Accuracy: **0.98**23\n\n## Base Model description\n\nThis model is a distilled version of the [RoBERTa-base model](https:\/\/huggingface.co\/roberta-base). It follows the same training procedure as [DistilBERT](https:\/\/huggingface.co\/distilbert-base-uncased).\nThe code for the distillation process can be found [here](https:\/\/github.com\/huggingface\/transformers\/tree\/master\/examples\/distillation).\nThis model is case-sensitive: it makes a difference between English and English.\n\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).\nOn average DistilRoBERTa is twice as fast as Roberta-base.\n\n## Training Data\n\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| No log        | 1.0   | 255  | 0.1670          | 0.9646   |\n| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |\n| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |\n| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |\n| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"pysentimiento\/robertuito-sentiment-analysis",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"pysentimiento",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"7235--7243",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000002178
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"RoBERTa",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4569787085
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"Macro F1 scores",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0034985146
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"{163--170}",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0117864879
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"7235--7243",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000164715
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"az-Galiano",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000073883
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"Spanish tweets",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0011761119
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "es"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "pytorch",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"pysentimiento import",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0006037897
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"English Twitter tasks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.007334095
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"to pre-train",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0331956074
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"for languages other than English, such models are not widely available",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000047682
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"TASS 2020 corpus",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000196328
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"cross-lingual abilities",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000158835
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Juan Carlos and Furman",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0717574134
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"cross-lingual abilities",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.001808758
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-04-07 22:46:56+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"state-of-the-art for natural language",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000167
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"2020",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0029962063
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"1.307 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: \n  - es\nlibrary_name: pysentimiento\n\ntags:\n  - twitter\n  - sentiment-analysis\n\n---\n# Sentiment Analysis in Spanish\n## robertuito-sentiment-analysis\n\nRepository: [https:\/\/github.com\/pysentimiento\/pysentimiento\/](https:\/\/github.com\/finiteautomata\/pysentimiento\/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https:\/\/github.com\/pysentimiento\/robertuito), a RoBERTa model trained in Spanish tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## Usage\n\nUse it directly with [pysentimiento](https:\/\/github.com\/pysentimiento\/pysentimiento)\n\n```python\nfrom pysentimiento import create_analyzer\nanalyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n\nanalyzer.predict(\"Qu\u00e9 gran jugador es Messi\")\n# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n```\n\n\n## Results\n\nResults for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores\n\n\n| model         | emotion       | hate_speech   | irony         | sentiment     |\n|:--------------|:--------------|:--------------|:--------------|:--------------|\n| robertuito    | 0.560 \u00b1 0.010 | 0.759 \u00b1 0.007 | 0.739 \u00b1 0.005 | 0.705 \u00b1 0.003 |\n| roberta       | 0.527 \u00b1 0.015 | 0.741 \u00b1 0.012 | 0.721 \u00b1 0.008 | 0.670 \u00b1 0.006 |\n| bertin        | 0.524 \u00b1 0.007 | 0.738 \u00b1 0.007 | 0.713 \u00b1 0.012 | 0.666 \u00b1 0.005 |\n| beto_uncased  | 0.532 \u00b1 0.012 | 0.727 \u00b1 0.016 | 0.701 \u00b1 0.007 | 0.651 \u00b1 0.006 |\n| beto_cased    | 0.516 \u00b1 0.012 | 0.724 \u00b1 0.012 | 0.705 \u00b1 0.009 | 0.662 \u00b1 0.005 |\n| mbert_uncased | 0.493 \u00b1 0.010 | 0.718 \u00b1 0.011 | 0.681 \u00b1 0.010 | 0.617 \u00b1 0.003 |\n| biGRU         | 0.264 \u00b1 0.007 | 0.592 \u00b1 0.018 | 0.631 \u00b1 0.011 | 0.585 \u00b1 0.011 |\n\n\nNote that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B\n\n## Citation\n\nIf you use this model in your research, please cite pysentimiento and RoBERTuito papers:\n\n```latex\n\n@article{perez2021pysentimiento,\n  title={pysentimiento: a python toolkit for opinion mining and social NLP tasks},\n  author={P{\\'e}rez, Juan Manuel and Rajngewerc, Mariela and Giudici, Juan Carlos and Furman, Dami{\\'a}n A and Luque, Franco and Alemany, Laura Alonso and Mart{\\'\\i}nez, Mar{\\'\\i}a Vanina},\n  journal={arXiv preprint arXiv:2106.09462},\n  year={2021}\n}\n\n@inproceedings{perez-etal-2022-robertuito,\n    title = \"{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish\",\n    author = \"P{\\'e}rez, Juan Manuel  and\n      Furman, Dami{\\'a}n Ariel  and\n      Alonso Alemany, Laura  and\n      Luque, Franco M.\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https:\/\/aclanthology.org\/2022.lrec-1.785\",\n    pages = \"7235--7243\",\n    abstract = \"Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.\",\n}\n\n@inproceedings{garcia2020overview,\n  title={Overview of TASS 2020: Introducing emotion detection},\n  author={Garc{\\'\\i}a-Vega, Manuel and D{\\'\\i}az-Galiano, MC and Garc{\\'\\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\\'a}ez, A and Jim{\\'e}nez-Zafra, SM and Mart{\\'\\i}nez C{\\'a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},\n  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\\'a}laga, Spain},\n  pages={163--170},\n  year={2020}\n}\n```",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"distilbert\/distilbert-base-uncased",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"distilbert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2358324528
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"distilbert-base-uncased",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3296915889
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2336604297
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.223574847
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2174940407
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2188214213
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "1910.01108"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2228890061
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "rust",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2178141028
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"lists, tables and headers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.322586298
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"to\nbe fine-tuned on a downstream task",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.5201228857
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2225318402
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"distilbert-base-uncased",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3293887377
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2296425551
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2334120721
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.22984761
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2023-08-18 14:59:41+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2240086198
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2278973311
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"1.530 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https:\/\/huggingface.co\/bert-base-uncased). It was\nintroduced in [this paper](https:\/\/arxiv.org\/abs\/1910.01108). The code for the distillation process can be found\n[here](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https:\/\/huggingface.co\/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https:\/\/huggingface.co\/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https:\/\/yknzhu.wixsite.com\/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https:\/\/en.wikipedia.org\/wiki\/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs\/1910.01108}\n}\n```\n\n<a href=\"https:\/\/huggingface.co\/exbert\/?model=distilbert-base-uncased\">\n\t<img width=\"300px\" src=\"https:\/\/cdn-media.huggingface.co\/exbert\/button.png\">\n<\/a>\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"FacebookAI\/roberta-base",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"FacebookAI",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"160GB of text",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000160792
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"RoBERTa base model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2005057633
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"When fine-tuned on downstream tasks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1101173833
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"## Evaluation results\n\nWhen fine-tuned on downstream tasks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000648217
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"{BERT} Pretraining Approach",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0012289248
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"59:33 +0200",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000008062
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "1907.11692",
                    "1806.02847"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"the reunion of five datasets",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000871156
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "rust",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"Hugging Face team",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0027579148
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"11,038 unpublished books",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0473945402
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"to be fine-tuned on a downstream task",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3081307709
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"This bias will also affect all fine-tuned versions of this model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.029398676
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"dynamically during pretraining",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0677449107
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"run the entire masked sentence through the model and has to predict\nthe masked words",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0002202895
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Yinhan Liu and\n               Myle Ott and\n               Naman Goyal",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0139182098
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"case-sensitive: it\nmakes a difference between english and English",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000188114
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-02-19 12:39:28+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"the masking is done dynamically during pretraining",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0004149544
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"RoBERTa model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000544042
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"2.815 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\ntags:\n- exbert\nlicense: mit\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https:\/\/arxiv.org\/abs\/1907.11692) and first released in\n[this repository](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https:\/\/huggingface.co\/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.<\/s>\",\n  'score': 0.3306540250778198,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a female model.<\/s>\",\n  'score': 0.04655390977859497,\n  'token': 2182,\n  'token_str': '\u0120female'},\n {'sequence': \"<s>Hello I'm a professional model.<\/s>\",\n  'score': 0.04232972860336304,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a fashion model.<\/s>\",\n  'score': 0.037216778844594955,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a Russian model.<\/s>\",\n  'score': 0.03253649175167084,\n  'token': 1083,\n  'token_str': '\u0120Russian'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.<\/s>',\n  'score': 0.08702439814805984,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a waiter.<\/s>',\n  'score': 0.0819653645157814,\n  'token': 38233,\n  'token_str': '\u0120waiter'},\n {'sequence': '<s>The man worked as a butcher.<\/s>',\n  'score': 0.073323555290699,\n  'token': 32364,\n  'token_str': '\u0120butcher'},\n {'sequence': '<s>The man worked as a miner.<\/s>',\n  'score': 0.046322137117385864,\n  'token': 18678,\n  'token_str': '\u0120miner'},\n {'sequence': '<s>The man worked as a guard.<\/s>',\n  'score': 0.040150221437215805,\n  'token': 2510,\n  'token_str': '\u0120guard'}]\n\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence': '<s>The Black woman worked as a waitress.<\/s>',\n  'score': 0.22177888453006744,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The Black woman worked as a prostitute.<\/s>',\n  'score': 0.19288744032382965,\n  'token': 36289,\n  'token_str': '\u0120prostitute'},\n {'sequence': '<s>The Black woman worked as a maid.<\/s>',\n  'score': 0.06498628109693527,\n  'token': 29754,\n  'token_str': '\u0120maid'},\n {'sequence': '<s>The Black woman worked as a secretary.<\/s>',\n  'score': 0.05375480651855469,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The Black woman worked as a nurse.<\/s>',\n  'score': 0.05245552211999893,\n  'token': 9008,\n  'token_str': '\u0120nurse'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https:\/\/yknzhu.wixsite.com\/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https:\/\/en.wikipedia.org\/wiki\/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https:\/\/commoncrawl.org\/2016\/10\/news-dataset-available\/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https:\/\/github.com\/jcpeterson\/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https:\/\/arxiv.org\/abs\/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `<\/s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals\/corr\/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs\/1907.11692},\n  year      = {2019},\n  url       = {http:\/\/arxiv.org\/abs\/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https:\/\/dblp.org\/rec\/journals\/corr\/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https:\/\/dblp.org}\n}\n```\n\n<a href=\"https:\/\/huggingface.co\/exbert\/?model=roberta-base\">\n\t<img width=\"300px\" src=\"https:\/\/cdn-media.huggingface.co\/exbert\/button.png\">\n<\/a>\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"sentence-transformers\/all-mpnet-base-v2",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "fill mask",
                    "feature extraction",
                    "sentence similarity"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"73,346",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0006769503
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"co\/microsoft\/mpnet-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0122603728
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0020365315
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"automated evaluation of this model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0068485029
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"2e-5",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000740973
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"TPU v3-8",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000027617
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "1904.06472",
                    "2102.07033",
                    "2104.08727",
                    "1704.05179",
                    "1810.09305"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"768 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0008807715
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "sentence-transformers",
                    "pytorch",
                    "safetensors",
                    "transformers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"Hugging Face",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2504135072
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1552921981
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4976650774
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.00020481
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.228872925
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"It maps sentences & paragraphs to a 768 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001034816
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Googles Flax, JAX, and Cloud team member",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0002120469
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"It maps sentences & paragraphs to a 768 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000482922
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-03-27 09:46:22+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"efficient hardware infrastructure",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001283781
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"train_script.py",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000001632
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"0.877 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings\/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data\/sentence-compression\n- embedding-data\/flickr30k-captions\n- embedding-data\/altlex\n- embedding-data\/simple-wiki\n- embedding-data\/QQP\n- embedding-data\/SPECTER\n- embedding-data\/PAQ_pairs\n- embedding-data\/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-mpnet-base-v2\nThis is a [sentence-transformers](https:\/\/www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https:\/\/www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers\/all-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https:\/\/www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/all-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers\/all-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https:\/\/seb.sbert.net](https:\/\/seb.sbert.net?model_name=sentence-transformers\/all-mpnet-base-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft\/mpnet-base`](https:\/\/huggingface.co\/microsoft\/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developped this model during the \n[Community week using JAX\/Flax for NLP & CV](https:\/\/discuss.huggingface.co\/t\/open-to-the-community-community-week-using-jax-flax-for-nlp-cv\/7104), \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https:\/\/discuss.huggingface.co\/t\/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs\/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft\/mpnet-base`](https:\/\/huggingface.co\/microsoft\/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https:\/\/github.com\/PolyAI-LDN\/conversational-datasets\/tree\/master\/reddit) | [paper](https:\/\/arxiv.org\/abs\/1904.06472) | 726,484,430 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Abstracts) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 116,288,806 |\n| [WikiAnswers](https:\/\/github.com\/afader\/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https:\/\/doi.org\/10.1145\/2623330.2623677) | 77,427,422 |\n| [PAQ](https:\/\/github.com\/facebookresearch\/PAQ) (Question, Answer) pairs | [paper](https:\/\/arxiv.org\/abs\/2102.07033) | 64,371,441 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Titles) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 52,603,982 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) (Title, Abstract) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 41,769,185 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https:\/\/microsoft.github.io\/msmarco\/) triplets | [paper](https:\/\/doi.org\/10.1145\/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https:\/\/github.com\/allenai\/gooaq) | [paper](https:\/\/arxiv.org\/pdf\/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https:\/\/huggingface.co\/datasets\/code_search_net) | - | 1,151,414 |\n| [COCO](https:\/\/cocodataset.org\/#home) Image captions | [paper](https:\/\/link.springer.com\/chapter\/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https:\/\/github.com\/allenai\/specter) citation triplets | [paper](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Question, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Question) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https:\/\/huggingface.co\/datasets\/search_qa) | [paper](https:\/\/arxiv.org\/abs\/1704.05179) | 582,261 |\n| [Eli5](https:\/\/huggingface.co\/datasets\/eli5) | [paper](https:\/\/doi.org\/10.18653\/v1\/p19-1346) | 325,475 |\n| [Flickr 30k](https:\/\/shannon.cs.illinois.edu\/DenotationGraph\/) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/229\/33) | 317,695 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/) and [MultiNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) | [paper SNLI](https:\/\/doi.org\/10.18653\/v1\/d15-1075), [paper MultiNLI](https:\/\/doi.org\/10.18653\/v1\/n18-1101) | 277,230 | \n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https:\/\/github.com\/google-research-datasets\/sentence-compression) | [paper](https:\/\/www.aclweb.org\/anthology\/D13-1155\/) | 180,000 |\n| [Wikihow](https:\/\/github.com\/pvl\/wikihow_pairs_dataset) | [paper](https:\/\/arxiv.org\/abs\/1810.09305) | 128,542 |\n| [Altlex](https:\/\/github.com\/chridey\/altlex\/) | [paper](https:\/\/aclanthology.org\/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https:\/\/quoradata.quora.com\/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https:\/\/cs.pomona.edu\/~dkauchak\/simplification\/) | [paper](https:\/\/www.aclweb.org\/anthology\/P11-2117\/) | 102,225 |\n| [Natural Questions (NQ)](https:\/\/ai.google.com\/research\/NaturalQuestions) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/1455) | 100,231 |\n| [SQuAD2.0](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/) | [paper](https:\/\/aclanthology.org\/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https:\/\/huggingface.co\/datasets\/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"microsoft\/layoutlmv3-base",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"microsoft",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-04-18 06:53:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"2022",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000745
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0056813145
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"Unified Text and Image Masking",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0005554198
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"simple unified architecture and training objectives",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000174257
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"Unified Text and Image Masking",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000010304
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000000989
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "2204.08387"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0042597656
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "cc-by-nc-sa-4.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "onnx",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"Yupan Huang",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0976630673
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"unified text and image masking",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001631105
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"general-purpose pre-trained model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0159575567
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"simple unified architecture and training objectives",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1089811623
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"pre-trained multimodal Transformer",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0205230359
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"unified text and image masking",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000171559
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Yupan Huang",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0323699936
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"simple unified architecture and training objectives",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000071852
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-04-10 14:20:22+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"licensed",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0054557058
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000030084
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"2.007 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\nlanguage: en\nlicense: cc-by-nc-sa-4.0\n\n---\n# LayoutLMv3\n\n[Microsoft Document AI](https:\/\/www.microsoft.com\/en-us\/research\/project\/document-ai\/) | [GitHub](https:\/\/aka.ms\/layoutlmv3)\n\n## Model description\n\nLayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.\n\n[LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https:\/\/arxiv.org\/abs\/2204.08387)\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, ACM Multimedia 2022.\n\n## Citation\n\nIf you find LayoutLM useful in your research, please cite the following paper:\n\n```\n@inproceedings{huang2022layoutlmv3,\n  author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},\n  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},\n  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},\n  year={2022}\n}\n```\n\n## License\n\nThe content of this project itself is licensed under the [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/).\nPortions of the source code are based on the [transformers](https:\/\/github.com\/huggingface\/transformers) project.\n[Microsoft Open Source Code of Conduct](https:\/\/opensource.microsoft.com\/codeofconduct)\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    },
    {
        "q_id_0_what is the name of the model?":[
            {
                "data":"FacebookAI\/roberta-large",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_1_what user shared the model?":[
            {
                "data":"FacebookAI",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_2_when was the model created\/uploaded?":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_3_what tasks can the model solve?":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_4_what datasets was the model trained on?":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_5_what was the split distribution for the datasets?":[
            {
                "data":"160GB of text",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000014259
            }
        ],
        "q_id_6_what datasets were used to finetune the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_7_what datasets were used to retrain the model?":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_8_what model is used as the base model?":[
            {
                "data":"roberta-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0228528995
            }
        ],
        "q_id_9_what evaluation metrics were used?":[
            {
                "data":"When fine-tuned on downstream tasks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1758357137
            }
        ],
        "q_id_10_what were the values of the evaluation metrics?":[
            {
                "data":"fine-tuned on downstream tasks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001692131
            }
        ],
        "q_id_11_what hyperparameters were optimized during the training process?":[
            {
                "data":"{BERT} Pretraining Approach",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0006740331
            }
        ],
        "q_id_12_what hyperparameters values were selected?":[
            {
                "data":"160GB of text",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000037635
            }
        ],
        "q_id_13_what publications are related to the model?":[
            {
                "data":[
                    "1907.11692",
                    "1806.02847"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_14_where is the model deployed?":[
            {
                "data":"1024 V100 GPUs",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001425457
            }
        ],
        "q_id_15_what use license is associated with the model?":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_16_what languages does the model work with?":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_17_what software libraries were used for the model to work?":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "onnx",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_18_who created the model?":[
            {
                "data":"RoBERTa",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0046710768
            }
        ],
        "q_id_19_what other datasets where used to test the model?":[
            {
                "data":"11,038 unpublished books",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0434553772
            }
        ],
        "q_id_20_what is the intended use of the model?":[
            {
                "data":"to be fine-tuned on a downstream task",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3056507111
            }
        ],
        "q_id_21_what are the risks and biases of the model?":[
            {
                "data":"This bias will also affect all fine-tuned versions of this model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0181400962
            }
        ],
        "q_id_22_how can the model be executed?":[
            {
                "data":"randomly masks 15% of the words in the input",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0160533506
            }
        ],
        "q_id_23_what are the processor requirements to run the model?":[
            {
                "data":"run the entire masked sentence through the model and has to predict\r\nthe masked words",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0002415474
            }
        ],
        "q_id_24_who are the authors of the model?":[
            {
                "data":"Yinhan Liu and\r\n               Myle Ott and\r\n               Naman Goyal",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0142007014
            }
        ],
        "q_id_25_what are the conditions of access of the model?":[
            {
                "data":"case-sensitive: it\r\nmakes a difference between english and English",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000213187
            }
        ],
        "q_id_26_when was the last time the model was updated?":[
            {
                "data":"2024-02-19 12:47:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_27_how is the project being funded?":[
            {
                "data":"the masking is done dynamically during pretraining",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000055584
            }
        ],
        "q_id_28_what is the current version of the model?":[
            {
                "data":"RoBERTa model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000029298
            }
        ],
        "q_id_29_what are the memory requirements of the model?":[
            {
                "data":"7.534 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ],
        "q_id_30_what are the readme.md file associated with the model?":[
            {
                "data":"---\r\nlanguage: en\r\ntags:\r\n- exbert\r\nlicense: mit\r\ndatasets:\r\n- bookcorpus\r\n- wikipedia\r\n---\r\n\r\n# RoBERTa large model\r\n\r\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\r\n[this paper](https:\/\/arxiv.org\/abs\/1907.11692) and first released in\r\n[this repository](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/roberta). This model is case-sensitive: it\r\nmakes a difference between english and English.\r\n\r\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\r\nthe Hugging Face team.\r\n\r\n## Model description\r\n\r\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\r\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\r\npublicly available data) with an automatic process to generate inputs and labels from those texts. \r\n\r\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\r\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\r\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\r\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\r\nlearn a bidirectional representation of the sentence.\r\n\r\nThis way, the model learns an inner representation of the English language that can then be used to extract features\r\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\r\nclassifier using the features produced by the BERT model as inputs.\r\n\r\n## Intended uses & limitations\r\n\r\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\r\nSee the [model hub](https:\/\/huggingface.co\/models?filter=roberta) to look for fine-tuned versions on a task that\r\ninterests you.\r\n\r\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\r\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\r\ngeneration you should look at model like GPT2.\r\n\r\n### How to use\r\n\r\nYou can use this model directly with a pipeline for masked language modeling:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"Hello I'm a <mask> model.\")\r\n\r\n[{'sequence': \"<s>Hello I'm a male model.<\/s>\",\r\n  'score': 0.3317350447177887,\r\n  'token': 2943,\r\n  'token_str': '\u0120male'},\r\n {'sequence': \"<s>Hello I'm a fashion model.<\/s>\",\r\n  'score': 0.14171843230724335,\r\n  'token': 2734,\r\n  'token_str': '\u0120fashion'},\r\n {'sequence': \"<s>Hello I'm a professional model.<\/s>\",\r\n  'score': 0.04291723668575287,\r\n  'token': 2038,\r\n  'token_str': '\u0120professional'},\r\n {'sequence': \"<s>Hello I'm a freelance model.<\/s>\",\r\n  'score': 0.02134818211197853,\r\n  'token': 18150,\r\n  'token_str': '\u0120freelance'},\r\n {'sequence': \"<s>Hello I'm a young model.<\/s>\",\r\n  'score': 0.021098261699080467,\r\n  'token': 664,\r\n  'token_str': '\u0120young'}]\r\n```\r\n\r\nHere is how to use this model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, RobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = RobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='pt')\r\noutput = model(**encoded_input)\r\n```\r\n\r\nand in TensorFlow:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, TFRobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = TFRobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='tf')\r\noutput = model(encoded_input)\r\n```\r\n\r\n### Limitations and bias\r\n\r\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\r\nneutral. Therefore, the model can have biased predictions:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"The man worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The man worked as a mechanic.<\/s>',\r\n  'score': 0.08260300755500793,\r\n  'token': 25682,\r\n  'token_str': '\u0120mechanic'},\r\n {'sequence': '<s>The man worked as a driver.<\/s>',\r\n  'score': 0.05736079439520836,\r\n  'token': 1393,\r\n  'token_str': '\u0120driver'},\r\n {'sequence': '<s>The man worked as a teacher.<\/s>',\r\n  'score': 0.04709019884467125,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The man worked as a bartender.<\/s>',\r\n  'score': 0.04641604796051979,\r\n  'token': 33080,\r\n  'token_str': '\u0120bartender'},\r\n {'sequence': '<s>The man worked as a waiter.<\/s>',\r\n  'score': 0.04239227622747421,\r\n  'token': 38233,\r\n  'token_str': '\u0120waiter'}]\r\n\r\n>>> unmasker(\"The woman worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The woman worked as a nurse.<\/s>',\r\n  'score': 0.2667474150657654,\r\n  'token': 9008,\r\n  'token_str': '\u0120nurse'},\r\n {'sequence': '<s>The woman worked as a waitress.<\/s>',\r\n  'score': 0.12280137836933136,\r\n  'token': 35698,\r\n  'token_str': '\u0120waitress'},\r\n {'sequence': '<s>The woman worked as a teacher.<\/s>',\r\n  'score': 0.09747499972581863,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The woman worked as a secretary.<\/s>',\r\n  'score': 0.05783602222800255,\r\n  'token': 2971,\r\n  'token_str': '\u0120secretary'},\r\n {'sequence': '<s>The woman worked as a cleaner.<\/s>',\r\n  'score': 0.05576248839497566,\r\n  'token': 16126,\r\n  'token_str': '\u0120cleaner'}]\r\n```\r\n\r\nThis bias will also affect all fine-tuned versions of this model.\r\n\r\n## Training data\r\n\r\nThe RoBERTa model was pretrained on the reunion of five datasets:\r\n- [BookCorpus](https:\/\/yknzhu.wixsite.com\/mbweb), a dataset consisting of 11,038 unpublished books;\r\n- [English Wikipedia](https:\/\/en.wikipedia.org\/wiki\/English_Wikipedia) (excluding lists, tables and headers) ;\r\n- [CC-News](https:\/\/commoncrawl.org\/2016\/10\/news-dataset-available\/), a dataset containing 63 millions English news\r\n  articles crawled between September 2016 and February 2019.\r\n- [OpenWebText](https:\/\/github.com\/jcpeterson\/openwebtext), an opensource recreation of the WebText dataset used to\r\n  train GPT-2,\r\n- [Stories](https:\/\/arxiv.org\/abs\/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\r\n  story-like style of Winograd schemas.\r\n\r\nTogether theses datasets weight 160GB of text.\r\n\r\n## Training procedure\r\n\r\n### Preprocessing\r\n\r\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\r\nthe model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked\r\nwith `<s>` and the end of one by `<\/s>`\r\n\r\nThe details of the masking procedure for each sentence are the following:\r\n- 15% of the tokens are masked.\r\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\r\n\r\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\r\n- In the 10% remaining cases, the masked tokens are left as is.\r\n\r\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\r\n\r\n### Pretraining\r\n\r\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\r\noptimizer used is Adam with a learning rate of 4e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\r\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 30,000 steps and linear decay of the learning\r\nrate after.\r\n\r\n## Evaluation results\r\n\r\nWhen fine-tuned on downstream tasks, this model achieves the following results:\r\n\r\nGlue test results:\r\n\r\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\r\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\r\n|      | 90.2 | 92.2 | 94.7 | 96.4  | 68.0 | 96.4  | 90.9 | 86.6 |\r\n\r\n\r\n### BibTeX entry and citation info\r\n\r\n```bibtex\r\n@article{DBLP:journals\/corr\/abs-1907-11692,\r\n  author    = {Yinhan Liu and\r\n               Myle Ott and\r\n               Naman Goyal and\r\n               Jingfei Du and\r\n               Mandar Joshi and\r\n               Danqi Chen and\r\n               Omer Levy and\r\n               Mike Lewis and\r\n               Luke Zettlemoyer and\r\n               Veselin Stoyanov},\r\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\r\n  journal   = {CoRR},\r\n  volume    = {abs\/1907.11692},\r\n  year      = {2019},\r\n  url       = {http:\/\/arxiv.org\/abs\/1907.11692},\r\n  archivePrefix = {arXiv},\r\n  eprint    = {1907.11692},\r\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\r\n  biburl    = {https:\/\/dblp.org\/rec\/journals\/corr\/abs-1907-11692.bib},\r\n  bibsource = {dblp computer science bibliography, https:\/\/dblp.org}\r\n}\r\n```\r\n\r\n<a href=\"https:\/\/huggingface.co\/exbert\/?model=roberta-base\">\r\n\t<img width=\"300px\" src=\"https:\/\/cdn-media.huggingface.co\/exbert\/button.png\">\r\n<\/a>\r\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0
            }
        ]
    }
]