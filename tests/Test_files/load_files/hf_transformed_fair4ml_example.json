[
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "bsd-3-clause"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"AudioSet classes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001380682,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Audio Spectrogram Transformer",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001132961,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"classifying audio into one of the AudioSet classes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2491736114,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "audio classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"several audio classification benchmarks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000010609,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"MIT",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"classifying audio into one of the AudioSet classes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0030048019,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/MIT\/ast-finetuned-audioset-10-10-0.4593",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"0.693 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"audio",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000017,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: bsd-3-clause\ntags:\n- audio-classification\n---\n\n# Audio Spectrogram Transformer (fine-tuned on AudioSet) \n\nAudio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper [AST: Audio Spectrogram Transformer](https:\/\/arxiv.org\/abs\/2104.01778) by Gong et al. and first released in [this repository](https:\/\/github.com\/YuanGongND\/ast). \n\nDisclaimer: The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Audio Spectrogram Transformer is equivalent to [ViT](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\n\n## Usage\n\nYou can use the raw model for classifying audio into one of the AudioSet classes. See the [documentation](https:\/\/huggingface.co\/docs\/transformers\/main\/en\/model_doc\/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"0.693 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/MIT\/ast-finetuned-audioset-10-10-0.4593\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/MIT\/ast-finetuned-audioset-10-10-0.4593\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "2104.01778"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Gong et al.",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1902675182,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-11-14 18:41:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2023-09-06 14:49:15+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-11-14 18:41:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/MIT\/ast-finetuned-audioset-10-10-0.4593\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"The team releasing Audio Spectrogram Transformer did not write a model card",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000002589,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "bsd-3-clause"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"MIT",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"Audio Spectrogram Transformer",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000003117,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/MIT\/ast-finetuned-audioset-10-10-0.4593",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"MIT\/ast-finetuned-audioset-10-10-0.4593",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/MIT\/ast-finetuned-audioset-10-10-0.4593",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.087618731,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Sentence-Transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0015299486,
                "extraction_time":"2024-07-16_09-14-41"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"a sentence and short paragraph encoder",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.5220896006,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "feature extraction",
                    "sentence similarity"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"can be used for tasks like clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001376511,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3831914663,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L12-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"0.401 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"It maps sentences & paragraphs to a 384 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000841634,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings\/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data\/sentence-compression\n- embedding-data\/flickr30k-captions\n- embedding-data\/altlex\n- embedding-data\/simple-wiki\n- embedding-data\/QQP\n- embedding-data\/SPECTER\n- embedding-data\/PAQ_pairs\n- embedding-data\/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L12-v2\nThis is a [sentence-transformers](https:\/\/www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https:\/\/www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers\/all-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https:\/\/www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/all-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers\/all-MiniLM-L12-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https:\/\/seb.sbert.net](https:\/\/seb.sbert.net?model_name=sentence-transformers\/all-MiniLM-L12-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft\/MiniLM-L12-H384-uncased`](https:\/\/huggingface.co\/microsoft\/MiniLM-L12-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developped this model during the \n[Community week using JAX\/Flax for NLP & CV](https:\/\/discuss.huggingface.co\/t\/open-to-the-community-community-week-using-jax-flax-for-nlp-cv\/7104), \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https:\/\/discuss.huggingface.co\/t\/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs\/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft\/MiniLM-L12-H384-uncased`](https:\/\/huggingface.co\/microsoft\/MiniLM-L12-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https:\/\/github.com\/PolyAI-LDN\/conversational-datasets\/tree\/master\/reddit) | [paper](https:\/\/arxiv.org\/abs\/1904.06472) | 726,484,430 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Abstracts) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 116,288,806 |\n| [WikiAnswers](https:\/\/github.com\/afader\/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https:\/\/doi.org\/10.1145\/2623330.2623677) | 77,427,422 |\n| [PAQ](https:\/\/github.com\/facebookresearch\/PAQ) (Question, Answer) pairs | [paper](https:\/\/arxiv.org\/abs\/2102.07033) | 64,371,441 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Titles) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 52,603,982 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) (Title, Abstract) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 41,769,185 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https:\/\/microsoft.github.io\/msmarco\/) triplets | [paper](https:\/\/doi.org\/10.1145\/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https:\/\/github.com\/allenai\/gooaq) | [paper](https:\/\/arxiv.org\/pdf\/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https:\/\/huggingface.co\/datasets\/code_search_net) | - | 1,151,414 |\n| [COCO](https:\/\/cocodataset.org\/#home) Image captions | [paper](https:\/\/link.springer.com\/chapter\/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https:\/\/github.com\/allenai\/specter) citation triplets | [paper](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Question, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Question) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https:\/\/huggingface.co\/datasets\/search_qa) | [paper](https:\/\/arxiv.org\/abs\/1704.05179) | 582,261 |\n| [Eli5](https:\/\/huggingface.co\/datasets\/eli5) | [paper](https:\/\/doi.org\/10.18653\/v1\/p19-1346) | 325,475 |\n| [Flickr 30k](https:\/\/shannon.cs.illinois.edu\/DenotationGraph\/) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/229\/33) | 317,695 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/) and [MultiNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) | [paper SNLI](https:\/\/doi.org\/10.18653\/v1\/d15-1075), [paper MultiNLI](https:\/\/doi.org\/10.18653\/v1\/n18-1101) | 277,230 | \n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https:\/\/github.com\/google-research-datasets\/sentence-compression) | [paper](https:\/\/www.aclweb.org\/anthology\/D13-1155\/) | 180,000 |\n| [Wikihow](https:\/\/github.com\/pvl\/wikihow_pairs_dataset) | [paper](https:\/\/arxiv.org\/abs\/1810.09305) | 128,542 |\n| [Altlex](https:\/\/github.com\/chridey\/altlex\/) | [paper](https:\/\/aclanthology.org\/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https:\/\/quoradata.quora.com\/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https:\/\/cs.pomona.edu\/~dkauchak\/simplification\/) | [paper](https:\/\/www.aclweb.org\/anthology\/P11-2117\/) | 102,225 |\n| [Natural Questions (NQ)](https:\/\/ai.google.com\/research\/NaturalQuestions) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/1455) | 100,231 |\n| [SQuAD2.0](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/) | [paper](https:\/\/aclanthology.org\/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https:\/\/huggingface.co\/datasets\/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "sentence-transformers",
                    "pytorch",
                    "rust",
                    "safetensors",
                    "transformers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"0.401 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L12-v2\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L12-v2\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "1904.06472",
                    "2102.07033",
                    "2104.08727",
                    "1704.05179",
                    "1810.09305"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Googles Flax, JAX, and Cloud team member",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000105025,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-03-26 14:05:34+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L12-v2\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"efficient hardware infrastructure",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0035555833,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"2020",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000025514,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L12-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"sentence-transformers\/all-MiniLM-L12-v2",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L12-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "cc-by-nc-4.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"French, Hindi and Polish",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2873133421,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"language identification model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001586784,
                "extraction_time":"2024-07-16_09-14-42"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"to predict the language of the input text",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.6774109602,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"this model can have biased predictions",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0347319692,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"facebook",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"reduced in size to even fit on mobile devices",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1378655434,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/facebook\/fasttext-language-identification",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"1.176 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"less than a few minutes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0002837083,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: cc-by-nc-4.0\nlibrary_name: fasttext\ntags:\n  - text-classification\n  - language-identification\n---\n\n# fastText (Language Identification)\n\nfastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in [this paper](https:\/\/arxiv.org\/abs\/1607.04606). The official website can be found [here](https:\/\/fasttext.cc\/).\n\nThis LID (Language IDentification) model is used to predict the language of the input text, and the hosted version (`lid218e`) was [released as part of the NLLB project](https:\/\/github.com\/facebookresearch\/fairseq\/blob\/nllb\/README.md#lid-model) and can detect 217 languages. You can find older versions (ones that can identify 157 languages) on the [official fastText website](https:\/\/fasttext.cc\/docs\/en\/language-identification.html).\n\n## Model description\n\nfastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.\n\nIt includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.\n\n## Intended uses & limitations\n\nYou can use pre-trained word vectors for text classification or language identification. See the [tutorials](https:\/\/fasttext.cc\/docs\/en\/supervised-tutorial.html) and [resources](https:\/\/fasttext.cc\/docs\/en\/english-vectors.html) on its official website to look for tasks that interest you.\n\n### How to use\n\nHere is how to use this model to detect the language of a given text:\n\n```python\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n\n>>> model_path = hf_hub_download(repo_id=\"facebook\/fasttext-language-identification\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.predict(\"Hello, world!\")\n\n(('__label__eng_Latn',), array([0.81148803]))\n\n>>> model.predict(\"Hello, world!\", k=5)\n\n(('__label__eng_Latn', '__label__vie_Latn', '__label__nld_Latn', '__label__pol_Latn', '__label__deu_Latn'), \n array([0.61224753, 0.21323682, 0.09696738, 0.01359863, 0.01319415]))\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. \n\nCosine similarity can be used to measure the similarity between two different word vectors. If two two vectors are identical, the cosine similarity will be 1. For two completely unrelated vectors, the value will be 0. If two vectors have an opposite relationship, the value will be -1.\n\n```python\n>>> import numpy as np\n\n>>> def cosine_similarity(word1, word2):\n>>>     return np.dot(model[word1], model[word2]) \/ (np.linalg.norm(model[word1]) * np.linalg.norm(model[word2]))\n\n>>> cosine_similarity(\"man\", \"boy\")\n\n0.061653383\n\n>>> cosine_similarity(\"man\", \"ceo\")\n\n0.11989131\n\n>>> cosine_similarity(\"woman\", \"ceo\")\n\n-0.08834904\n```\n\n## Training data\n\nPre-trained word vectors for 157 languages were trained on [Common Crawl](http:\/\/commoncrawl.org\/) and [Wikipedia](https:\/\/www.wikipedia.org\/) using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.\n\n## Training procedure\n\n### Tokenization\n\nWe used the [Stanford word segmenter](https:\/\/nlp.stanford.edu\/software\/segmenter.html) for Chinese, [Mecab](http:\/\/taku910.github.io\/mecab\/) for Japanese and [UETsegmenter](https:\/\/github.com\/phongnt570\/UETsegmenter) for Vietnamese. For languages using the Latin, Cyrillic, Hebrew or Greek scripts, we used the tokenizer from the [Europarl](https:\/\/www.statmt.org\/europarl\/) preprocessing tools. For the remaining languages, we used the ICU tokenizer.\n\nMore information about the training of these models can be found in the article [Learning Word Vectors for 157 Languages](https:\/\/arxiv.org\/abs\/1802.06893).\n\n### License\n\nThe language identification model is distributed under the [*Creative Commons Attribution-NonCommercial 4.0 International Public License*](https:\/\/creativecommons.org\/licenses\/by-nc\/4.0\/).\n\n### Evaluation datasets\n\nThe analogy evaluation datasets described in the paper are available here: [French](https:\/\/dl.fbaipublicfiles.com\/fasttext\/word-analogies\/questions-words-fr.txt), [Hindi](https:\/\/dl.fbaipublicfiles.com\/fasttext\/word-analogies\/questions-words-hi.txt), [Polish](https:\/\/dl.fbaipublicfiles.com\/fasttext\/word-analogies\/questions-words-pl.txt).\n\n### BibTeX entry and citation info\n\nPlease cite [1] if using this code for learning word representations or [2] if using for text classification.\n\n[1] P. Bojanowski\\*, E. Grave\\*, A. Joulin, T. Mikolov, [*Enriching Word Vectors with Subword Information*](https:\/\/arxiv.org\/abs\/1607.04606)\n\n```markup\n@article{bojanowski2016enriching,\n  title={Enriching Word Vectors with Subword Information},\n  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.04606},\n  year={2016}\n}\n```\n\n[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, [*Bag of Tricks for Efficient Text Classification*](https:\/\/arxiv.org\/abs\/1607.01759)\n\n```markup\n@article{joulin2016bag,\n  title={Bag of Tricks for Efficient Text Classification},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.01759},\n  year={2016}\n}\n```\n\n[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, T. Mikolov, [*FastText.zip: Compressing text classification models*](https:\/\/arxiv.org\/abs\/1612.03651)\n\n```markup\n@article{joulin2016fasttext,\n  title={FastText.zip: Compressing text classification models},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{'e}gou, H{'e}rve and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1612.03651},\n  year={2016}\n}\n```\n\nIf you use these word vectors, please cite the following paper:\n\n[4] E. Grave\\*, P. Bojanowski\\*, P. Gupta, A. Joulin, T. Mikolov, [*Learning Word Vectors for 157 Languages*](https:\/\/arxiv.org\/abs\/1802.06893)\n\n```markup\n@inproceedings{grave2018learning,\n  title={Learning Word Vectors for 157 Languages},\n  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},\n  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\n  year={2018}\n}\n```\n\n(\\* These authors contributed equally.)\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "fasttext"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"1.176 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/facebook\/fasttext-language-identification\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/facebook\/fasttext-language-identification\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "1607.04606",
                    "1802.06893",
                    "1607.01759",
                    "1612.03651"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Grave, Edouard and Bojanowski, Piotr and Gupta",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0702918395,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2023-03-06 12:52:50+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2023-06-09 12:39:43+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2023-03-06 12:52:50+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/facebook\/fasttext-language-identification\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"equally",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000068818,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "cc-by-nc-4.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"facebook",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"lid218e",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000050948,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/facebook\/fasttext-language-identification",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"facebook\/fasttext-language-identification",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/facebook\/fasttext-language-identification",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"trivia_qa",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1737683713,
                "extraction_time":"2024-07-16_09-14-44"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Sentence-Transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0015394479,
                "extraction_time":"2024-07-16_09-14-43"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4735915959,
                "extraction_time":"2024-07-16_09-14-44"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "feature extraction",
                    "sentence similarity"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001389649,
                "extraction_time":"2024-07-16_09-14-44"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3852038085,
                "extraction_time":"2024-07-16_09-14-44"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"0.455 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"It maps sentences & paragraphs to a 384 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000810265,
                "extraction_time":"2024-07-16_09-14-44"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings\/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data\/sentence-compression\n- embedding-data\/flickr30k-captions\n- embedding-data\/altlex\n- embedding-data\/simple-wiki\n- embedding-data\/QQP\n- embedding-data\/SPECTER\n- embedding-data\/PAQ_pairs\n- embedding-data\/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https:\/\/www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https:\/\/www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers\/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https:\/\/www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers\/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https:\/\/seb.sbert.net](https:\/\/seb.sbert.net?model_name=sentence-transformers\/all-MiniLM-L6-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers\/MiniLM-L6-H384-uncased`](https:\/\/huggingface.co\/nreimers\/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX\/Flax for NLP & CV](https:\/\/discuss.huggingface.co\/t\/open-to-the-community-community-week-using-jax-flax-for-nlp-cv\/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https:\/\/discuss.huggingface.co\/t\/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs\/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers\/MiniLM-L6-H384-uncased`](https:\/\/huggingface.co\/nreimers\/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https:\/\/github.com\/PolyAI-LDN\/conversational-datasets\/tree\/master\/reddit) | [paper](https:\/\/arxiv.org\/abs\/1904.06472) | 726,484,430 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Abstracts) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 116,288,806 |\n| [WikiAnswers](https:\/\/github.com\/afader\/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https:\/\/doi.org\/10.1145\/2623330.2623677) | 77,427,422 |\n| [PAQ](https:\/\/github.com\/facebookresearch\/PAQ) (Question, Answer) pairs | [paper](https:\/\/arxiv.org\/abs\/2102.07033) | 64,371,441 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Titles) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 52,603,982 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) (Title, Abstract) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 41,769,185 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https:\/\/microsoft.github.io\/msmarco\/) triplets | [paper](https:\/\/doi.org\/10.1145\/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https:\/\/github.com\/allenai\/gooaq) | [paper](https:\/\/arxiv.org\/pdf\/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https:\/\/huggingface.co\/datasets\/code_search_net) | - | 1,151,414 |\n| [COCO](https:\/\/cocodataset.org\/#home) Image captions | [paper](https:\/\/link.springer.com\/chapter\/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https:\/\/github.com\/allenai\/specter) citation triplets | [paper](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Question, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Question) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https:\/\/huggingface.co\/datasets\/search_qa) | [paper](https:\/\/arxiv.org\/abs\/1704.05179) | 582,261 |\n| [Eli5](https:\/\/huggingface.co\/datasets\/eli5) | [paper](https:\/\/doi.org\/10.18653\/v1\/p19-1346) | 325,475 |\n| [Flickr 30k](https:\/\/shannon.cs.illinois.edu\/DenotationGraph\/) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/229\/33) | 317,695 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/) and [MultiNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) | [paper SNLI](https:\/\/doi.org\/10.18653\/v1\/d15-1075), [paper MultiNLI](https:\/\/doi.org\/10.18653\/v1\/n18-1101) | 277,230 | \n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https:\/\/github.com\/google-research-datasets\/sentence-compression) | [paper](https:\/\/www.aclweb.org\/anthology\/D13-1155\/) | 180,000 |\n| [Wikihow](https:\/\/github.com\/pvl\/wikihow_pairs_dataset) | [paper](https:\/\/arxiv.org\/abs\/1810.09305) | 128,542 |\n| [Altlex](https:\/\/github.com\/chridey\/altlex\/) | [paper](https:\/\/aclanthology.org\/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https:\/\/quoradata.quora.com\/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https:\/\/cs.pomona.edu\/~dkauchak\/simplification\/) | [paper](https:\/\/www.aclweb.org\/anthology\/P11-2117\/) | 102,225 |\n| [Natural Questions (NQ)](https:\/\/ai.google.com\/research\/NaturalQuestions) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/1455) | 100,231 |\n| [SQuAD2.0](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/) | [paper](https:\/\/aclanthology.org\/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https:\/\/huggingface.co\/datasets\/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "sentence-transformers",
                    "pytorch",
                    "rust",
                    "onnx",
                    "safetensors",
                    "transformers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"0.455 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "1904.06472",
                    "2102.07033",
                    "2104.08727",
                    "1704.05179",
                    "1810.09305"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Googles Flax, JAX, and Cloud team member",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000028655,
                "extraction_time":"2024-07-16_09-14-45"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-05-29 14:43:28+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"self-supervised \ncontrastive learning objective",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000397042,
                "extraction_time":"2024-07-16_09-14-45"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"2020",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000007232,
                "extraction_time":"2024-07-16_09-14-45"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"sentence-transformers\/all-MiniLM-L6-v2",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2166174054,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2291464806,
                "extraction_time":"2024-07-16_09-14-45"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"to\nbe fine-tuned on a downstream task",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2647350132,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2058063149,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"google-bert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2048392296,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/google-bert\/bert-base-uncased",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"3.454 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2213628292,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https:\/\/arxiv.org\/abs\/1810.04805) and first released in\n[this repository](https:\/\/github.com\/google-research\/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research\/bert readme](https:\/\/github.com\/google-research\/bert\/blob\/master\/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https:\/\/huggingface.co\/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https:\/\/huggingface.co\/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https:\/\/huggingface.co\/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https:\/\/huggingface.co\/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https:\/\/huggingface.co\/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https:\/\/huggingface.co\/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https:\/\/huggingface.co\/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https:\/\/huggingface.co\/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https:\/\/huggingface.co\/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https:\/\/yknzhu.wixsite.com\/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https:\/\/en.wikipedia.org\/wiki\/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m\/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6\/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals\/corr\/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs\/1810.04805},\n  year      = {2018},\n  url       = {http:\/\/arxiv.org\/abs\/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https:\/\/dblp.org\/rec\/journals\/corr\/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https:\/\/dblp.org}\n}\n```\n\n<a href=\"https:\/\/huggingface.co\/exbert\/?model=bert-base-uncased\">\n\t<img width=\"300px\" src=\"https:\/\/cdn-media.huggingface.co\/exbert\/button.png\">\n<\/a>\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "rust",
                    "onnx",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"3.454 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/google-bert\/bert-base-uncased\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/google-bert\/bert-base-uncased\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "1810.04805"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3222407103,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-02-19 11:06:12+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/google-bert\/bert-base-uncased\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.195142746,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"google-bert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2265984267,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/google-bert\/bert-base-uncased",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"google-bert\/bert-base-uncased",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/google-bert\/bert-base-uncased",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"pre-existing image datasets",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0449205376,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"ViT-L\/14 Transformer architecture",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3469974101,
                "extraction_time":"2024-07-16_09-14-46"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2787895799,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "zero shot image classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0964859426,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"The model card is taken and modified from the official CLIP repository",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.025931837,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-large-patch14",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"6.847 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0114825713,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\ntags:\n- vision\nwidget:\n- src: https:\/\/huggingface.co\/datasets\/mishig\/sample_images\/resolve\/main\/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https:\/\/github.com\/openai\/CLIP\/blob\/main\/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L\/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https:\/\/openai.com\/blog\/clip\/)\n- [CLIP Paper](https:\/\/arxiv.org\/abs\/2103.00020)\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai\/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai\/clip-vit-large-patch14\")\n\nurl = \"http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http:\/\/projects.dfki.uni-kl.de\/yfcc100m\/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https:\/\/arxiv.org\/abs\/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement\/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https:\/\/forms.gle\/Uv7afRH5dvY34ZEs9)",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"6.847 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-large-patch14\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-large-patch14\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "2103.00020",
                    "1908.04913"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"researchers at OpenAI",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0020058858,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2023-09-15 15:49:35+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-large-patch14\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"The model is intended as a research output for research communities",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000036503,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"gle\/Uv7afRH5dvY34ZEs9",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000165806,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-large-patch14",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"openai\/clip-vit-large-patch14",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-large-patch14",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"import Audio",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0175927021,
                "extraction_time":"2024-07-16_09-14-48"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Whisper\n\nWhisper",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0257400088,
                "extraction_time":"2024-07-16_09-14-47"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"to transcribe and translate speech",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4854536653,
                "extraction_time":"2024-07-16_09-14-49"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "automatic speech recognition"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"robustness, generalization, capabilities, biases, and constraints",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.263910681,
                "extraction_time":"2024-07-16_09-14-49"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"through Transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.197304219,
                "extraction_time":"2024-07-16_09-14-49"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/openai\/whisper-small",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"3.873 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"Robust Speech Recognition via Large-Scale Weak Supervision",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0026974028,
                "extraction_time":"2024-07-16_09-14-49"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: \n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- no\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https:\/\/cdn-media.huggingface.co\/speech_samples\/sample1.flac\n- example_title: Librispeech sample 2\n  src: https:\/\/cdn-media.huggingface.co\/speech_samples\/sample2.flac\nmodel-index:\n- name: whisper-small\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (clean)\n      type: librispeech_asr\n      config: clean\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 3.432213777886737\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (other)\n      type: librispeech_asr\n      config: other\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 7.628304527060248\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 11.0\n      type: mozilla-foundation\/common_voice_11_0\n      config: hi\n      split: test\n      args:\n        language: hi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 87.3\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 13.0\n      type: mozilla-foundation\/common_voice_13_0\n      config: dv\n      split: test\n      args:\n        language: dv\n    metrics:\n    - name: Wer\n      type: wer\n      value: 125.69809089960707\npipeline_tag: automatic-speech-recognition\nlicense: apache-2.0\n---\n\n# Whisper\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours \nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains **without** the need \nfor fine-tuning.\n\nWhisper was proposed in the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https:\/\/arxiv.org\/abs\/2212.04356) \nby Alec Radford et al from OpenAI. The original code repository can be found [here](https:\/\/github.com\/openai\/whisper).\n\n**Disclaimer**: Content for this model card has partly been written by the Hugging Face team, and parts of it were \ncopied and pasted from the original model card.\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. \nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. \n\nThe models were trained on either English-only data or multilingual data. The English-only models were trained \non the task of speech recognition. The multilingual models were trained on both speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. \nFor speech translation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https:\/\/huggingface.co\/models?search=openai\/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [\u2713](https:\/\/huggingface.co\/openai\/whisper-tiny.en)   | [\u2713](https:\/\/huggingface.co\/openai\/whisper-tiny)     |\n| base     | 74 M       | [\u2713](https:\/\/huggingface.co\/openai\/whisper-base.en)   | [\u2713](https:\/\/huggingface.co\/openai\/whisper-base)     |\n| small    | 244 M      | [\u2713](https:\/\/huggingface.co\/openai\/whisper-small.en)  | [\u2713](https:\/\/huggingface.co\/openai\/whisper-small)    |\n| medium   | 769 M      | [\u2713](https:\/\/huggingface.co\/openai\/whisper-medium.en) | [\u2713](https:\/\/huggingface.co\/openai\/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [\u2713](https:\/\/huggingface.co\/openai\/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [\u2713](https:\/\/huggingface.co\/openai\/whisper-large-v2) |\n\n# Usage\n\nTo transcribe audio samples, the model has to be used alongside a [`WhisperProcessor`](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/whisper#transformers.WhisperProcessor).\n\nThe `WhisperProcessor` is used to:\n1. Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n2. Post-process the model outputs (converting them from tokens to text)\n\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens \nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\n1. The transcription always starts with the `<|startoftranscript|>` token\n2. The second token is the language token (e.g. `<|en|>` for English)\n3. The third token is the \"task token\". It can take one of two values: `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation\n4. In addition, a `<|notimestamps|>` token is added if the model should not include timestamp prediction\n\nThus, a typical sequence of context tokens might look as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n```\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\n\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at \neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced, \nthe Whisper model will automatically predict the output langauge and task itself.\n\nThe context tokens can be set accordingly:\n\n```python\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n```\n\nWhich forces the model to predict in English under the task of speech recognition.\n\n## Transcription\n\n### English to English \nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai\/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai\/whisper-small\")\n>>> model.config.forced_decoder_ids = None\n\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n```\nThe context tokens can be removed from the start of the transcription by setting `skip_special_tokens=True`.\n\n### French to French \nThe following example demonstrates French to French transcription by setting the decoder ids appropriately. \n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai\/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai\/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.']\n```\n\n## Translation \nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\n\n### French to English\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai\/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai\/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\n```\n\n## Evaluation\n\nThis code snippet shows how to evaluate Whisper Small on [LibriSpeech test-clean](https:\/\/huggingface.co\/datasets\/librispeech_asr):\n \n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai\/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai\/whisper-small\").to(\"cuda\")\n\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>> \n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n\n>>> result = librispeech_test_clean.map(map_to_pred)\n\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n3.432213777886737\n```\n\n## Long-Form Transcription\n\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking \nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers \n[`pipeline`](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/pipelines#transformers.AutomaticSpeechRecognitionPipeline) \nmethod. Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline. With chunking enabled, the pipeline \ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing `return_timestamps=True`:\n\n```python\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai\/whisper-small\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n\n>>> ds = load_dataset(\"hf-internal-testing\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n  'timestamp': (0.0, 5.44)}]\n```\n\nRefer to the blog post [ASR Chunking](https:\/\/huggingface.co\/blog\/asr-chunking) for more details on the chunking algorithm.\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with \ud83e\udd17 Transformers](https:\/\/huggingface.co\/blog\/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https:\/\/cdn.openai.com\/papers\/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and\/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https:\/\/cdn.openai.com\/papers\/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https:\/\/cdn.openai.com\/papers\/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and\/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models\u2019 transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box \u2013 their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550\/ARXIV.2212.04356},\n  url = {https:\/\/arxiv.org\/abs\/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"3.873 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/openai\/whisper-small\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/openai\/whisper-small\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "2212.04356"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Alec Radford et al",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1524628699,
                "extraction_time":"2024-07-16_09-14-49"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-09-26 06:51:27+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-02-29 10:57:38+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-09-26 06:51:27+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/openai\/whisper-small\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"through Transformers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000918519,
                "extraction_time":"2024-07-16_09-14-49"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"Whisper\n\nWhisper",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000563442,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/openai\/whisper-small",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"openai\/whisper-small",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/openai\/whisper-small",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"pre-existing image datasets",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0449205376,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"accuracy",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000220152,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2787895799,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "zero shot image classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0964859426,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"The model card is taken and modified from the official CLIP repository",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0204826538,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch32",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"1.820 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0114825713,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\ntags:\n- vision\nwidget:\n- src: https:\/\/huggingface.co\/datasets\/mishig\/sample_images\/resolve\/main\/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https:\/\/github.com\/openai\/CLIP\/blob\/main\/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe model uses a ViT-B\/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https:\/\/openai.com\/blog\/clip\/)\n- [CLIP Paper](https:\/\/arxiv.org\/abs\/2103.00020)\n\n\n### Use with Transformers\n\n```python3\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai\/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai\/clip-vit-base-patch32\")\n\nurl = \"http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http:\/\/projects.dfki.uni-kl.de\/yfcc100m\/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https:\/\/arxiv.org\/abs\/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement\/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https:\/\/forms.gle\/Uv7afRH5dvY34ZEs9)",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"1.820 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch32\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch32\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "2103.00020",
                    "1908.04913"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"researchers at OpenAI",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0020362833,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-02-29 09:45:55+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch32\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"The model is intended as a research output for research communities",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000036503,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"gle\/Uv7afRH5dvY34ZEs9",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000165806,
                "extraction_time":"2024-07-16_09-14-50"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch32",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"openai\/clip-vit-base-patch32",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch32",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1552921981,
                "extraction_time":"2024-07-16_09-14-51"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"co\/microsoft\/mpnet-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0122603728,
                "extraction_time":"2024-07-16_09-14-51"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4976650774,
                "extraction_time":"2024-07-16_09-14-51"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "fill mask",
                    "feature extraction",
                    "sentence similarity"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.00020481,
                "extraction_time":"2024-07-16_09-14-51"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"clustering or semantic search",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.228872925,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":[
                    "s2orc",
                    "flax-sentence-embeddings\/stackexchange_xml",
                    "ms_marco",
                    "gooaq",
                    "yahoo_answers_topics",
                    "code_search_net",
                    "search_qa",
                    "eli5",
                    "snli",
                    "multi_nli",
                    "wikihow",
                    "natural_questions",
                    "trivia_qa",
                    "embedding-data\/sentence-compression",
                    "embedding-data\/flickr30k-captions",
                    "embedding-data\/altlex",
                    "embedding-data\/simple-wiki",
                    "embedding-data\/QQP",
                    "embedding-data\/SPECTER",
                    "embedding-data\/PAQ_pairs",
                    "embedding-data\/WikiAnswers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-mpnet-base-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"0.877 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"It maps sentences & paragraphs to a 768 dimensional dense vector space",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001034816,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings\/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data\/sentence-compression\n- embedding-data\/flickr30k-captions\n- embedding-data\/altlex\n- embedding-data\/simple-wiki\n- embedding-data\/QQP\n- embedding-data\/SPECTER\n- embedding-data\/PAQ_pairs\n- embedding-data\/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-mpnet-base-v2\nThis is a [sentence-transformers](https:\/\/www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https:\/\/www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers\/all-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https:\/\/www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/all-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers\/all-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https:\/\/seb.sbert.net](https:\/\/seb.sbert.net?model_name=sentence-transformers\/all-mpnet-base-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft\/mpnet-base`](https:\/\/huggingface.co\/microsoft\/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developped this model during the \n[Community week using JAX\/Flax for NLP & CV](https:\/\/discuss.huggingface.co\/t\/open-to-the-community-community-week-using-jax-flax-for-nlp-cv\/7104), \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https:\/\/discuss.huggingface.co\/t\/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs\/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft\/mpnet-base`](https:\/\/huggingface.co\/microsoft\/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https:\/\/github.com\/PolyAI-LDN\/conversational-datasets\/tree\/master\/reddit) | [paper](https:\/\/arxiv.org\/abs\/1904.06472) | 726,484,430 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Abstracts) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 116,288,806 |\n| [WikiAnswers](https:\/\/github.com\/afader\/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https:\/\/doi.org\/10.1145\/2623330.2623677) | 77,427,422 |\n| [PAQ](https:\/\/github.com\/facebookresearch\/PAQ) (Question, Answer) pairs | [paper](https:\/\/arxiv.org\/abs\/2102.07033) | 64,371,441 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) Citation pairs (Titles) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 52,603,982 |\n| [S2ORC](https:\/\/github.com\/allenai\/s2orc) (Title, Abstract) | [paper](https:\/\/aclanthology.org\/2020.acl-main.447\/) | 41,769,185 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https:\/\/microsoft.github.io\/msmarco\/) triplets | [paper](https:\/\/doi.org\/10.1145\/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https:\/\/github.com\/allenai\/gooaq) | [paper](https:\/\/arxiv.org\/pdf\/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https:\/\/huggingface.co\/datasets\/code_search_net) | - | 1,151,414 |\n| [COCO](https:\/\/cocodataset.org\/#home) Image captions | [paper](https:\/\/link.springer.com\/chapter\/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https:\/\/github.com\/allenai\/specter) citation triplets | [paper](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Question, Answer) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https:\/\/www.kaggle.com\/soumikrakshit\/yahoo-answers-dataset) (Title, Question) | [paper](https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https:\/\/huggingface.co\/datasets\/search_qa) | [paper](https:\/\/arxiv.org\/abs\/1704.05179) | 582,261 |\n| [Eli5](https:\/\/huggingface.co\/datasets\/eli5) | [paper](https:\/\/doi.org\/10.18653\/v1\/p19-1346) | 325,475 |\n| [Flickr 30k](https:\/\/shannon.cs.illinois.edu\/DenotationGraph\/) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/229\/33) | 317,695 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/) and [MultiNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) | [paper SNLI](https:\/\/doi.org\/10.18653\/v1\/d15-1075), [paper MultiNLI](https:\/\/doi.org\/10.18653\/v1\/n18-1101) | 277,230 | \n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https:\/\/huggingface.co\/datasets\/flax-sentence-embeddings\/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https:\/\/github.com\/google-research-datasets\/sentence-compression) | [paper](https:\/\/www.aclweb.org\/anthology\/D13-1155\/) | 180,000 |\n| [Wikihow](https:\/\/github.com\/pvl\/wikihow_pairs_dataset) | [paper](https:\/\/arxiv.org\/abs\/1810.09305) | 128,542 |\n| [Altlex](https:\/\/github.com\/chridey\/altlex\/) | [paper](https:\/\/aclanthology.org\/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https:\/\/quoradata.quora.com\/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https:\/\/cs.pomona.edu\/~dkauchak\/simplification\/) | [paper](https:\/\/www.aclweb.org\/anthology\/P11-2117\/) | 102,225 |\n| [Natural Questions (NQ)](https:\/\/ai.google.com\/research\/NaturalQuestions) | [paper](https:\/\/transacl.org\/ojs\/index.php\/tacl\/article\/view\/1455) | 100,231 |\n| [SQuAD2.0](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/) | [paper](https:\/\/aclanthology.org\/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https:\/\/huggingface.co\/datasets\/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "sentence-transformers",
                    "pytorch",
                    "safetensors",
                    "transformers"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"0.877 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-mpnet-base-v2\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-mpnet-base-v2\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "1904.06472",
                    "2102.07033",
                    "2104.08727",
                    "1704.05179",
                    "1810.09305"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Googles Flax, JAX, and Cloud team member",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0002120469,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-03-27 09:46:22+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-mpnet-base-v2\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"efficient hardware infrastructure",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001283781,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"sentence-transformers",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"train_script.py",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.000001632,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-mpnet-base-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"sentence-transformers\/all-mpnet-base-v2",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/sentence-transformers\/all-mpnet-base-v2",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"The model can be used directly (without a language model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0108902687,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"XLSR-53",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0021992121,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"speech recognition",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1605238169,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "automatic speech recognition"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"english --dataset mozilla-foundation",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000305167,
                "extraction_time":"2024-07-16_09-14-52"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"jonatasgrosman",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "common_voice",
                    "mozilla-foundation\/common_voice_6_0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "common_voice",
                    "mozilla-foundation\/common_voice_6_0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"directly",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2019733042,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":[
                    "common_voice",
                    "mozilla-foundation\/common_voice_6_0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"4.656 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"Wav2Vec2Processor",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.5176950693,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\ndatasets:\n- common_voice\n- mozilla-foundation\/common_voice_6_0\nmetrics:\n- wer\n- cer\ntags:\n- audio\n- automatic-speech-recognition\n- en\n- hf-asr-leaderboard\n- mozilla-foundation\/common_voice_6_0\n- robust-speech-event\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 English by Jonatas Grosman\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice en\n      type: common_voice\n      args: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 19.06\n    - name: Test CER\n      type: cer\n      value: 7.69\n    - name: Test WER (+LM)\n      type: wer\n      value: 14.81\n    - name: Test CER (+LM)\n      type: cer\n      value: 6.84\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2\/dev_data\n      args: en\n    metrics:\n    - name: Dev WER\n      type: wer\n      value: 27.72\n    - name: Dev CER\n      type: cer\n      value: 11.65\n    - name: Dev WER (+LM)\n      type: wer\n      value: 20.85\n    - name: Dev CER (+LM)\n      type: cer\n      value: 11.01\n---\n\n# Fine-tuned XLSR-53 large model for speech recognition in English\n\nFine-tuned [facebook\/wav2vec2-large-xlsr-53](https:\/\/huggingface.co\/facebook\/wav2vec2-large-xlsr-53) on English using the train and validation splits of [Common Voice 6.1](https:\/\/huggingface.co\/datasets\/common_voice).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https:\/\/www.ovhcloud.com\/en\/public-cloud\/ai-training\/) :)\n\nThe script used for training can be found here: https:\/\/github.com\/jonatasgrosman\/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https:\/\/github.com\/jonatasgrosman\/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman\/wav2vec2-large-xlsr-53-english\")\naudio_paths = [\"\/path\/to\/file.mp3\", \"\/path\/to\/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"en\"\nMODEL_ID = \"jonatasgrosman\/wav2vec2-large-xlsr-53-english\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \"SHE'LL BE ALL RIGHT.\" | SHE'LL BE ALL RIGHT |\n| SIX | SIX |\n| \"ALL'S WELL THAT ENDS WELL.\" | ALL AS WELL THAT ENDS WELL |\n| DO YOU MEAN IT? | DO YOU MEAN IT |\n| THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE, BUT STILL CAUSES REGRESSIONS. | THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE BUT STILL CAUSES REGRESSION |\n| HOW IS MOZILLA GOING TO HANDLE AMBIGUITIES LIKE QUEUE AND CUE? | HOW IS MOSLILLAR GOING TO HANDLE ANDBEWOOTH HIS LIKE Q AND Q |\n| \"I GUESS YOU MUST THINK I'M KINDA BATTY.\" | RUSTIAN WASTIN PAN ONTE BATTLY |\n| NO ONE NEAR THE REMOTE MACHINE YOU COULD RING? | NO ONE NEAR THE REMOTE MACHINE YOU COULD RING |\n| SAUCE FOR THE GOOSE IS SAUCE FOR THE GANDER. | SAUCE FOR THE GUICE IS SAUCE FOR THE GONDER |\n| GROVES STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD. | GRAFS STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD |\n\n## Evaluation\n\n1. To evaluate on `mozilla-foundation\/common_voice_6_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman\/wav2vec2-large-xlsr-53-english --dataset mozilla-foundation\/common_voice_6_0 --config en --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2\/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman\/wav2vec2-large-xlsr-53-english --dataset speech-recognition-community-v2\/dev_data --config en --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-english,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {E}nglish},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english}},\n  year={2021}\n}\n```",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"4.656 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Grosman, Jonatas",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.7634411454,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2023-03-25 10:56:55+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"fine-tuned",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000148449,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"jonatasgrosman",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"2021",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000155617,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"jonatasgrosman\/wav2vec2-large-xlsr-53-english",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/wav2vec2-large-xlsr-53-english",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"gender, race and age classification as well as denigration harms",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0529832691,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"ViT-B\/16 Transformer architecture",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3371519744,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"a research output for research communities",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2124619931,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "zero shot image classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"can depend significantly on class design",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0683842078,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"The model card is taken and modified from the official CLIP repository",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0263923388,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch16",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"1.201 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"to evaluate performance of the model across people and surface potential risks",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0500589609,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\ntags:\n- vision\nwidget:\n- src: https:\/\/huggingface.co\/datasets\/mishig\/sample_images\/resolve\/main\/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n# Model Card: CLIP\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https:\/\/github.com\/openai\/CLIP\/blob\/main\/model-card.md).\n\n\n## Model Details\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n\n### Model Date\nJanuary 2021\n\n\n### Model Type\nThe base model uses a ViT-B\/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n- [Blog Post](https:\/\/openai.com\/blog\/clip\/)\n- [CLIP Paper](https:\/\/arxiv.org\/abs\/2103.00020)\n\n\n### Use with Transformers\n```python3\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"openai\/clip-vit-base-patch16\")\nprocessor = CLIPProcessor.from_pretrained(\"openai\/clip-vit-base-patch16\")\nurl = \"http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n\n#### Primary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n\n### Out-of-Scope Use Cases\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n## Data\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http:\/\/projects.dfki.uni-kl.de\/yfcc100m\/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n\n### Data Mission Statement\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n## Performance and Limitations\n\n\n### Performance\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n\n## Limitations\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n\n### Bias and Fairness\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https:\/\/arxiv.org\/abs\/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement\/enthusiasm for such tasks.\n\n\n## Feedback\n\n\n### Where to send questions or comments about the model\nPlease use [this Google Form](https:\/\/forms.gle\/Uv7afRH5dvY34ZEs9)\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"1.201 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch16\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch16\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "2103.00020",
                    "1908.04913"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"researchers at OpenAI",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0020237712,
                "extraction_time":"2024-07-16_09-14-53"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2022-10-04 09:42:28+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch16\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"through a combination of crawling a handful of websites",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000010098,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"openai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"CLIPModel\nmodel = CLIPModel",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000012316,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch16",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"openai\/clip-vit-base-patch16",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/openai\/clip-vit-base-patch16",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"lists, tables and headers",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.322586298,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"distilbert-base-uncased",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3296915889,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"to\nbe fine-tuned on a downstream task",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.5201228857,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2225318402,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"distilbert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"distilbert-base-uncased",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3293887377,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":[
                    "bookcorpus",
                    "wikipedia"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/distilbert\/distilbert-base-uncased",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"1.530 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2296425551,
                "extraction_time":"2024-07-16_09-14-54"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https:\/\/huggingface.co\/bert-base-uncased). It was\nintroduced in [this paper](https:\/\/arxiv.org\/abs\/1910.01108). The code for the distillation process can be found\n[here](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https:\/\/huggingface.co\/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https:\/\/huggingface.co\/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https:\/\/yknzhu.wixsite.com\/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https:\/\/en.wikipedia.org\/wiki\/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs\/1910.01108}\n}\n```\n\n<a href=\"https:\/\/huggingface.co\/exbert\/?model=distilbert-base-uncased\">\n\t<img width=\"300px\" src=\"https:\/\/cdn-media.huggingface.co\/exbert\/button.png\">\n<\/a>\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "rust",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"1.530 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/distilbert\/distilbert-base-uncased\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/distilbert\/distilbert-base-uncased\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "1910.01108"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2334120721,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-05-06 13:44:53+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/distilbert\/distilbert-base-uncased\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2240086198,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"distilbert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2278973311,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/distilbert\/distilbert-base-uncased",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"distilbert\/distilbert-base-uncased",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/distilbert\/distilbert-base-uncased",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"sequence classification, token classification or question answering",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1568724513,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"xlm-roberta-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2343570739,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"to be fine-tuned on a downstream task",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3313596249,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "fill mask"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"xlm-roberta-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0138894534,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"FacebookAI",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"randomly masks 15% of the words in the input",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.001170533,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/FacebookAI\/xlm-roberta-large",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"11.243 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"xlm-roberta-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0065004406,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\ntags:\n- exbert\nlanguage:\n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- no\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\nlicense: mit\n---\n\n# XLM-RoBERTa (large-sized model) \n\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https:\/\/arxiv.org\/abs\/1911.02116) by Conneau et al. and first released in [this repository](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/xlmr). \n\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. \n\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https:\/\/huggingface.co\/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\n\n## Usage\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")\n\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\n# forward pass\noutput = model(**encoded_input)\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals\/corr\/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs\/1911.02116},\n  year      = {2019},\n  url       = {http:\/\/arxiv.org\/abs\/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https:\/\/dblp.org\/rec\/journals\/corr\/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https:\/\/dblp.org}\n}\n```\n\n<a href=\"https:\/\/huggingface.co\/exbert\/?model=xlm-roberta-base\">\n\t<img width=\"300px\" src=\"https:\/\/cdn-media.huggingface.co\/exbert\/button.png\">\n<\/a>\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "onnx",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"11.243 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/FacebookAI\/xlm-roberta-large\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/FacebookAI\/xlm-roberta-large\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "1911.02116"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Alexis Conneau and\n               Kartikay Khandelwal",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0601572394,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-02-19 12:48:30+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/FacebookAI\/xlm-roberta-large\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"The team releasing XLM-RoBERTa did not write a model card",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000012949,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"FacebookAI",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"xlm-roberta-base",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000832947,
                "extraction_time":"2024-07-16_09-14-55"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/FacebookAI\/xlm-roberta-large",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"FacebookAI\/xlm-roberta-large",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/FacebookAI\/xlm-roberta-large",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"runtime metrics",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0018841701,
                "extraction_time":"2024-07-16_09-15-00"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"create_model",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0001392409,
                "extraction_time":"2024-07-16_09-14-56"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"request import urlopen",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000948201,
                "extraction_time":"2024-07-16_09-15-00"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "image classification"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"Deep Residual Learning for Image Recognition",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0097572654,
                "extraction_time":"2024-07-16_09-15-01"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"timm",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"improved training procedure",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000340804,
                "extraction_time":"2024-07-16_09-15-01"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/timm\/resnet50.a1_in1k",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"0.205 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"request import urlopen\nfrom PIL import Image\nimport",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000194,
                "extraction_time":"2024-07-16_09-15-02"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: apache-2.0\nlibrary_name: timm\ntags:\n- image-classification\n- timm\n---\n# Model card for resnet50.a1_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A1` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification \/ feature backbone\n- **Model Stats:**\n  - Params (M): 25.6\n  - GMACs: 4.1\n  - Activations (M): 11.1\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https:\/\/arxiv.org\/abs\/2110.00476\n  - Deep Residual Learning for Image Recognition: https:\/\/arxiv.org\/abs\/1512.03385\n- **Original:** https:\/\/github.com\/huggingface\/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https:\/\/huggingface.co\/datasets\/huggingface\/documentation-images\/resolve\/main\/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https:\/\/huggingface.co\/datasets\/huggingface\/documentation-images\/resolve\/main\/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https:\/\/huggingface.co\/datasets\/huggingface\/documentation-images\/resolve\/main\/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https:\/\/github.com\/huggingface\/pytorch-image-models\/tree\/main\/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img\/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https:\/\/huggingface.co\/timm\/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https:\/\/huggingface.co\/timm\/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https:\/\/huggingface.co\/timm\/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https:\/\/huggingface.co\/timm\/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https:\/\/huggingface.co\/timm\/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https:\/\/huggingface.co\/timm\/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https:\/\/huggingface.co\/timm\/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https:\/\/huggingface.co\/timm\/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https:\/\/huggingface.co\/timm\/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https:\/\/huggingface.co\/timm\/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https:\/\/huggingface.co\/timm\/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https:\/\/huggingface.co\/timm\/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https:\/\/huggingface.co\/timm\/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https:\/\/huggingface.co\/timm\/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https:\/\/huggingface.co\/timm\/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https:\/\/huggingface.co\/timm\/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https:\/\/huggingface.co\/timm\/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https:\/\/huggingface.co\/timm\/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https:\/\/huggingface.co\/timm\/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https:\/\/huggingface.co\/timm\/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https:\/\/huggingface.co\/timm\/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https:\/\/huggingface.co\/timm\/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https:\/\/huggingface.co\/timm\/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https:\/\/huggingface.co\/timm\/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https:\/\/huggingface.co\/timm\/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https:\/\/huggingface.co\/timm\/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https:\/\/huggingface.co\/timm\/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https:\/\/huggingface.co\/timm\/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https:\/\/huggingface.co\/timm\/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https:\/\/huggingface.co\/timm\/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https:\/\/huggingface.co\/timm\/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https:\/\/huggingface.co\/timm\/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https:\/\/huggingface.co\/timm\/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https:\/\/huggingface.co\/timm\/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https:\/\/huggingface.co\/timm\/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https:\/\/huggingface.co\/timm\/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https:\/\/huggingface.co\/timm\/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https:\/\/huggingface.co\/timm\/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https:\/\/huggingface.co\/timm\/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https:\/\/huggingface.co\/timm\/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https:\/\/huggingface.co\/timm\/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https:\/\/huggingface.co\/timm\/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https:\/\/huggingface.co\/timm\/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https:\/\/huggingface.co\/timm\/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https:\/\/huggingface.co\/timm\/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https:\/\/huggingface.co\/timm\/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https:\/\/huggingface.co\/timm\/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https:\/\/huggingface.co\/timm\/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https:\/\/huggingface.co\/timm\/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https:\/\/huggingface.co\/timm\/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https:\/\/huggingface.co\/timm\/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https:\/\/huggingface.co\/timm\/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https:\/\/huggingface.co\/timm\/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https:\/\/huggingface.co\/timm\/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https:\/\/huggingface.co\/timm\/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https:\/\/huggingface.co\/timm\/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https:\/\/huggingface.co\/timm\/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https:\/\/huggingface.co\/timm\/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https:\/\/huggingface.co\/timm\/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https:\/\/huggingface.co\/timm\/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https:\/\/huggingface.co\/timm\/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https:\/\/huggingface.co\/timm\/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https:\/\/huggingface.co\/timm\/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https:\/\/huggingface.co\/timm\/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https:\/\/huggingface.co\/timm\/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https:\/\/huggingface.co\/timm\/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https:\/\/huggingface.co\/timm\/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https:\/\/huggingface.co\/timm\/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https:\/\/huggingface.co\/timm\/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https:\/\/huggingface.co\/timm\/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https:\/\/huggingface.co\/timm\/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https:\/\/huggingface.co\/timm\/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https:\/\/huggingface.co\/timm\/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https:\/\/huggingface.co\/timm\/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https:\/\/huggingface.co\/timm\/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https:\/\/huggingface.co\/timm\/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https:\/\/huggingface.co\/timm\/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https:\/\/huggingface.co\/timm\/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https:\/\/huggingface.co\/timm\/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https:\/\/huggingface.co\/timm\/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https:\/\/huggingface.co\/timm\/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https:\/\/huggingface.co\/timm\/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https:\/\/huggingface.co\/timm\/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https:\/\/huggingface.co\/timm\/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https:\/\/huggingface.co\/timm\/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https:\/\/huggingface.co\/timm\/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https:\/\/huggingface.co\/timm\/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https:\/\/huggingface.co\/timm\/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https:\/\/huggingface.co\/timm\/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https:\/\/huggingface.co\/timm\/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https:\/\/huggingface.co\/timm\/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https:\/\/huggingface.co\/timm\/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https:\/\/huggingface.co\/timm\/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https:\/\/huggingface.co\/timm\/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https:\/\/huggingface.co\/timm\/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https:\/\/huggingface.co\/timm\/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https:\/\/huggingface.co\/timm\/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https:\/\/huggingface.co\/timm\/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https:\/\/huggingface.co\/timm\/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https:\/\/huggingface.co\/timm\/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https:\/\/huggingface.co\/timm\/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https:\/\/huggingface.co\/timm\/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https:\/\/huggingface.co\/timm\/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https:\/\/huggingface.co\/timm\/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https:\/\/huggingface.co\/timm\/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https:\/\/huggingface.co\/timm\/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https:\/\/huggingface.co\/timm\/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https:\/\/huggingface.co\/timm\/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https:\/\/huggingface.co\/timm\/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https:\/\/huggingface.co\/timm\/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https:\/\/huggingface.co\/timm\/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https:\/\/huggingface.co\/timm\/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https:\/\/huggingface.co\/timm\/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https:\/\/huggingface.co\/timm\/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https:\/\/huggingface.co\/timm\/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https:\/\/huggingface.co\/timm\/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https:\/\/huggingface.co\/timm\/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https:\/\/huggingface.co\/timm\/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https:\/\/huggingface.co\/timm\/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https:\/\/huggingface.co\/timm\/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https:\/\/huggingface.co\/timm\/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https:\/\/huggingface.co\/timm\/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https:\/\/huggingface.co\/timm\/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https:\/\/huggingface.co\/timm\/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https:\/\/huggingface.co\/timm\/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https:\/\/huggingface.co\/timm\/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https:\/\/huggingface.co\/timm\/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https:\/\/huggingface.co\/timm\/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https:\/\/huggingface.co\/timm\/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https:\/\/huggingface.co\/timm\/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https:\/\/huggingface.co\/timm\/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https:\/\/huggingface.co\/timm\/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https:\/\/huggingface.co\/timm\/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https:\/\/huggingface.co\/timm\/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https:\/\/huggingface.co\/timm\/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https:\/\/huggingface.co\/timm\/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https:\/\/huggingface.co\/timm\/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https:\/\/huggingface.co\/timm\/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https:\/\/huggingface.co\/timm\/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https:\/\/huggingface.co\/timm\/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https:\/\/huggingface.co\/timm\/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https:\/\/huggingface.co\/timm\/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https:\/\/huggingface.co\/timm\/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https:\/\/huggingface.co\/timm\/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https:\/\/huggingface.co\/timm\/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https:\/\/huggingface.co\/timm\/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https:\/\/huggingface.co\/timm\/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https:\/\/huggingface.co\/timm\/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https:\/\/huggingface.co\/timm\/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https:\/\/huggingface.co\/timm\/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281\/zenodo.4414861},\n  howpublished = {\\url{https:\/\/github.com\/huggingface\/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "timm",
                    "pytorch",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"0.205 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/timm\/resnet50.a1_in1k\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/timm\/resnet50.a1_in1k\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "2110.00476",
                    "1512.03385"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.8591290712,
                "extraction_time":"2024-07-16_09-15-02"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2023-04-05 18:07:45+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-02-10 23:39:02+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2023-04-05 18:07:45+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/timm\/resnet50.a1_in1k\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"request import urlopen\nfrom PIL import Image\nimport",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000000013,
                "extraction_time":"2024-07-16_09-15-03"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"timm",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"PyTorch Image Models",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.0000135102,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/timm\/resnet50.a1_in1k",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"timm\/resnet50.a1_in1k",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/timm\/resnet50.a1_in1k",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"14 million images and 21k classes",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.3655447066,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1893543452,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Not extracted",
                "extraction_method":"None",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"image classification",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.4477760792,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "image feature extraction"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:modelCategory":"",
        "fair4ml:modelRisks":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1872897744,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"google",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "imagenet-21k"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "imagenet-21k"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            },
            {
                "data":null,
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1854605824,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":[
                    "imagenet-21k"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"https:\/\/huggingface.co\/google\/vit-base-patch16-224-in21k",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"1.383 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:operatingSystem":"",
        "schema.org:processorRequirements":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.1998307854,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: apache-2.0\ntags:\n- vision\ndatasets:\n- imagenet-21k\ninference: false\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https:\/\/arxiv.org\/abs\/2010.11929) by Dosovitskiy et al. and first released in [this repository](https:\/\/github.com\/google-research\/vision_transformer). However, the weights were converted from the [timm repository](https:\/\/github.com\/rwightman\/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. \n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nNote that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https:\/\/huggingface.co\/models?search=google\/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google\/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google\/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n```\n\nHere is how to use this model in JAX\/Flax:\n\n```python\nfrom transformers import ViTImageProcessor, FlaxViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google\/vit-base-patch16-224-in21k')\nmodel = FlaxViTModel.from_pretrained('google\/vit-base-patch16-224-in21k')\n\ninputs = processor(images=image, return_tensors=\"np\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n```\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http:\/\/www.image-net.org\/), a dataset consisting of 14 million images and 21k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training\/validation can be found [here](https:\/\/github.com\/google-research\/vision_transformer\/blob\/master\/vit_jax\/input_pipeline.py). \n\nImages are resized\/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:softwareHelp":"",
        "schema.org:softwareRequirements":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "jax",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            },
            {
                "data":"Python",
                "extraction_method":"Added in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"1.383 Gbytes",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "codemeta:buildInstructions":"",
        "codemeta:developmentStatus":"",
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/google\/vit-base-patch16-224-in21k\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/google\/vit-base-patch16-224-in21k\/blob\/main\/README.md",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "2010.11929",
                    "2006.03677"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:archivedAt":"",
        "schema.org:author":[
            {
                "data":"Bichen Wu and Chenfeng Xu",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2968289256,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "schema.org:citation":"",
        "schema.org:conditionsOfAccess":"",
        "schema.org:contributor":"",
        "schema.org:copyrightHolder":"",
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-02-05 16:37:39+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02 23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/google\/vit-base-patch16-224-in21k\/discussions",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:funding":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.214481473,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "schema.org:inLanguage":"",
        "schema.org:isAccessibleForFree":"",
        "schema.org:keywords":"",
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-37"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"google",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:version":[
            {
                "data":"[CLS]",
                "extraction_method":"Intel\/dynamic_tinybert",
                "confidence":0.2076940835,
                "extraction_time":"2024-07-16_09-15-04"
            }
        ],
        "schema.org:description":"",
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/google\/vit-base-patch16-224-in21k",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ],
        "schema.org:name":[
            {
                "data":"google\/vit-base-patch16-224-in21k",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-14-40"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/google\/vit-base-patch16-224-in21k",
                "extraction_method":"Built in transform stage",
                "confidence":1.0,
                "extraction_time":"2024-07-16_09-43-00"
            }
        ]
    }
]