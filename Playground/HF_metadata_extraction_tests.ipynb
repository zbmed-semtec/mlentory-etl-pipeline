{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models to extract particular data from txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Paris\n",
      "Confidence score: 0.9864407777786255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your question and context (text passage)\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"\"\"France is a country located in Western Europe. \n",
    "             The capital of France is Paris. It is a major global city \n",
    "             and a center for finance, diplomacy, and culture.\"\"\"\n",
    "\n",
    "# Load the QA pipeline with a pre-trained model\n",
    "qa_pipeline =  pipeline(\"question-answering\", model=\"Intel/dynamic_tinybert\")\n",
    "\n",
    "# Perform the QA task\n",
    "answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print the answer\n",
    "print(f\"Answer: {answer['answer']}\")\n",
    "print(f\"Confidence score: {answer['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7cd859c78a491b9bec61a04b780451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████| 184M/184M [00:05<00:00, 31.9MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████| 68.1M/68.1M [00:02<00:00, 24.2MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████| 68.3M/68.3M [00:02<00:00, 23.7MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709bf0f7acb94d439a0d6460ed0028b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/575348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_models = load_dataset(\"librarian-bots/model_cards_with_metadata\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "language: \n",
      "  - es\n",
      "library_name: pysentimiento\n",
      "\n",
      "tags:\n",
      "  - twitter\n",
      "  - sentiment-analysis\n",
      "\n",
      "---\n",
      "# Sentiment Analysis in Spanish\n",
      "## robertuito-sentiment-analysis\n",
      "\n",
      "Repository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n",
      "\n",
      "\n",
      "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets.\n",
      "\n",
      "Uses `POS`, `NEG`, `NEU` labels.\n",
      "\n",
      "## Usage\n",
      "\n",
      "Use it directly with [pysentimiento](https://github.com/pysentimiento/pysentimiento)\n",
      "\n",
      "```python\n",
      "from pysentimiento import create_analyzer\n",
      "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
      "\n",
      "analyzer.predict(\"Qué gran jugador es Messi\")\n",
      "# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n",
      "```\n",
      "\n",
      "\n",
      "## Results\n",
      "\n",
      "Results for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores\n",
      "\n",
      "\n",
      "| model         | emotion       | hate_speech   | irony         | sentiment     |\n",
      "|:--------------|:--------------|:--------------|:--------------|:--------------|\n",
      "| robertuito    | 0.560 ± 0.010 | 0.759 ± 0.007 | 0.739 ± 0.005 | 0.705 ± 0.003 |\n",
      "| roberta       | 0.527 ± 0.015 | 0.741 ± 0.012 | 0.721 ± 0.008 | 0.670 ± 0.006 |\n",
      "| bertin        | 0.524 ± 0.007 | 0.738 ± 0.007 | 0.713 ± 0.012 | 0.666 ± 0.005 |\n",
      "| beto_uncased  | 0.532 ± 0.012 | 0.727 ± 0.016 | 0.701 ± 0.007 | 0.651 ± 0.006 |\n",
      "| beto_cased    | 0.516 ± 0.012 | 0.724 ± 0.012 | 0.705 ± 0.009 | 0.662 ± 0.005 |\n",
      "| mbert_uncased | 0.493 ± 0.010 | 0.718 ± 0.011 | 0.681 ± 0.010 | 0.617 ± 0.003 |\n",
      "| biGRU         | 0.264 ± 0.007 | 0.592 ± 0.018 | 0.631 ± 0.011 | 0.585 ± 0.011 |\n",
      "\n",
      "\n",
      "Note that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B\n",
      "\n",
      "## Citation\n",
      "\n",
      "If you use this model in your research, please cite pysentimiento and RoBERTuito papers:\n",
      "\n",
      "```\n",
      "@misc{perez2021pysentimiento,\n",
      "      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n",
      "      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},\n",
      "      year={2021},\n",
      "      eprint={2106.09462},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.CL}\n",
      "}\n",
      "@inproceedings{perez-etal-2022-robertuito,\n",
      "    title = \"{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish\",\n",
      "    author = \"P{\\'e}rez, Juan Manuel  and\n",
      "      Furman, Dami{\\'a}n Ariel  and\n",
      "      Alonso Alemany, Laura  and\n",
      "      Luque, Franco M.\",\n",
      "    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n",
      "    month = jun,\n",
      "    year = \"2022\",\n",
      "    address = \"Marseille, France\",\n",
      "    publisher = \"European Language Resources Association\",\n",
      "    url = \"https://aclanthology.org/2022.lrec-1.785\",\n",
      "    pages = \"7235--7243\",\n",
      "    abstract = \"Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.\",\n",
      "}\n",
      "\n",
      "@inproceedings{garcia2020overview,\n",
      "  title={Overview of TASS 2020: Introducing emotion detection},\n",
      "  author={Garc{\\'\\i}a-Vega, Manuel and D{\\'\\i}az-Galiano, MC and Garc{\\'\\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\\'a}ez, A and Jim{\\'e}nez-Zafra, SM and Mart{\\'\\i}nez C{\\'a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},\n",
      "  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\\'a}laga, Spain},\n",
      "  pages={163--170},\n",
      "  year={2020}\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "context = dataset_models[\"card\"][0]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: TASS 2020 corpus\n",
      "Confidence score: 0.2816251814365387\n"
     ]
    }
   ],
   "source": [
    "question = 'What datasets was the model trained on?'\n",
    "\n",
    "# Perform the QA task using the pipeline\n",
    "predicted_answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print original answer and predicted answer with confidence score \n",
    "# print(f\"Original Answer: {answer['text']}\")\n",
    "print(f\"Predicted Answer: {predicted_answer['answer']}\")\n",
    "print(f\"Confidence score: {predicted_answer['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque\n",
      "Confidence score: 0.861379861831665\n"
     ]
    }
   ],
   "source": [
    "question = 'Who are the authors of the model?'\n",
    "\n",
    "# Perform the QA task using the pipeline\n",
    "predicted_answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print original answer and predicted answer with confidence score \n",
    "# print(f\"Original Answer: {answer['text']}\")\n",
    "print(f\"Predicted Answer: {predicted_answer['answer']}\")\n",
    "print(f\"Confidence score: {predicted_answer['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a processing pipeline\n",
    "\n",
    "Process the data from the HF metadata dataset.\n",
    "\n",
    "* Get the questions we want to answer from Huggingface metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the name of the model?']\n",
      "['What user uploaded the model?']\n",
      "['When was the model created/uploaded?']\n",
      "['What tasks can the model solve?']\n",
      "['What datasets was the model trained on?']\n",
      "['What was the split distribution for the datasets?']\n",
      "['What datasets were used to finetune the model?']\n",
      "['What datasets were used to retrain the model?']\n",
      "['What model is used as the base model?']\n",
      "['What evaluation metrics were used?']\n",
      "['What were the values of the evaluation metrics?']\n",
      "['What hyperparameters were optimized during the training process?']\n",
      "['What hyperparameters values were selected?']\n",
      "['What publications are related to the model?']\n",
      "['Where is the model deployed?']\n",
      "['What use license is associated with the model?']\n",
      "['What languages does the model work with?']\n",
      "['What software libraries were used for the model to work?']\n",
      "['Who created the model?']\n"
     ]
    }
   ],
   "source": [
    "questions = pd.read_csv(\"questions.tsv\",sep='\\t').values.tolist()\n",
    "for q in questions:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            modelId          author  \\\n",
      "0       pysentimiento/robertuito-sentiment-analysis   pysentimiento   \n",
      "1  cardiffnlp/twitter-roberta-base-sentiment-latest      cardiffnlp   \n",
      "2     jonatasgrosman/wav2vec2-large-xlsr-53-english  jonatasgrosman   \n",
      "3                     openai/clip-vit-large-patch14          openai   \n",
      "4                     google-bert/bert-base-uncased     google-bert   \n",
      "\n",
      "              last_modified  downloads  likes   library_name  \\\n",
      "0 2024-02-27 20:46:41+00:00  127797475     50  pysentimiento   \n",
      "1 2023-05-28 05:45:10+00:00  107275926    356   transformers   \n",
      "2 2023-03-25 10:56:55+00:00   54407155    394   transformers   \n",
      "3 2023-09-15 15:49:35+00:00   48829033   1050   transformers   \n",
      "4 2024-02-19 11:06:12+00:00   41541158   1454   transformers   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [pysentimiento, pytorch, tf, safetensors, robe...   \n",
      "1  [transformers, pytorch, tf, roberta, text-clas...   \n",
      "2  [transformers, pytorch, jax, safetensors, wav2...   \n",
      "3  [transformers, pytorch, tf, jax, safetensors, ...   \n",
      "4  [transformers, pytorch, tf, jax, rust, coreml,...   \n",
      "\n",
      "                     pipeline_tag                 createdAt  \\\n",
      "0                            None 2022-03-02 23:29:05+00:00   \n",
      "1             text-classification 2022-03-15 01:21:58+00:00   \n",
      "2    automatic-speech-recognition 2022-03-02 23:29:05+00:00   \n",
      "3  zero-shot-image-classification 2022-03-02 23:29:05+00:00   \n",
      "4                       fill-mask 2022-03-02 23:29:04+00:00   \n",
      "\n",
      "                                                card  \n",
      "0  ---\\nlanguage: \\n  - es\\nlibrary_name: pysenti...  \n",
      "1  ---\\nlanguage: en\\nwidget:\\n- text: Covid case...  \n",
      "2  ---\\nlanguage: en\\ndatasets:\\n- common_voice\\n...  \n",
      "3  ---\\ntags:\\n- vision\\nwidget:\\n- src: https://...  \n",
      "4  ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"librarian-bots/model_cards_with_metadata\")['train']\n",
    "HF_df = dataset.to_pandas()\n",
    "print(HF_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get tag information to extract data and answer some of the questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the unique number of tags\n",
    "# all_tags_set = set()\n",
    "# for index, row in HF_df.iterrows():\n",
    "#     tags = row['tags']\n",
    "#     for tag in tags:\n",
    "#         all_tags_set.add(tag)\n",
    "# print(len(all_tags_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tags we can get the following information:\n",
    "* The task they are performing\n",
    "* The libraries that were used to train the model\n",
    "* The datasets used to train the model\n",
    "* The languages that the model can handle\n",
    "* The Licenses under which the model can be use. \n",
    "* Other type of tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "tr\n",
      "yi\n",
      "sv\n",
      "fi\n"
     ]
    }
   ],
   "source": [
    "#Getting the language tags\n",
    "tags_language = [val[0].lower() for val in pd.read_csv(\"tags_language.tsv\",sep='\\t').values.tolist()]\n",
    "tags_language = set(tags_language)\n",
    "i = 0\n",
    "for tag in tags_language:\n",
    "    if i == 5:\n",
    "        break\n",
    "    i+=1\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core ml\n",
      "asteroid\n",
      "unity-sentis\n",
      "nemo\n",
      "spanmarker\n",
      "peft\n",
      "sample-factory\n",
      "gguf\n",
      "ml-agents\n",
      "timm\n",
      "espnet\n",
      "joblib\n",
      "flair\n",
      "pyannote.audio\n",
      "bertopic\n",
      "transformers.js\n",
      "setfit\n",
      "sentence-transformers\n",
      "fasttext\n",
      "openclip\n",
      "speechbrain\n",
      "diffusers\n",
      "transformers\n",
      "rust\n",
      "tensorboard\n",
      "tensorflow\n",
      "paddlepaddle\n",
      "openvino\n",
      "habana\n",
      "mlx\n",
      "scikit-learn\n",
      "adapters\n",
      "graphcore\n",
      "paddlenlp\n",
      "fastai\n",
      "tf lite\n",
      "stanza\n",
      "keras\n",
      "jax\n",
      "stable-baselines3\n",
      "pythae\n",
      "spacy\n",
      "onnx\n",
      "safetensors\n",
      "fairseq\n",
      "allennlp\n",
      "pytorch\n"
     ]
    }
   ],
   "source": [
    "#Getting the library tags\n",
    "tags_libraries = [val[0].lower() for val in pd.read_csv(\"tags_libraries.tsv\",sep='\\t').values.tolist()]\n",
    "tags_libraries = set(tags_libraries)\n",
    "for tag in tags_libraries:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbon emissions\n",
      "tags\n",
      "eval results\n",
      "mixture of experts\n",
      "8-bit precision\n",
      "inference endpoints\n",
      "autotrain compatible\n",
      "custom_code\n",
      "merge\n",
      "4-bit precision\n",
      "text-generation-inference\n",
      "has a space\n"
     ]
    }
   ],
   "source": [
    "#Getting the other tags\n",
    "tags_other = [val[0].lower() for val in pd.read_csv(\"tags_other.tsv\",sep='\\t').values.tolist()]\n",
    "tags_other = set(tags_other)\n",
    "for tag in tags_other:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text to audio\n",
      "visual question answering\n",
      "unconditional image generation\n",
      "zero shot object detection\n",
      "feature extraction\n",
      "mask generation\n",
      "depth estimation\n",
      "image to 3d\n",
      "text2text generation\n",
      "text classification\n",
      "voice activity detection\n",
      "video classification\n",
      "image feature extraction\n",
      "reinforcement learning\n",
      "zero shot classification\n",
      "image classification\n",
      "object detection\n",
      "robotics\n",
      "translation\n",
      "image to text\n",
      "sentence similarity\n",
      "text to speech\n",
      "table question answering\n",
      "image text to text\n",
      "audio to audio\n",
      "audio classification\n",
      "document question answering\n",
      "text to 3d\n",
      "text to video\n",
      "automatic speech recognition\n",
      "text generation\n",
      "image to video\n",
      "token classification\n",
      "text to image\n",
      "tabular regression\n",
      "fill mask\n",
      "image segmentation\n",
      "question answering\n",
      "tabular classification\n",
      "graph machine learning\n",
      "summarization\n",
      "zero shot image classification\n"
     ]
    }
   ],
   "source": [
    "#Getting the task tags\n",
    "tags_task = [val[0].lower().replace('-',\" \") for val in pd.read_csv(\"tags_task.tsv\",sep='\\t').values.tolist()]\n",
    "tags_task = set(tags_task)\n",
    "for tag in tags_task:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ineoApp/factures\n",
      "mattwiner/autotrain-data-baseball-jersey-detection\n",
      "MThonar/link\n",
      "misshimichka/flower_faces_dataset_v2\n",
      "andyflinn/heidiland\n"
     ]
    }
   ],
   "source": [
    "#Getting the dataset tags\n",
    "dataset_datasets = load_dataset(\"librarian-bots/dataset_cards_with_metadata\")['train']\n",
    "tags_dataset = set(dataset_datasets[\"datasetId\"])\n",
    "i = 5\n",
    "for tag in tags_dataset:\n",
    "    if(i==0):\n",
    "        break\n",
    "    print(tag)\n",
    "    i-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let's set up the questions we want the model to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform QA for a question-context pair\n",
    "def answer_question(question, context):\n",
    "  answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "  print(\"Question:\", question)\n",
    "  print(\"Answer:\", answer,\"/n\")\n",
    "  return answer['answer']+''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            modelId          author  \\\n",
      "0       pysentimiento/robertuito-sentiment-analysis   pysentimiento   \n",
      "1  cardiffnlp/twitter-roberta-base-sentiment-latest      cardiffnlp   \n",
      "2     jonatasgrosman/wav2vec2-large-xlsr-53-english  jonatasgrosman   \n",
      "3                     openai/clip-vit-large-patch14          openai   \n",
      "4                     google-bert/bert-base-uncased     google-bert   \n",
      "\n",
      "              last_modified  downloads  likes   library_name  \\\n",
      "0 2024-02-27 20:46:41+00:00  127797475     50  pysentimiento   \n",
      "1 2023-05-28 05:45:10+00:00  107275926    356   transformers   \n",
      "2 2023-03-25 10:56:55+00:00   54407155    394   transformers   \n",
      "3 2023-09-15 15:49:35+00:00   48829033   1050   transformers   \n",
      "4 2024-02-19 11:06:12+00:00   41541158   1454   transformers   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [pysentimiento, pytorch, tf, safetensors, robe...   \n",
      "1  [transformers, pytorch, tf, roberta, text-clas...   \n",
      "2  [transformers, pytorch, jax, safetensors, wav2...   \n",
      "3  [transformers, pytorch, tf, jax, safetensors, ...   \n",
      "4  [transformers, pytorch, tf, jax, rust, coreml,...   \n",
      "\n",
      "                     pipeline_tag                 createdAt  \\\n",
      "0                            None 2022-03-02 23:29:05+00:00   \n",
      "1             text-classification 2022-03-15 01:21:58+00:00   \n",
      "2    automatic-speech-recognition 2022-03-02 23:29:05+00:00   \n",
      "3  zero-shot-image-classification 2022-03-02 23:29:05+00:00   \n",
      "4                       fill-mask 2022-03-02 23:29:04+00:00   \n",
      "\n",
      "                                                card  ... q_id_9 q_id_10  \\\n",
      "0  ---\\nlanguage: \\n  - es\\nlibrary_name: pysenti...  ...   None    None   \n",
      "1  ---\\nlanguage: en\\nwidget:\\n- text: Covid case...  ...   None    None   \n",
      "2  ---\\nlanguage: en\\ndatasets:\\n- common_voice\\n...  ...   None    None   \n",
      "3  ---\\ntags:\\n- vision\\nwidget:\\n- src: https://...  ...   None    None   \n",
      "4  ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  ...   None    None   \n",
      "\n",
      "  q_id_11 q_id_12 q_id_13 q_id_14 q_id_15 q_id_16 q_id_17 q_id_18  \n",
      "0    None    None    None    None    None    None    None    None  \n",
      "1    None    None    None    None    None    None    None    None  \n",
      "2    None    None    None    None    None    None    None    None  \n",
      "3    None    None    None    None    None    None    None    None  \n",
      "4    None    None    None    None    None    None    None    None  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create new columns in the dataframe\n",
    "HF_df_small = HF_df.iloc[0:10] \n",
    "\n",
    "new_columns = {}\n",
    "\n",
    "for idx in range(len(questions)):\n",
    "    q_id = \"q_id_\"+str(idx)\n",
    "    new_columns[q_id] = [None for _ in range(len(HF_df_small))]\n",
    "\n",
    "HF_df_small = HF_df_small.assign(**new_columns)\n",
    "\n",
    "print(HF_df_small.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process basic default information\n",
    "HF_df_small.loc[:,\"q_id_0\"] = HF_df_small.loc[:, (\"modelId\")]\n",
    "HF_df_small.loc[:,\"q_id_1\"] = HF_df_small.loc[:, (\"author\")]\n",
    "HF_df_small.loc[:,\"q_id_2\"] = HF_df_small.loc[:, (\"createdAt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              q_id_0                 q_id_1  \\\n",
      "0        pysentimiento/robertuito-sentiment-analysis          pysentimiento   \n",
      "1   cardiffnlp/twitter-roberta-base-sentiment-latest             cardiffnlp   \n",
      "2      jonatasgrosman/wav2vec2-large-xlsr-53-english         jonatasgrosman   \n",
      "3                      openai/clip-vit-large-patch14                 openai   \n",
      "4                      google-bert/bert-base-uncased            google-bert   \n",
      "5   CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy              CAMeL-Lab   \n",
      "6                      tohoku-nlp/bert-base-japanese             tohoku-nlp   \n",
      "7             sentence-transformers/all-MiniLM-L6-v2  sentence-transformers   \n",
      "8  mrm8488/distilroberta-finetuned-financial-news...                mrm8488   \n",
      "9                 distilbert/distilbert-base-uncased             distilbert   \n",
      "\n",
      "                      q_id_2  \n",
      "0  2022-03-02 23:29:05+00:00  \n",
      "1  2022-03-15 01:21:58+00:00  \n",
      "2  2022-03-02 23:29:05+00:00  \n",
      "3  2022-03-02 23:29:05+00:00  \n",
      "4  2022-03-02 23:29:04+00:00  \n",
      "5  2022-03-02 23:29:04+00:00  \n",
      "6  2022-03-02 23:29:05+00:00  \n",
      "7  2022-03-02 23:29:05+00:00  \n",
      "8  2022-03-02 23:29:05+00:00  \n",
      "9  2022-03-02 23:29:04+00:00  \n"
     ]
    }
   ],
   "source": [
    "print(HF_df_small[['q_id_0', 'q_id_1', 'q_id_2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pysentimiento\n",
      "0 pytorch\n",
      "0 tf\n",
      "0 safetensors\n",
      "0 roberta\n",
      "0 twitter\n",
      "0 sentiment-analysis\n",
      "0 es\n",
      "0 arxiv:2106.09462\n",
      "0 has_space\n",
      "0 region:us\n",
      "1 transformers\n",
      "1 pytorch\n",
      "1 tf\n",
      "1 roberta\n",
      "1 text-classification\n",
      "1 en\n",
      "1 dataset:tweet_eval\n",
      "1 arxiv:2202.03829\n",
      "1 autotrain_compatible\n",
      "1 endpoints_compatible\n",
      "1 has_space\n",
      "1 region:us\n",
      "2 transformers\n",
      "2 pytorch\n",
      "2 jax\n",
      "2 safetensors\n",
      "2 wav2vec2\n",
      "2 automatic-speech-recognition\n",
      "2 audio\n",
      "2 en\n",
      "2 hf-asr-leaderboard\n",
      "2 mozilla-foundation/common_voice_6_0\n",
      "2 robust-speech-event\n",
      "2 speech\n",
      "2 xlsr-fine-tuning-week\n",
      "2 dataset:common_voice\n",
      "2 dataset:mozilla-foundation/common_voice_6_0\n",
      "2 license:apache-2.0\n",
      "2 model-index\n",
      "2 endpoints_compatible\n",
      "2 has_space\n",
      "2 region:us\n",
      "3 transformers\n",
      "3 pytorch\n",
      "3 tf\n",
      "3 jax\n",
      "3 safetensors\n",
      "3 clip\n",
      "3 zero-shot-image-classification\n",
      "3 vision\n",
      "3 arxiv:2103.00020\n",
      "3 arxiv:1908.04913\n",
      "3 endpoints_compatible\n",
      "3 has_space\n",
      "3 region:us\n",
      "4 transformers\n",
      "4 pytorch\n",
      "4 tf\n",
      "4 jax\n",
      "4 rust\n",
      "4 coreml\n",
      "4 onnx\n",
      "4 safetensors\n",
      "4 bert\n",
      "4 fill-mask\n",
      "4 exbert\n",
      "4 en\n",
      "4 dataset:bookcorpus\n",
      "4 dataset:wikipedia\n",
      "4 arxiv:1810.04805\n",
      "4 license:apache-2.0\n",
      "4 autotrain_compatible\n",
      "4 endpoints_compatible\n",
      "4 has_space\n",
      "4 region:us\n",
      "5 transformers\n",
      "5 pytorch\n",
      "5 tf\n",
      "5 bert\n",
      "5 token-classification\n",
      "5 ar\n",
      "5 arxiv:2103.06678\n",
      "5 license:apache-2.0\n",
      "5 autotrain_compatible\n",
      "5 endpoints_compatible\n",
      "5 has_space\n",
      "5 region:us\n",
      "6 transformers\n",
      "6 pytorch\n",
      "6 tf\n",
      "6 jax\n",
      "6 bert\n",
      "6 fill-mask\n",
      "6 ja\n",
      "6 dataset:wikipedia\n",
      "6 license:cc-by-sa-4.0\n",
      "6 autotrain_compatible\n",
      "6 endpoints_compatible\n",
      "6 has_space\n",
      "6 region:us\n",
      "7 sentence-transformers\n",
      "7 pytorch\n",
      "7 tf\n",
      "7 rust\n",
      "7 safetensors\n",
      "7 bert\n",
      "7 feature-extraction\n",
      "7 sentence-similarity\n",
      "7 transformers\n",
      "7 en\n",
      "7 dataset:s2orc\n",
      "7 dataset:flax-sentence-embeddings/stackexchange_xml\n",
      "7 dataset:ms_marco\n",
      "7 dataset:gooaq\n",
      "7 dataset:yahoo_answers_topics\n",
      "7 dataset:code_search_net\n",
      "7 dataset:search_qa\n",
      "7 dataset:eli5\n",
      "7 dataset:snli\n",
      "7 dataset:multi_nli\n",
      "7 dataset:wikihow\n",
      "7 dataset:natural_questions\n",
      "7 dataset:trivia_qa\n",
      "7 dataset:embedding-data/sentence-compression\n",
      "7 dataset:embedding-data/flickr30k-captions\n",
      "7 dataset:embedding-data/altlex\n",
      "7 dataset:embedding-data/simple-wiki\n",
      "7 dataset:embedding-data/QQP\n",
      "7 dataset:embedding-data/SPECTER\n",
      "7 dataset:embedding-data/PAQ_pairs\n",
      "7 dataset:embedding-data/WikiAnswers\n",
      "7 arxiv:1904.06472\n",
      "7 arxiv:2102.07033\n",
      "7 arxiv:2104.08727\n",
      "7 arxiv:1704.05179\n",
      "7 arxiv:1810.09305\n",
      "7 license:apache-2.0\n",
      "7 endpoints_compatible\n",
      "7 has_space\n",
      "7 region:us\n",
      "8 transformers\n",
      "8 pytorch\n",
      "8 tensorboard\n",
      "8 safetensors\n",
      "8 roberta\n",
      "8 text-classification\n",
      "8 generated_from_trainer\n",
      "8 financial\n",
      "8 stocks\n",
      "8 sentiment\n",
      "8 dataset:financial_phrasebank\n",
      "8 license:apache-2.0\n",
      "8 model-index\n",
      "8 autotrain_compatible\n",
      "8 endpoints_compatible\n",
      "8 has_space\n",
      "8 region:us\n",
      "9 transformers\n",
      "9 pytorch\n",
      "9 tf\n",
      "9 jax\n",
      "9 rust\n",
      "9 safetensors\n",
      "9 distilbert\n",
      "9 fill-mask\n",
      "9 exbert\n",
      "9 en\n",
      "9 dataset:bookcorpus\n",
      "9 dataset:wikipedia\n",
      "9 arxiv:1910.01108\n",
      "9 license:apache-2.0\n",
      "9 autotrain_compatible\n",
      "9 endpoints_compatible\n",
      "9 has_space\n",
      "9 region:us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87/4265278258.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_17\"][index] = []\n",
      "/tmp/ipykernel_87/4265278258.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_3\"][index] = []\n",
      "/tmp/ipykernel_87/4265278258.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_16\"][index] = []\n",
      "/tmp/ipykernel_87/4265278258.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_4\"][index] = []\n",
      "/tmp/ipykernel_87/4265278258.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_13\"][index] = []\n",
      "/tmp/ipykernel_87/4265278258.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_15\"][index] = []\n"
     ]
    }
   ],
   "source": [
    "# Process tags\n",
    "\n",
    "for index, row in HF_df_small.iterrows():\n",
    "    for tag in row['tags']:\n",
    "        print(index,tag)\n",
    "        #Check question 3\n",
    "        tag_for_q3 = tag.replace('-',\" \")\n",
    "        if tag_for_q3 in tags_task:\n",
    "            if(HF_df_small[\"q_id_3\"][index] == None):\n",
    "                HF_df_small[\"q_id_3\"][index] = []\n",
    "            HF_df_small[\"q_id_3\"][index].append(tag_for_q3)\n",
    "        #Check question 4\n",
    "        if \"dataset:\" in tag:\n",
    "            if(HF_df_small[\"q_id_4\"][index] == None):\n",
    "                HF_df_small[\"q_id_4\"][index] = []\n",
    "            HF_df_small[\"q_id_4\"][index].append(tag.replace(\"dataset:\",\"\"))\n",
    "        #Check question 13\n",
    "        if \"arxiv:\" in tag:\n",
    "            if(HF_df_small[\"q_id_13\"][index] == None):\n",
    "                HF_df_small[\"q_id_13\"][index] = []\n",
    "            HF_df_small[\"q_id_13\"][index].append(tag.replace(\"arxiv:\",\"\"))\n",
    "         #Check question 15\n",
    "        if \"license:\" in tag:\n",
    "            if(HF_df_small[\"q_id_15\"][index] == None):\n",
    "                HF_df_small[\"q_id_15\"][index] = []\n",
    "            HF_df_small[\"q_id_15\"][index].append(tag.replace(\"license:\",\"\"))\n",
    "        #Check question 16\n",
    "        if tag in tags_language:\n",
    "            if(HF_df_small[\"q_id_16\"][index] == None):\n",
    "                HF_df_small[\"q_id_16\"][index] = []\n",
    "            HF_df_small[\"q_id_16\"][index].append(tag)\n",
    "        #Check question 17\n",
    "        if tag in tags_libraries:\n",
    "            if(HF_df_small[\"q_id_17\"][index] == None):\n",
    "                HF_df_small[\"q_id_17\"][index] = []\n",
    "            HF_df_small[\"q_id_17\"][index].append(tag)\n",
    "    #Check question 3 again\n",
    "    if(row['pipeline_tag'] != None):\n",
    "        tag_for_q3 = row['pipeline_tag'].replace(\"-\",\" \")\n",
    "        if(HF_df_small[\"q_id_3\"][index] == None):\n",
    "            HF_df_small[\"q_id_3\"][index] = []\n",
    "        if(tag_for_q3 not in HF_df_small[\"q_id_3\"][index]):\n",
    "            HF_df_small[\"q_id_3\"][index].append(tag_for_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      q_id_3  \\\n",
      "0                                       None   \n",
      "1                      [text classification]   \n",
      "2             [automatic speech recognition]   \n",
      "3           [zero shot image classification]   \n",
      "4                                [fill mask]   \n",
      "5                     [token classification]   \n",
      "6                                [fill mask]   \n",
      "7  [feature extraction, sentence similarity]   \n",
      "8                      [text classification]   \n",
      "9                                [fill mask]   \n",
      "\n",
      "                                              q_id_4  \\\n",
      "0                                               None   \n",
      "1                                       [tweet_eval]   \n",
      "2  [common_voice, mozilla-foundation/common_voice...   \n",
      "3                                               None   \n",
      "4                            [bookcorpus, wikipedia]   \n",
      "5                                               None   \n",
      "6                                        [wikipedia]   \n",
      "7  [s2orc, flax-sentence-embeddings/stackexchange...   \n",
      "8                             [financial_phrasebank]   \n",
      "9                            [bookcorpus, wikipedia]   \n",
      "\n",
      "                                             q_id_13         q_id_15  \\\n",
      "0                           [2106.09462, 2106.09462]            None   \n",
      "1                                       [2202.03829]            None   \n",
      "2                                               None    [apache-2.0]   \n",
      "3                           [2103.00020, 1908.04913]            None   \n",
      "4                                       [1810.04805]    [apache-2.0]   \n",
      "5                                       [2103.06678]    [apache-2.0]   \n",
      "6                                               None  [cc-by-sa-4.0]   \n",
      "7  [1904.06472, 2102.07033, 2104.08727, 1704.0517...    [apache-2.0]   \n",
      "8                                               None    [apache-2.0]   \n",
      "9                                       [1910.01108]    [apache-2.0]   \n",
      "\n",
      "    q_id_16                                            q_id_17 q_id_18  \n",
      "0  [es, es]       [pytorch, safetensors, pytorch, safetensors]    None  \n",
      "1      [en]                            [transformers, pytorch]    None  \n",
      "2      [en]          [transformers, pytorch, jax, safetensors]    None  \n",
      "3      None          [transformers, pytorch, jax, safetensors]    None  \n",
      "4      [en]  [transformers, pytorch, jax, rust, onnx, safet...    None  \n",
      "5      [ar]                            [transformers, pytorch]    None  \n",
      "6      [ja]                       [transformers, pytorch, jax]    None  \n",
      "7      [en]  [sentence-transformers, pytorch, rust, safeten...    None  \n",
      "8      None  [transformers, pytorch, tensorboard, safetensors]    None  \n",
      "9      [en]    [transformers, pytorch, jax, rust, safetensors]    None  \n"
     ]
    }
   ],
   "source": [
    "print(HF_df_small[['q_id_3', 'q_id_4','q_id_13','q_id_15','q_id_16','q_id_17']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 1.1775950881087738e-08, 'start': 1277, 'end': 1292, 'answer': '| 0.670 ± 0.006'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.4569787085056305, 'start': 440, 'end': 447, 'answer': 'RoBERTa'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.003498514648526907, 'start': 949, 'end': 964, 'answer': 'Macro F1 scores'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 8.506896847393364e-05, 'start': 924, 'end': 964, 'answer': 'Results are expressed as Macro F1 scores'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 2.9859681944799377e-06, 'start': 3407, 'end': 3425, 'answer': '500 million tweets'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.0011761118657886982, 'start': 465, 'end': 479, 'answer': 'Spanish tweets'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.000603789696469903, 'start': 627, 'end': 647, 'answer': 'pysentimiento import'} /n\n",
      "Answers added for row 0\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.0057952990755438805, 'start': 1601, 'end': 1612, 'answer': \"t = '@user'\"} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.0323258601129055, 'start': 169, 'end': 181, 'answer': 'RoBERTa-base'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 1.642545248614624e-05, 'start': 4029, 'end': 4050, 'answer': 'System Demonstrations'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.031056074425578117, 'start': 4293, 'end': 4301, 'answer': '251--260'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 4.4644980334851425e-06, 'start': 200, 'end': 204, 'answer': '124M'} /n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87/2624007833.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[question][index] = answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.0015904817264527082, 'start': 4104, 'end': 4119, 'answer': 'Dublin, Ireland'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.0062449295073747635, 'start': 3819, 'end': 3880, 'answer': 'Francesco  and\\n      Neves, Leonardo  and\\n      Espinosa Anke'} /n\n",
      "Answers added for row 1\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.2762574851512909, 'start': 1466, 'end': 1482, 'answer': 'Common Voice 6.1'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.0021992120891809464, 'start': 1257, 'end': 1264, 'answer': 'XLSR-53'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 8.480218821205199e-05, 'start': 991, 'end': 1005, 'answer': 'en\\n    metrics'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.02585451304912567, 'start': 4477, 'end': 4493, 'answer': '## Evaluation\\n\\n1'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 3.49427665469193e-07, 'start': 1601, 'end': 1606, 'answer': '16kHz'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 1.9328506823512726e-05, 'start': 1891, 'end': 1899, 'answer': 'directly'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.6643454432487488, 'start': 4819, 'end': 4833, 'answer': 'jonatasgrosman'} /n\n",
      "Answers added for row 2\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.11769308149814606, 'start': 7478, 'end': 7517, 'answer': '~93% for racial classification and ~63%'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.3469974100589752, 'start': 947, 'end': 980, 'answer': 'ViT-L/14 Transformer architecture'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.1654023975133896, 'start': 6249, 'end': 6262, 'answer': 'linear probes'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.06842001527547836, 'start': 7652, 'end': 7730, 'answer': 'to evaluate performance of the model across people and surface potential risks'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 4.6569503808768786e-08, 'start': 647, 'end': 656, 'answer': 'zero-shot'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.028397947549819946, 'start': 870, 'end': 876, 'answer': 'within'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.1021275520324707, 'start': 435, 'end': 456, 'answer': 'researchers at OpenAI'} /n\n",
      "Answers added for row 3\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.230930894613266, 'start': 6117, 'end': 6122, 'answer': '[CLS]'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.22914648056030273, 'start': 6117, 'end': 6122, 'answer': '[CLS]'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.15860265493392944, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.21457292139530182, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.23381945490837097, 'start': 6117, 'end': 6122, 'answer': '[CLS]'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.18620802462100983, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.10725255310535431, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Answers added for row 4\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.2043287605047226, 'start': 2717, 'end': 2719, 'answer': '12'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.0018236819887533784, 'start': 693, 'end': 714, 'answer': 'CAMeLBERT-Mix POS-EGY'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.02407822571694851, 'start': 2717, 'end': 2728, 'answer': '12 datasets'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 1.7243621186935343e-05, 'start': 2821, 'end': 2867, 'answer': 'more important than the pre-training data size'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.0020815732423216105, 'start': 472, 'end': 500, 'answer': 'Variant, Size, and Task Type'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.0009599383338354528, 'start': 2541, 'end': 2575, 'answer': 'scaled-down set of the MSA variant'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.02278573252260685, 'start': 2275, 'end': 2343, 'answer': 'Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic'} /n\n",
      "Answers added for row 5\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.039773814380168915, 'start': 1219, 'end': 1238, 'answer': 'WordPiece algorithm'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.46355438232421875, 'start': 612, 'end': 627, 'answer': 'BERT base model'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 9.21803803066723e-06, 'start': 1409, 'end': 1426, 'answer': '1M training steps'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 1.6510583691342617e-06, 'start': 629, 'end': 695, 'answer': '12 layers, 768 dimensions of hidden states, and 12 attention heads'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.000491648621391505, 'start': 1409, 'end': 1411, 'answer': '1M'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.023578545078635216, 'start': 740, 'end': 758, 'answer': 'Japanese Wikipedia'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.0011689970269799232, 'start': 612, 'end': 627, 'answer': 'BERT base model'} /n\n",
      "Answers added for row 6\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 1.412919027643511e-05, 'start': 5365, 'end': 5375, 'answer': '128 tokens'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.001539447926916182, 'start': 860, 'end': 881, 'answer': 'Sentence-Transformers'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.004938929341733456, 'start': 818, 'end': 847, 'answer': 'clustering or semantic search'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 9.756104191183113e-06, 'start': 2959, 'end': 2993, 'answer': 'automated evaluation of this model'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.0011639486765488982, 'start': 5197, 'end': 5205, 'answer': 'TPU v3-8'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.00019294161756988615, 'start': 752, 'end': 786, 'answer': '384 dimensional dense vector space'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.3940327763557434, 'start': 3873, 'end': 3885, 'answer': 'Hugging Face'} /n\n",
      "Answers added for row 7\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.604581356048584, 'start': 2213, 'end': 2245, 'answer': 'agreement rate of 5-8 annotators'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.07886279374361038, 'start': 1441, 'end': 1459, 'answer': 'RoBERTa-base model'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.09674693644046783, 'start': 2315, 'end': 2330, 'answer': 'hyperparameters'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.1959497183561325, 'start': 3076, 'end': 3102, 'answer': '1.12.1\\n- Tokenizers 0.10.3'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.07028830796480179, 'start': 2301, 'end': 2330, 'answer': 'The following hyperparameters'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 1.0802518772834446e-05, 'start': 3036, 'end': 3052, 'answer': '4.10.2\\n- Pytorch'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.05295714735984802, 'start': 3036, 'end': 3052, 'answer': '4.10.2\\n- Pytorch'} /n\n",
      "Answers added for row 8\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.23583245277404785, 'start': 6763, 'end': 6768, 'answer': '[CLS]'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.3296915888786316, 'start': 8468, 'end': 8491, 'answer': 'distilbert-base-uncased'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.23366042971611023, 'start': 4759, 'end': 4764, 'answer': '[CLS]'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.22357484698295593, 'start': 4759, 'end': 4764, 'answer': '[CLS]'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.21749404072761536, 'start': 6763, 'end': 6768, 'answer': '[CLS]'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.2228890061378479, 'start': 6763, 'end': 6768, 'answer': '[CLS]'} /n\n",
      "Question: Who created the model?\n",
      "Answer: {'score': 0.217814102768898, 'start': 4759, 'end': 4764, 'answer': '[CLS]'} /n\n",
      "Answers added for row 9\n"
     ]
    }
   ],
   "source": [
    "# Process card\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"Intel/dynamic_tinybert\",device=0)\n",
    "\n",
    "questions_to_process = {5,8,9,10,11,14,18}\n",
    "\n",
    "# Process each row of the DataFrame\n",
    "for index, row in HF_df_small.iterrows():\n",
    "  context = row[\"card\"]  # Assuming \"card\" column contains the context\n",
    "\n",
    "  # Create an empty dictionary to store answers for each question\n",
    "  answers = {}\n",
    "  q_cnt = -1\n",
    "  for question in questions:\n",
    "    q_cnt+=1\n",
    "    if(q_cnt not in questions_to_process): continue\n",
    "    q = question[0]\n",
    "    answer = answer_question(q, context)\n",
    "    q_id = \"q_id_\"+str(q_cnt)\n",
    "    answers[q_id] = answer  # Store answer for each question\n",
    "  \n",
    "  # Add a new column for each question and populate with answers\n",
    "  for question, answer in answers.items():\n",
    "    HF_df_small[question][index] = answer\n",
    "    \n",
    "\n",
    "  print(\"Answers added for row\", index)  # Optional progress indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    q_id_5                             q_id_8  \\\n",
      "0                          | 0.670 ± 0.006                            RoBERTa   \n",
      "1                              t = '@user'                       RoBERTa-base   \n",
      "2                         Common Voice 6.1                            XLSR-53   \n",
      "3  ~93% for racial classification and ~63%  ViT-L/14 Transformer architecture   \n",
      "4                                    [CLS]                              [CLS]   \n",
      "5                                       12              CAMeLBERT-Mix POS-EGY   \n",
      "6                      WordPiece algorithm                    BERT base model   \n",
      "7                               128 tokens              Sentence-Transformers   \n",
      "8         agreement rate of 5-8 annotators                 RoBERTa-base model   \n",
      "9                                    [CLS]            distilbert-base-uncased   \n",
      "\n",
      "                          q_id_9  \\\n",
      "0                Macro F1 scores   \n",
      "1          System Demonstrations   \n",
      "2                en\\n    metrics   \n",
      "3                  linear probes   \n",
      "4                          [CLS]   \n",
      "5                    12 datasets   \n",
      "6              1M training steps   \n",
      "7  clustering or semantic search   \n",
      "8                hyperparameters   \n",
      "9                          [CLS]   \n",
      "\n",
      "                                             q_id_10  \\\n",
      "0           Results are expressed as Macro F1 scores   \n",
      "1                                           251--260   \n",
      "2                                 ## Evaluation\\n\\n1   \n",
      "3  to evaluate performance of the model across pe...   \n",
      "4                                              [CLS]   \n",
      "5     more important than the pre-training data size   \n",
      "6  12 layers, 768 dimensions of hidden states, an...   \n",
      "7                 automated evaluation of this model   \n",
      "8                        1.12.1\\n- Tokenizers 0.10.3   \n",
      "9                                              [CLS]   \n",
      "\n",
      "                         q_id_11                             q_id_14  \\\n",
      "0             500 million tweets                      Spanish tweets   \n",
      "1                           124M                     Dublin, Ireland   \n",
      "2                          16kHz                            directly   \n",
      "3                      zero-shot                              within   \n",
      "4                          [CLS]                               [CLS]   \n",
      "5   Variant, Size, and Task Type  scaled-down set of the MSA variant   \n",
      "6                             1M                  Japanese Wikipedia   \n",
      "7                       TPU v3-8  384 dimensional dense vector space   \n",
      "8  The following hyperparameters                   4.10.2\\n- Pytorch   \n",
      "9                          [CLS]                               [CLS]   \n",
      "\n",
      "                                             q_id_18  \n",
      "0                               pysentimiento import  \n",
      "1  Francesco  and\\n      Neves, Leonardo  and\\n  ...  \n",
      "2                                     jonatasgrosman  \n",
      "3                              researchers at OpenAI  \n",
      "4                                              [CLS]  \n",
      "5  Modern Standard Arabic (MSA), dialectal Arabic...  \n",
      "6                                    BERT base model  \n",
      "7                                       Hugging Face  \n",
      "8                                  4.10.2\\n- Pytorch  \n",
      "9                                              [CLS]  \n"
     ]
    }
   ],
   "source": [
    "print(HF_df_small[['q_id_5', 'q_id_8','q_id_9','q_id_10','q_id_11','q_id_14','q_id_18']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['modelId', 'author', 'last_modified', 'downloads', 'likes',\n",
      "       'library_name', 'tags', 'pipeline_tag', 'createdAt', 'card', 'q_id_0',\n",
      "       'q_id_1', 'q_id_2', 'q_id_3', 'q_id_4', 'q_id_5', 'q_id_6', 'q_id_7',\n",
      "       'q_id_8', 'q_id_9', 'q_id_10', 'q_id_11', 'q_id_12', 'q_id_13',\n",
      "       'q_id_14', 'q_id_15', 'q_id_16', 'q_id_17', 'q_id_18'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(HF_df_small.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             q_id_0          q_id_1  \\\n",
      "0       pysentimiento/robertuito-sentiment-analysis   pysentimiento   \n",
      "1  cardiffnlp/twitter-roberta-base-sentiment-latest      cardiffnlp   \n",
      "2     jonatasgrosman/wav2vec2-large-xlsr-53-english  jonatasgrosman   \n",
      "3                     openai/clip-vit-large-patch14          openai   \n",
      "4                     google-bert/bert-base-uncased     google-bert   \n",
      "\n",
      "                      q_id_2                            q_id_3  \\\n",
      "0  2022-03-02 23:29:05+00:00                              None   \n",
      "1  2022-03-15 01:21:58+00:00             [text classification]   \n",
      "2  2022-03-02 23:29:05+00:00    [automatic speech recognition]   \n",
      "3  2022-03-02 23:29:05+00:00  [zero shot image classification]   \n",
      "4  2022-03-02 23:29:04+00:00                       [fill mask]   \n",
      "\n",
      "                                              q_id_4  \\\n",
      "0                                               None   \n",
      "1                                       [tweet_eval]   \n",
      "2  [common_voice, mozilla-foundation/common_voice...   \n",
      "3                                               None   \n",
      "4                            [bookcorpus, wikipedia]   \n",
      "\n",
      "                                    q_id_5 q_id_6 q_id_7  \\\n",
      "0                          | 0.670 ± 0.006   None   None   \n",
      "1                              t = '@user'   None   None   \n",
      "2                         Common Voice 6.1   None   None   \n",
      "3  ~93% for racial classification and ~63%   None   None   \n",
      "4                                    [CLS]   None   None   \n",
      "\n",
      "                              q_id_8                 q_id_9  \\\n",
      "0                            RoBERTa        Macro F1 scores   \n",
      "1                       RoBERTa-base  System Demonstrations   \n",
      "2                            XLSR-53        en\\n    metrics   \n",
      "3  ViT-L/14 Transformer architecture          linear probes   \n",
      "4                              [CLS]                  [CLS]   \n",
      "\n",
      "                                             q_id_10             q_id_11  \\\n",
      "0           Results are expressed as Macro F1 scores  500 million tweets   \n",
      "1                                           251--260                124M   \n",
      "2                                 ## Evaluation\\n\\n1               16kHz   \n",
      "3  to evaluate performance of the model across pe...           zero-shot   \n",
      "4                                              [CLS]               [CLS]   \n",
      "\n",
      "  q_id_12                   q_id_13          q_id_14       q_id_15   q_id_16  \\\n",
      "0    None  [2106.09462, 2106.09462]   Spanish tweets          None  [es, es]   \n",
      "1    None              [2202.03829]  Dublin, Ireland          None      [en]   \n",
      "2    None                      None         directly  [apache-2.0]      [en]   \n",
      "3    None  [2103.00020, 1908.04913]           within          None      None   \n",
      "4    None              [1810.04805]            [CLS]  [apache-2.0]      [en]   \n",
      "\n",
      "                                             q_id_17  \\\n",
      "0       [pytorch, safetensors, pytorch, safetensors]   \n",
      "1                            [transformers, pytorch]   \n",
      "2          [transformers, pytorch, jax, safetensors]   \n",
      "3          [transformers, pytorch, jax, safetensors]   \n",
      "4  [transformers, pytorch, jax, rust, onnx, safet...   \n",
      "\n",
      "                                             q_id_18  \n",
      "0                               pysentimiento import  \n",
      "1  Francesco  and\\n      Neves, Leonardo  and\\n  ...  \n",
      "2                                     jonatasgrosman  \n",
      "3                              researchers at OpenAI  \n",
      "4                                              [CLS]  \n"
     ]
    }
   ],
   "source": [
    "#Removing unused columns\n",
    "\n",
    "HF_df_small_clean = HF_df_small.drop(columns=['modelId', 'author', 'last_modified', 'downloads', 'likes',\n",
    "       'library_name', 'tags', 'pipeline_tag', 'createdAt', 'card'])\n",
    "\n",
    "print(HF_df_small_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
