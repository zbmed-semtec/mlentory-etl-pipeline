{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to extract particular data from txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Paris\n",
      "Confidence score: 0.9864407777786255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your question and context (text passage)\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"\"\"France is a country located in Western Europe. \n",
    "             The capital of France is Paris. It is a major global city \n",
    "             and a center for finance, diplomacy, and culture.\"\"\"\n",
    "\n",
    "# Load the QA pipeline with a pre-trained model\n",
    "qa_pipeline =  pipeline(\"question-answering\", model=\"Intel/dynamic_tinybert\")\n",
    "\n",
    "# Perform the QA task\n",
    "answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print the answer\n",
    "print(f\"Answer: {answer['answer']}\")\n",
    "print(f\"Confidence score: {answer['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_models = load_dataset(\"librarian-bots/model_cards_with_metadata\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "language: \n",
      "  - es\n",
      "library_name: pysentimiento\n",
      "\n",
      "tags:\n",
      "  - twitter\n",
      "  - sentiment-analysis\n",
      "\n",
      "---\n",
      "# Sentiment Analysis in Spanish\n",
      "## robertuito-sentiment-analysis\n",
      "\n",
      "Repository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n",
      "\n",
      "\n",
      "Model trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets.\n",
      "\n",
      "Uses `POS`, `NEG`, `NEU` labels.\n",
      "\n",
      "## Usage\n",
      "\n",
      "Use it directly with [pysentimiento](https://github.com/pysentimiento/pysentimiento)\n",
      "\n",
      "```python\n",
      "from pysentimiento import create_analyzer\n",
      "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
      "\n",
      "analyzer.predict(\"Qué gran jugador es Messi\")\n",
      "# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n",
      "```\n",
      "\n",
      "\n",
      "## Results\n",
      "\n",
      "Results for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores\n",
      "\n",
      "\n",
      "| model         | emotion       | hate_speech   | irony         | sentiment     |\n",
      "|:--------------|:--------------|:--------------|:--------------|:--------------|\n",
      "| robertuito    | 0.560 ± 0.010 | 0.759 ± 0.007 | 0.739 ± 0.005 | 0.705 ± 0.003 |\n",
      "| roberta       | 0.527 ± 0.015 | 0.741 ± 0.012 | 0.721 ± 0.008 | 0.670 ± 0.006 |\n",
      "| bertin        | 0.524 ± 0.007 | 0.738 ± 0.007 | 0.713 ± 0.012 | 0.666 ± 0.005 |\n",
      "| beto_uncased  | 0.532 ± 0.012 | 0.727 ± 0.016 | 0.701 ± 0.007 | 0.651 ± 0.006 |\n",
      "| beto_cased    | 0.516 ± 0.012 | 0.724 ± 0.012 | 0.705 ± 0.009 | 0.662 ± 0.005 |\n",
      "| mbert_uncased | 0.493 ± 0.010 | 0.718 ± 0.011 | 0.681 ± 0.010 | 0.617 ± 0.003 |\n",
      "| biGRU         | 0.264 ± 0.007 | 0.592 ± 0.018 | 0.631 ± 0.011 | 0.585 ± 0.011 |\n",
      "\n",
      "\n",
      "Note that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B\n",
      "\n",
      "## Citation\n",
      "\n",
      "If you use this model in your research, please cite pysentimiento and RoBERTuito papers:\n",
      "\n",
      "```\n",
      "@misc{perez2021pysentimiento,\n",
      "      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n",
      "      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},\n",
      "      year={2021},\n",
      "      eprint={2106.09462},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.CL}\n",
      "}\n",
      "@inproceedings{perez-etal-2022-robertuito,\n",
      "    title = \"{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish\",\n",
      "    author = \"P{\\'e}rez, Juan Manuel  and\n",
      "      Furman, Dami{\\'a}n Ariel  and\n",
      "      Alonso Alemany, Laura  and\n",
      "      Luque, Franco M.\",\n",
      "    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n",
      "    month = jun,\n",
      "    year = \"2022\",\n",
      "    address = \"Marseille, France\",\n",
      "    publisher = \"European Language Resources Association\",\n",
      "    url = \"https://aclanthology.org/2022.lrec-1.785\",\n",
      "    pages = \"7235--7243\",\n",
      "    abstract = \"Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.\",\n",
      "}\n",
      "\n",
      "@inproceedings{garcia2020overview,\n",
      "  title={Overview of TASS 2020: Introducing emotion detection},\n",
      "  author={Garc{\\'\\i}a-Vega, Manuel and D{\\'\\i}az-Galiano, MC and Garc{\\'\\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\\'a}ez, A and Jim{\\'e}nez-Zafra, SM and Mart{\\'\\i}nez C{\\'a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},\n",
      "  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\\'a}laga, Spain},\n",
      "  pages={163--170},\n",
      "  year={2020}\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "context = dataset_models[\"card\"][0]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: TASS 2020 corpus\n",
      "Confidence score: 0.2816251814365387\n"
     ]
    }
   ],
   "source": [
    "question = 'What datasets was the model trained on?'\n",
    "\n",
    "# Perform the QA task using the pipeline\n",
    "predicted_answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print original answer and predicted answer with confidence score \n",
    "# print(f\"Original Answer: {answer['text']}\")\n",
    "print(f\"Predicted Answer: {predicted_answer['answer']}\")\n",
    "print(f\"Confidence score: {predicted_answer['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque\n",
      "Confidence score: 0.861379861831665\n"
     ]
    }
   ],
   "source": [
    "question = 'Who are the authors of the model?'\n",
    "\n",
    "# Perform the QA task using the pipeline\n",
    "predicted_answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print original answer and predicted answer with confidence score \n",
    "# print(f\"Original Answer: {answer['text']}\")\n",
    "print(f\"Predicted Answer: {predicted_answer['answer']}\")\n",
    "print(f\"Confidence score: {predicted_answer['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a processing pipeline\n",
    "\n",
    "Process the data from the HF metadata dataset.\n",
    "\n",
    "* Get the questions we want to answer from Huggingface metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the name of the model?']\n",
      "['Who created/uploaded the model?']\n",
      "['When was the model created/uploaded?']\n",
      "['What tasks can the model solve?']\n",
      "['What datasets was the model trained on?']\n",
      "['What was the split distribution for the datasets?']\n",
      "['What datasets were used to finetune the model?']\n",
      "['What datasets were used to retrain the model?']\n",
      "['What model is used as the base model?']\n",
      "['What evaluation metrics were used?']\n",
      "['What were the values of the evaluation metrics?']\n",
      "['What hyperparameters were optimized during the training process?']\n",
      "['What hyperparameters values were selected?']\n",
      "['What publications are related to the model?']\n",
      "['Where is the model deployed?']\n",
      "['What use license is associated with the model?']\n",
      "['What languages does the model work with?']\n",
      "['What software libraries were used for the model to work?']\n"
     ]
    }
   ],
   "source": [
    "questions = pd.read_csv(\"questions.tsv\",sep='\\t').values.tolist()\n",
    "for q in questions:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            modelId          author  \\\n",
      "0       pysentimiento/robertuito-sentiment-analysis   pysentimiento   \n",
      "1  cardiffnlp/twitter-roberta-base-sentiment-latest      cardiffnlp   \n",
      "2     jonatasgrosman/wav2vec2-large-xlsr-53-english  jonatasgrosman   \n",
      "3                     openai/clip-vit-large-patch14          openai   \n",
      "4                     google-bert/bert-base-uncased     google-bert   \n",
      "\n",
      "              last_modified  downloads  likes   library_name  \\\n",
      "0 2024-02-27 20:46:41+00:00  164336016     49  pysentimiento   \n",
      "1 2023-05-28 05:45:10+00:00  132880138    348   transformers   \n",
      "2 2023-03-25 10:56:55+00:00   57980766    391   transformers   \n",
      "3 2023-09-15 15:49:35+00:00   45378724   1039   transformers   \n",
      "4 2024-02-19 11:06:12+00:00   40564287   1443   transformers   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [pysentimiento, pytorch, tf, safetensors, robe...   \n",
      "1  [transformers, pytorch, tf, roberta, text-clas...   \n",
      "2  [transformers, pytorch, jax, safetensors, wav2...   \n",
      "3  [transformers, pytorch, tf, jax, safetensors, ...   \n",
      "4  [transformers, pytorch, tf, jax, rust, coreml,...   \n",
      "\n",
      "                     pipeline_tag                 createdAt  \\\n",
      "0                            None 2022-03-02 23:29:05+00:00   \n",
      "1             text-classification 2022-03-15 01:21:58+00:00   \n",
      "2    automatic-speech-recognition 2022-03-02 23:29:05+00:00   \n",
      "3  zero-shot-image-classification 2022-03-02 23:29:05+00:00   \n",
      "4                       fill-mask 2022-03-02 23:29:04+00:00   \n",
      "\n",
      "                                                card  \n",
      "0  ---\\nlanguage: \\n  - es\\nlibrary_name: pysenti...  \n",
      "1  ---\\nlanguage: en\\nwidget:\\n- text: Covid case...  \n",
      "2  ---\\nlanguage: en\\ndatasets:\\n- common_voice\\n...  \n",
      "3  ---\\ntags:\\n- vision\\nwidget:\\n- src: https://...  \n",
      "4  ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"librarian-bots/model_cards_with_metadata\")['train']\n",
    "HF_df = dataset.to_pandas()\n",
    "print(HF_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get tag information to extract data and answer some of the questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40747\n"
     ]
    }
   ],
   "source": [
    "#Counting the unique number of tags\n",
    "all_tags_set = set()\n",
    "for index, row in HF_df.iterrows():\n",
    "    tags = row['tags']\n",
    "    for tag in tags:\n",
    "        all_tags_set.add(tag)\n",
    "print(len(all_tags_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tags we can get the following information:\n",
    "* The task they are performing\n",
    "* The libraries that were used to train the model\n",
    "* The datasets used to train the model\n",
    "* The languages that the model can handle\n",
    "* The Licenses under which the model can be use. \n",
    "* Other type of tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "tr\n",
      "yi\n",
      "sv\n",
      "fi\n"
     ]
    }
   ],
   "source": [
    "#Getting the language tags\n",
    "tags_language = [val[0] for val in pd.read_csv(\"tags_language.tsv\",sep='\\t').values.tolist()]\n",
    "tags_language = set(tags_language)\n",
    "i = 0\n",
    "for tag in tags_language:\n",
    "    if i == 5:\n",
    "        break\n",
    "    i+=1\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX\n",
      "unity-sentis\n",
      "TensorBoard\n",
      "Graphcore\n",
      "Rust\n",
      "sample-factory\n",
      "AllenNLP\n",
      "Core ML\n",
      "JAX\n",
      "Diffusers\n",
      "PaddlePaddle\n",
      "OpenVINO\n",
      "ml-agents\n",
      "Keras\n",
      "timm\n",
      "ESPnet\n",
      "NeMo\n",
      "Scikit-learn\n",
      "SpanMarker\n",
      "setfit\n",
      "fastText\n",
      "sentence-transformers\n",
      "speechbrain\n",
      "BERTopic\n",
      "Fairseq\n",
      "Safetensors\n",
      "Habana\n",
      "spaCy\n",
      "TensorFlow\n",
      "PEFT\n",
      "Joblib\n",
      "Transformers\n",
      "Adapters\n",
      "OpenCLIP\n",
      "paddlenlp\n",
      "fastai\n",
      "Flair\n",
      "Stanza\n",
      "GGUF\n",
      "PyTorch\n",
      "stable-baselines3\n",
      "TF Lite\n",
      "pythae\n",
      "Transformers.js\n",
      "pyannote.audio\n",
      "Asteroid\n",
      "ONNX\n"
     ]
    }
   ],
   "source": [
    "#Getting the library tags\n",
    "tags_libraries = [val[0] for val in pd.read_csv(\"tags_libraries.tsv\",sep='\\t').values.tolist()]\n",
    "tags_libraries = set(tags_libraries)\n",
    "for tag in tags_libraries:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge\n",
      "AutoTrain Compatible\n",
      "Has a Space\n",
      "tags\n",
      "Eval Results\n",
      "8-bit precision\n",
      "Inference Endpoints\n",
      "Carbon Emissions\n",
      "Mixture of Experts\n",
      "custom_code\n",
      "text-generation-inference\n",
      "4-bit precision\n"
     ]
    }
   ],
   "source": [
    "#Getting the other tags\n",
    "tags_other = [val[0] for val in pd.read_csv(\"tags_other.tsv\",sep='\\t').values.tolist()]\n",
    "tags_other = set(tags_other)\n",
    "for tag in tags_other:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-Audio\n",
      "Feature Extraction\n",
      "Depth Estimation\n",
      "Text-to-3D\n",
      "Image-to-Video\n",
      "Robotics\n",
      "Token Classification\n",
      "Zero-Shot Classification\n",
      "Object Detection\n",
      "Automatic Speech Recognition\n",
      "Image Segmentation\n",
      "Text-to-Video\n",
      "Audio-to-Audio\n",
      "Question Answering\n",
      "Image-to-Text\n",
      "Image-Text-to-Text\n",
      "Zero-Shot Object Detection\n",
      "Text-to-Image\n",
      "Visual Question Answering\n",
      "Unconditional Image Generation\n",
      "Zero-Shot Image Classification\n",
      "Image Classification\n",
      "Graph Machine Learning\n",
      "Video Classification\n",
      "Translation\n",
      "Image-to-3D\n",
      "Tabular Regression\n",
      "Text-to-Speech\n",
      "Summarization\n",
      "Mask Generation\n",
      "Document Question Answering\n",
      "Text Generation\n",
      "Fill-Mask\n",
      "Voice Activity Detection\n",
      "Text2Text Generation\n",
      "Table Question Answering\n",
      "Sentence Similarity\n",
      "Reinforcement Learning\n",
      "Text Classification\n",
      "Audio Classification\n",
      "Tabular Classification\n",
      "Image Feature Extraction\n"
     ]
    }
   ],
   "source": [
    "#Getting the task tags\n",
    "tags_task = [val[0] for val in pd.read_csv(\"tags_task.tsv\",sep='\\t').values.tolist()]\n",
    "tags_task = set(tags_task)\n",
    "for tag in tags_task:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ineoApp/factures\n",
      "mattwiner/autotrain-data-baseball-jersey-detection\n",
      "MThonar/link\n",
      "misshimichka/flower_faces_dataset_v2\n",
      "andyflinn/heidiland\n"
     ]
    }
   ],
   "source": [
    "#Getting the dataset tags\n",
    "dataset_datasets = load_dataset(\"librarian-bots/dataset_cards_with_metadata\")['train']\n",
    "tags_dataset = set(dataset_datasets[\"datasetId\"])\n",
    "i = 5\n",
    "for tag in tags_dataset:\n",
    "    if(i==0):\n",
    "        break\n",
    "    print(tag)\n",
    "    i-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let's set up the questions we want the model to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform QA for a question-context pair\n",
    "def answer_question(question, context):\n",
    "  answer = qa_pipeline({\"question\": question, \"context\": context})\n",
    "  print(\"Question:\", question)\n",
    "  print(\"Answer:\", answer,\"/n\")\n",
    "  return answer['answer']+''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            modelId          author  \\\n",
      "0       pysentimiento/robertuito-sentiment-analysis   pysentimiento   \n",
      "1  cardiffnlp/twitter-roberta-base-sentiment-latest      cardiffnlp   \n",
      "2     jonatasgrosman/wav2vec2-large-xlsr-53-english  jonatasgrosman   \n",
      "3                     openai/clip-vit-large-patch14          openai   \n",
      "4                     google-bert/bert-base-uncased     google-bert   \n",
      "\n",
      "              last_modified  downloads  likes   library_name  \\\n",
      "0 2024-02-27 20:46:41+00:00  164336016     49  pysentimiento   \n",
      "1 2023-05-28 05:45:10+00:00  132880138    348   transformers   \n",
      "2 2023-03-25 10:56:55+00:00   57980766    391   transformers   \n",
      "3 2023-09-15 15:49:35+00:00   45378724   1039   transformers   \n",
      "4 2024-02-19 11:06:12+00:00   40564287   1443   transformers   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [pysentimiento, pytorch, tf, safetensors, robe...   \n",
      "1  [transformers, pytorch, tf, roberta, text-clas...   \n",
      "2  [transformers, pytorch, jax, safetensors, wav2...   \n",
      "3  [transformers, pytorch, tf, jax, safetensors, ...   \n",
      "4  [transformers, pytorch, tf, jax, rust, coreml,...   \n",
      "\n",
      "                     pipeline_tag                 createdAt  \\\n",
      "0                            None 2022-03-02 23:29:05+00:00   \n",
      "1             text-classification 2022-03-15 01:21:58+00:00   \n",
      "2    automatic-speech-recognition 2022-03-02 23:29:05+00:00   \n",
      "3  zero-shot-image-classification 2022-03-02 23:29:05+00:00   \n",
      "4                       fill-mask 2022-03-02 23:29:04+00:00   \n",
      "\n",
      "                                                card  ... q_id_8 q_id_9  \\\n",
      "0  ---\\nlanguage: \\n  - es\\nlibrary_name: pysenti...  ...   None   None   \n",
      "1  ---\\nlanguage: en\\nwidget:\\n- text: Covid case...  ...   None   None   \n",
      "2  ---\\nlanguage: en\\ndatasets:\\n- common_voice\\n...  ...   None   None   \n",
      "3  ---\\ntags:\\n- vision\\nwidget:\\n- src: https://...  ...   None   None   \n",
      "4  ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  ...   None   None   \n",
      "\n",
      "  q_id_10 q_id_11 q_id_12 q_id_13 q_id_14 q_id_15 q_id_16 q_id_17  \n",
      "0    None    None    None    None    None    None    None    None  \n",
      "1    None    None    None    None    None    None    None    None  \n",
      "2    None    None    None    None    None    None    None    None  \n",
      "3    None    None    None    None    None    None    None    None  \n",
      "4    None    None    None    None    None    None    None    None  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create new columns in the dataframe\n",
    "HF_df_small = HF_df.iloc[0:10] \n",
    "\n",
    "new_columns = {}\n",
    "\n",
    "for idx in range(len(questions)):\n",
    "    q_id = \"q_id_\"+str(idx)\n",
    "    new_columns[q_id] = [None for _ in range(len(HF_df_small))]\n",
    "\n",
    "HF_df_small = HF_df_small.assign(**new_columns)\n",
    "\n",
    "print(HF_df_small.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pysentimiento\n",
      "0 pytorch\n",
      "0 tf\n",
      "0 safetensors\n",
      "0 roberta\n",
      "0 twitter\n",
      "0 sentiment-analysis\n",
      "0 es\n",
      "0 arxiv:2106.09462\n",
      "0 has_space\n",
      "0 region:us\n",
      "1 transformers\n",
      "1 pytorch\n",
      "1 tf\n",
      "1 roberta\n",
      "1 text-classification\n",
      "1 en\n",
      "1 dataset:tweet_eval\n",
      "1 arxiv:2202.03829\n",
      "1 autotrain_compatible\n",
      "1 endpoints_compatible\n",
      "1 has_space\n",
      "1 region:us\n",
      "2 transformers\n",
      "2 pytorch\n",
      "2 jax\n",
      "2 safetensors\n",
      "2 wav2vec2\n",
      "2 automatic-speech-recognition\n",
      "2 audio\n",
      "2 en\n",
      "2 hf-asr-leaderboard\n",
      "2 mozilla-foundation/common_voice_6_0\n",
      "2 robust-speech-event\n",
      "2 speech\n",
      "2 xlsr-fine-tuning-week\n",
      "2 dataset:common_voice\n",
      "2 dataset:mozilla-foundation/common_voice_6_0\n",
      "2 license:apache-2.0\n",
      "2 model-index\n",
      "2 endpoints_compatible\n",
      "2 has_space\n",
      "2 region:us\n",
      "3 transformers\n",
      "3 pytorch\n",
      "3 tf\n",
      "3 jax\n",
      "3 safetensors\n",
      "3 clip\n",
      "3 zero-shot-image-classification\n",
      "3 vision\n",
      "3 arxiv:2103.00020\n",
      "3 arxiv:1908.04913\n",
      "3 endpoints_compatible\n",
      "3 has_space\n",
      "3 region:us\n",
      "4 transformers\n",
      "4 pytorch\n",
      "4 tf\n",
      "4 jax\n",
      "4 rust\n",
      "4 coreml\n",
      "4 onnx\n",
      "4 safetensors\n",
      "4 bert\n",
      "4 fill-mask\n",
      "4 exbert\n",
      "4 en\n",
      "4 dataset:bookcorpus\n",
      "4 dataset:wikipedia\n",
      "4 arxiv:1810.04805\n",
      "4 license:apache-2.0\n",
      "4 autotrain_compatible\n",
      "4 endpoints_compatible\n",
      "4 has_space\n",
      "4 region:us\n",
      "5 transformers\n",
      "5 pytorch\n",
      "5 tensorboard\n",
      "5 safetensors\n",
      "5 roberta\n",
      "5 text-classification\n",
      "5 generated_from_trainer\n",
      "5 financial\n",
      "5 stocks\n",
      "5 sentiment\n",
      "5 dataset:financial_phrasebank\n",
      "5 license:apache-2.0\n",
      "5 model-index\n",
      "5 autotrain_compatible\n",
      "5 endpoints_compatible\n",
      "5 has_space\n",
      "5 region:us\n",
      "6 transformers\n",
      "6 pytorch\n",
      "6 tf\n",
      "6 bert\n",
      "6 token-classification\n",
      "6 ar\n",
      "6 arxiv:2103.06678\n",
      "6 license:apache-2.0\n",
      "6 autotrain_compatible\n",
      "6 endpoints_compatible\n",
      "6 has_space\n",
      "6 region:us\n",
      "7 sentence-transformers\n",
      "7 pytorch\n",
      "7 tf\n",
      "7 rust\n",
      "7 safetensors\n",
      "7 bert\n",
      "7 feature-extraction\n",
      "7 sentence-similarity\n",
      "7 transformers\n",
      "7 en\n",
      "7 dataset:s2orc\n",
      "7 dataset:flax-sentence-embeddings/stackexchange_xml\n",
      "7 dataset:ms_marco\n",
      "7 dataset:gooaq\n",
      "7 dataset:yahoo_answers_topics\n",
      "7 dataset:code_search_net\n",
      "7 dataset:search_qa\n",
      "7 dataset:eli5\n",
      "7 dataset:snli\n",
      "7 dataset:multi_nli\n",
      "7 dataset:wikihow\n",
      "7 dataset:natural_questions\n",
      "7 dataset:trivia_qa\n",
      "7 dataset:embedding-data/sentence-compression\n",
      "7 dataset:embedding-data/flickr30k-captions\n",
      "7 dataset:embedding-data/altlex\n",
      "7 dataset:embedding-data/simple-wiki\n",
      "7 dataset:embedding-data/QQP\n",
      "7 dataset:embedding-data/SPECTER\n",
      "7 dataset:embedding-data/PAQ_pairs\n",
      "7 dataset:embedding-data/WikiAnswers\n",
      "7 arxiv:1904.06472\n",
      "7 arxiv:2102.07033\n",
      "7 arxiv:2104.08727\n",
      "7 arxiv:1704.05179\n",
      "7 arxiv:1810.09305\n",
      "7 license:apache-2.0\n",
      "7 endpoints_compatible\n",
      "7 has_space\n",
      "7 region:us\n",
      "8 transformers\n",
      "8 pytorch\n",
      "8 tf\n",
      "8 jax\n",
      "8 bert\n",
      "8 fill-mask\n",
      "8 ja\n",
      "8 dataset:wikipedia\n",
      "8 license:cc-by-sa-4.0\n",
      "8 autotrain_compatible\n",
      "8 endpoints_compatible\n",
      "8 has_space\n",
      "8 region:us\n",
      "9 transformers\n",
      "9 pytorch\n",
      "9 tf\n",
      "9 jax\n",
      "9 rust\n",
      "9 safetensors\n",
      "9 distilbert\n",
      "9 fill-mask\n",
      "9 exbert\n",
      "9 en\n",
      "9 dataset:bookcorpus\n",
      "9 dataset:wikipedia\n",
      "9 arxiv:1910.01108\n",
      "9 license:apache-2.0\n",
      "9 autotrain_compatible\n",
      "9 endpoints_compatible\n",
      "9 has_space\n",
      "9 region:us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87/3521967764.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_16\"][index] = []\n",
      "/tmp/ipykernel_87/3521967764.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_13\"][index] = []\n",
      "/tmp/ipykernel_87/3521967764.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_4\"][index] = []\n",
      "/tmp/ipykernel_87/3521967764.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_15\"][index] = []\n",
      "/tmp/ipykernel_87/3521967764.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[\"q_id_17\"][index] = []\n"
     ]
    }
   ],
   "source": [
    "# Process tags\n",
    "\n",
    "for index, row in HF_df_small.iterrows():\n",
    "    for tag in row['tags']:\n",
    "        print(index,tag)\n",
    "        #Check question 3\n",
    "        if tag in tags_task:\n",
    "            if(HF_df_small[\"q_id_3\"][index] == None):\n",
    "                HF_df_small[\"q_id_3\"][index] = []\n",
    "            HF_df_small[\"q_id_3\"][index].append(tag)\n",
    "        #Check question 4\n",
    "        if \"dataset:\" in tag:\n",
    "            if(HF_df_small[\"q_id_4\"][index] == None):\n",
    "                HF_df_small[\"q_id_4\"][index] = []\n",
    "            HF_df_small[\"q_id_4\"][index].append(tag.replace(\"dataset:\",\"\"))\n",
    "        #Check question 13\n",
    "        if \"arxiv:\" in tag:\n",
    "            if(HF_df_small[\"q_id_13\"][index] == None):\n",
    "                HF_df_small[\"q_id_13\"][index] = []\n",
    "            HF_df_small[\"q_id_13\"][index].append(tag.replace(\"arxiv:\",\"\"))\n",
    "         #Check question 15\n",
    "        if \"license:\" in tag:\n",
    "            if(HF_df_small[\"q_id_15\"][index] == None):\n",
    "                HF_df_small[\"q_id_15\"][index] = []\n",
    "            HF_df_small[\"q_id_15\"][index].append(tag.replace(\"license:\",\"\"))\n",
    "        #Check question 16\n",
    "        if tag in tags_language:\n",
    "            if(HF_df_small[\"q_id_16\"][index] == None):\n",
    "                HF_df_small[\"q_id_16\"][index] = []\n",
    "            HF_df_small[\"q_id_16\"][index].append(tag)\n",
    "        #Check question 17\n",
    "        if tag in tags_libraries:\n",
    "            if(HF_df_small[\"q_id_17\"][index] == None):\n",
    "                HF_df_small[\"q_id_17\"][index] = []\n",
    "            HF_df_small[\"q_id_17\"][index].append(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  q_id_3                                             q_id_4  \\\n",
      "0   None                                               None   \n",
      "1   None                                       [tweet_eval]   \n",
      "2   None  [common_voice, mozilla-foundation/common_voice...   \n",
      "3   None                                               None   \n",
      "4   None                            [bookcorpus, wikipedia]   \n",
      "5   None                             [financial_phrasebank]   \n",
      "6   None                                               None   \n",
      "7   None  [s2orc, flax-sentence-embeddings/stackexchange...   \n",
      "8   None                                        [wikipedia]   \n",
      "9   None                            [bookcorpus, wikipedia]   \n",
      "\n",
      "                                             q_id_13         q_id_15 q_id_16  \\\n",
      "0                       pysentimiento and RoBERTuito            None    [es]   \n",
      "1          Association for Computational Linguistics            None    [en]   \n",
      "2                    grosman2021xlsr53-large-english    [apache-2.0]    [en]   \n",
      "3      crime-related and non-human animal categories            None    None   \n",
      "4                                              [CLS]    [apache-2.0]    [en]   \n",
      "5                     Transformers 4.10.2\\n- Pytorch    [apache-2.0]    None   \n",
      "6  Modern Standard Arabic (MSA), dialectal Arabic...    [apache-2.0]    [ar]   \n",
      "7                      aclanthology.org/P18-2124.pdf    [apache-2.0]    [en]   \n",
      "8       Creative Commons Attribution-ShareAlike 3.0]  [cc-by-sa-4.0]    [ja]   \n",
      "9                                              [CLS]    [apache-2.0]    [en]   \n",
      "\n",
      "                   q_id_17  \n",
      "0                     None  \n",
      "1                     None  \n",
      "2                     None  \n",
      "3                     None  \n",
      "4                     None  \n",
      "5                     None  \n",
      "6                     None  \n",
      "7  [sentence-transformers]  \n",
      "8                     None  \n",
      "9                     None  \n"
     ]
    }
   ],
   "source": [
    "print(HF_df_small[['q_id_3', 'q_id_4','q_id_13','q_id_15','q_id_16','q_id_17']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.0015278777573257685, 'start': 3876, 'end': 3886, 'answer': 'RoBERTuito'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 1.177582831246582e-08, 'start': 1277, 'end': 1292, 'answer': '| 0.670 ± 0.006'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.45697927474975586, 'start': 440, 'end': 447, 'answer': 'RoBERTa'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.0034985386300832033, 'start': 949, 'end': 964, 'answer': 'Macro F1 scores'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 8.506868471158668e-05, 'start': 924, 'end': 964, 'answer': 'Results are expressed as Macro F1 scores'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 2.9859666028642096e-06, 'start': 3407, 'end': 3425, 'answer': '500 million tweets'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 0.3909691572189331, 'start': 1857, 'end': 1885, 'answer': 'pysentimiento and RoBERTuito'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.001176113961264491, 'start': 465, 'end': 479, 'answer': 'Spanish tweets'} /n\n",
      "Answers added for row 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87/892254660.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  HF_df_small[question][index] = answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 3.0807539133093087e-06, 'start': 169, 'end': 181, 'answer': 'RoBERTa-base'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.005795312579721212, 'start': 1601, 'end': 1612, 'answer': \"t = '@user'\"} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.032325588166713715, 'start': 169, 'end': 181, 'answer': 'RoBERTa-base'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 1.642551796976477e-05, 'start': 4029, 'end': 4050, 'answer': 'System Demonstrations'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.031056080013513565, 'start': 4293, 'end': 4301, 'answer': '251--260'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 4.464515313884476e-06, 'start': 200, 'end': 204, 'answer': '124M'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 0.00018329665181227028, 'start': 4139, 'end': 4180, 'answer': 'Association for Computational Linguistics'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.0015904769534245133, 'start': 4104, 'end': 4119, 'answer': 'Dublin, Ireland'} /n\n",
      "Answers added for row 1\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.07963903993368149, 'start': 331, 'end': 346, 'answer': 'Jonatas Grosman'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.2762572467327118, 'start': 1466, 'end': 1482, 'answer': 'Common Voice 6.1'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.002199214417487383, 'start': 1257, 'end': 1264, 'answer': 'XLSR-53'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 8.480140240862966e-05, 'start': 991, 'end': 1005, 'answer': 'en\\n    metrics'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.02585449628531933, 'start': 4477, 'end': 4493, 'answer': '## Evaluation\\n\\n1'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 3.494284896987665e-07, 'start': 1601, 'end': 1606, 'answer': '16kHz'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 2.1557440277319984e-08, 'start': 5073, 'end': 5104, 'answer': 'grosman2021xlsr53-large-english'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 1.9328557755216025e-05, 'start': 1891, 'end': 1899, 'answer': 'directly'} /n\n",
      "Answers added for row 2\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.007590511813759804, 'start': 435, 'end': 456, 'answer': 'researchers at OpenAI'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.11769309639930725, 'start': 7478, 'end': 7517, 'answer': '~93% for racial classification and ~63%'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.34699735045433044, 'start': 947, 'end': 980, 'answer': 'ViT-L/14 Transformer architecture'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.16540269553661346, 'start': 6249, 'end': 6262, 'answer': 'linear probes'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.06842001527547836, 'start': 7652, 'end': 7730, 'answer': 'to evaluate performance of the model across people and surface potential risks'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 4.65692835405207e-08, 'start': 647, 'end': 656, 'answer': 'zero-shot'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 9.940432391886134e-06, 'start': 6743, 'end': 6788, 'answer': 'crime-related and non-human animal categories'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.02839803323149681, 'start': 870, 'end': 876, 'answer': 'within'} /n\n",
      "Answers added for row 3\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.21401749551296234, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.23093131184577942, 'start': 6117, 'end': 6122, 'answer': '[CLS]'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.22914651036262512, 'start': 6117, 'end': 6122, 'answer': '[CLS]'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.1586027294397354, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.21457310020923615, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.23381927609443665, 'start': 6117, 'end': 6122, 'answer': '[CLS]'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 0.19773608446121216, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.1862080991268158, 'start': 8017, 'end': 8022, 'answer': '[CLS]'} /n\n",
      "Answers added for row 4\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.0034645982086658478, 'start': 786, 'end': 793, 'answer': 'Trainer'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.6045812368392944, 'start': 2213, 'end': 2245, 'answer': 'agreement rate of 5-8 annotators'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.07886247336864471, 'start': 1441, 'end': 1459, 'answer': 'RoBERTa-base model'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.09674705564975739, 'start': 2315, 'end': 2330, 'answer': 'hyperparameters'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.1959497481584549, 'start': 3076, 'end': 3102, 'answer': '1.12.1\\n- Tokenizers 0.10.3'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.07028842717409134, 'start': 2301, 'end': 2330, 'answer': 'The following hyperparameters'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 4.727654595626518e-05, 'start': 3023, 'end': 3052, 'answer': 'Transformers 4.10.2\\n- Pytorch'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 1.0802537872223184e-05, 'start': 3036, 'end': 3052, 'answer': '4.10.2\\n- Pytorch'} /n\n",
      "Answers added for row 5\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.043573614209890366, 'start': 1690, 'end': 1695, 'answer': 'Inoue'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.20432844758033752, 'start': 2717, 'end': 2719, 'answer': '12'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.0018236850155517459, 'start': 693, 'end': 714, 'answer': 'CAMeLBERT-Mix POS-EGY'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.024078188464045525, 'start': 2717, 'end': 2728, 'answer': '12 datasets'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 1.724358298815787e-05, 'start': 2821, 'end': 2867, 'answer': 'more important than the pre-training data size'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.002081586280837655, 'start': 472, 'end': 500, 'answer': 'Variant, Size, and Task Type'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 0.222324937582016, 'start': 2275, 'end': 2343, 'answer': 'Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.0009599358891136944, 'start': 2541, 'end': 2575, 'answer': 'scaled-down set of the MSA variant'} /n\n",
      "Answers added for row 6\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.49784594774246216, 'start': 3873, 'end': 3885, 'answer': 'Hugging Face'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 1.4129298506304622e-05, 'start': 5365, 'end': 5375, 'answer': '128 tokens'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.0015394422225654125, 'start': 860, 'end': 881, 'answer': 'Sentence-Transformers'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.004939008504152298, 'start': 818, 'end': 847, 'answer': 'clustering or semantic search'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 9.756165127328131e-06, 'start': 2959, 'end': 2993, 'answer': 'automated evaluation of this model'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.0011639489093795419, 'start': 5197, 'end': 5205, 'answer': 'TPU v3-8'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 4.5487368538488226e-07, 'start': 10511, 'end': 10540, 'answer': 'aclanthology.org/P18-2124.pdf'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.00019294196681585163, 'start': 752, 'end': 786, 'answer': '384 dimensional dense vector space'} /n\n",
      "Answers added for row 7\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 4.289135176804848e-06, 'start': 612, 'end': 627, 'answer': 'BERT base model'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.03977341949939728, 'start': 1219, 'end': 1238, 'answer': 'WordPiece algorithm'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.4635535776615143, 'start': 612, 'end': 627, 'answer': 'BERT base model'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 9.21803257369902e-06, 'start': 1409, 'end': 1426, 'answer': '1M training steps'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 1.6510809928149683e-06, 'start': 629, 'end': 695, 'answer': '12 layers, 768 dimensions of hidden states, and 12 attention heads'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.0004916482721455395, 'start': 1409, 'end': 1411, 'answer': '1M'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 8.88812865014188e-05, 'start': 1504, 'end': 1548, 'answer': 'Creative Commons Attribution-ShareAlike 3.0]'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.02357877977192402, 'start': 740, 'end': 758, 'answer': 'Japanese Wikipedia'} /n\n",
      "Answers added for row 8\n",
      "Question: Who created/uploaded the model?\n",
      "Answer: {'score': 0.23268979787826538, 'start': 4759, 'end': 4764, 'answer': '[CLS]'} /n\n",
      "Question: What was the split distribution for the datasets?\n",
      "Answer: {'score': 0.23583264648914337, 'start': 6763, 'end': 6768, 'answer': '[CLS]'} /n\n",
      "Question: What model is used as the base model?\n",
      "Answer: {'score': 0.329690545797348, 'start': 8468, 'end': 8491, 'answer': 'distilbert-base-uncased'} /n\n",
      "Question: What evaluation metrics were used?\n",
      "Answer: {'score': 0.23366063833236694, 'start': 4759, 'end': 4764, 'answer': '[CLS]'} /n\n",
      "Question: What were the values of the evaluation metrics?\n",
      "Answer: {'score': 0.2235749512910843, 'start': 4759, 'end': 4764, 'answer': '[CLS]'} /n\n",
      "Question: What hyperparameters were optimized during the training process?\n",
      "Answer: {'score': 0.2174941748380661, 'start': 6763, 'end': 6768, 'answer': '[CLS]'} /n\n",
      "Question: What publications are related to the model?\n",
      "Answer: {'score': 0.23931065201759338, 'start': 4759, 'end': 4764, 'answer': '[CLS]'} /n\n",
      "Question: Where is the model deployed?\n",
      "Answer: {'score': 0.22288861870765686, 'start': 6763, 'end': 6768, 'answer': '[CLS]'} /n\n",
      "Answers added for row 9\n"
     ]
    }
   ],
   "source": [
    "# Process card\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"Intel/dynamic_tinybert\")\n",
    "\n",
    "questions_to_process = {5,8,9,10,11,14}\n",
    "\n",
    "# Process each row of the DataFrame\n",
    "for index, row in HF_df_small.iterrows():\n",
    "  context = row[\"card\"]  # Assuming \"card\" column contains the context\n",
    "\n",
    "  # Create an empty dictionary to store answers for each question\n",
    "  answers = {}\n",
    "  q_cnt = -1\n",
    "  for question in questions:\n",
    "    q_cnt+=1\n",
    "    if(q_cnt not in questions_to_process): continue\n",
    "    q = question[0]\n",
    "    answer = answer_question(q, context)\n",
    "    q_id = \"q_id_\"+str(q_cnt)\n",
    "    answers[q_id] = answer  # Store answer for each question\n",
    "  \n",
    "  # Add a new column for each question and populate with answers\n",
    "  for question, answer in answers.items():\n",
    "    HF_df_small[question][index] = answer\n",
    "    \n",
    "\n",
    "  print(\"Answers added for row\", index)  # Optional progress indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             modelId          author  \\\n",
      "1   cardiffnlp/twitter-roberta-base-sentiment-latest      cardiffnlp   \n",
      "2      jonatasgrosman/wav2vec2-large-xlsr-53-english  jonatasgrosman   \n",
      "3                      openai/clip-vit-large-patch14          openai   \n",
      "4                      google-bert/bert-base-uncased     google-bert   \n",
      "5  mrm8488/distilroberta-finetuned-financial-news...         mrm8488   \n",
      "\n",
      "              last_modified  downloads  likes  library_name  \\\n",
      "1 2023-05-28 05:45:10+00:00  132880138    348  transformers   \n",
      "2 2023-03-25 10:56:55+00:00   57980766    391  transformers   \n",
      "3 2023-09-15 15:49:35+00:00   45378724   1039  transformers   \n",
      "4 2024-02-19 11:06:12+00:00   40564287   1443  transformers   \n",
      "5 2024-01-21 15:17:58+00:00   34522962    213  transformers   \n",
      "\n",
      "                                                tags  \\\n",
      "1  [transformers, pytorch, tf, roberta, text-clas...   \n",
      "2  [transformers, pytorch, jax, safetensors, wav2...   \n",
      "3  [transformers, pytorch, tf, jax, safetensors, ...   \n",
      "4  [transformers, pytorch, tf, jax, rust, coreml,...   \n",
      "5  [transformers, pytorch, tensorboard, safetenso...   \n",
      "\n",
      "                     pipeline_tag                 createdAt  \\\n",
      "1             text-classification 2022-03-15 01:21:58+00:00   \n",
      "2    automatic-speech-recognition 2022-03-02 23:29:05+00:00   \n",
      "3  zero-shot-image-classification 2022-03-02 23:29:05+00:00   \n",
      "4                       fill-mask 2022-03-02 23:29:04+00:00   \n",
      "5             text-classification 2022-03-02 23:29:05+00:00   \n",
      "\n",
      "                                                card  ...  \\\n",
      "1  ---\\nlanguage: en\\nwidget:\\n- text: Covid case...  ...   \n",
      "2  ---\\nlanguage: en\\ndatasets:\\n- common_voice\\n...  ...   \n",
      "3  ---\\ntags:\\n- vision\\nwidget:\\n- src: https://...  ...   \n",
      "4  ---\\nlanguage: en\\ntags:\\n- exbert\\nlicense: a...  ...   \n",
      "5  ---\\nlicense: apache-2.0\\nthumbnail: https://h...  ...   \n",
      "\n",
      "                              q_id_8                 q_id_9  \\\n",
      "1                       RoBERTa-base  System Demonstrations   \n",
      "2                            XLSR-53        en\\n    metrics   \n",
      "3  ViT-L/14 Transformer architecture          linear probes   \n",
      "4                              [CLS]                  [CLS]   \n",
      "5                 RoBERTa-base model        hyperparameters   \n",
      "\n",
      "                                             q_id_10  \\\n",
      "1                                           251--260   \n",
      "2                                 ## Evaluation\\n\\n1   \n",
      "3  to evaluate performance of the model across pe...   \n",
      "4                                              [CLS]   \n",
      "5                        1.12.1\\n- Tokenizers 0.10.3   \n",
      "\n",
      "                         q_id_11 q_id_12  \\\n",
      "1                           124M    None   \n",
      "2                          16kHz    None   \n",
      "3                      zero-shot    None   \n",
      "4                          [CLS]    None   \n",
      "5  The following hyperparameters    None   \n",
      "\n",
      "                                         q_id_13            q_id_14 q_id_15  \\\n",
      "1      Association for Computational Linguistics    Dublin, Ireland    None   \n",
      "2                grosman2021xlsr53-large-english           directly    None   \n",
      "3  crime-related and non-human animal categories             within    None   \n",
      "4                                          [CLS]              [CLS]    None   \n",
      "5                 Transformers 4.10.2\\n- Pytorch  4.10.2\\n- Pytorch    None   \n",
      "\n",
      "  q_id_16 q_id_17  \n",
      "1    [en]    None  \n",
      "2    [en]    None  \n",
      "3    None    None  \n",
      "4    [en]    None  \n",
      "5    None    None  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "print(HF_df_small.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
