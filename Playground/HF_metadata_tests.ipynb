{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to extract data from HF repository\n",
    "\n",
    "The idea is to get familiarize with the HF API to extract data from ML repositories.\n",
    "Is important to realize that relevant metadata basically can be found in the readme.MD file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a specific file\n",
    "\n",
    "In the quickstart guide of the [Hugging Face Hub API](https://huggingface.co/docs/huggingface_hub/v0.8.0/en/quick-start), there is an example on how to retrieve particular documents from the ML repository.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vscode/.cache/huggingface/hub/models--google--pegasus-xsum/snapshots/8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b/README.md'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"google/pegasus-xsum\", filename=\"README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to pick where is the document being downloaded. Even check if we can bring it to memory right away.\n",
    "\n",
    "For the first problem we have the following [documentation](https://huggingface.co/docs/huggingface_hub/main/en/guides/download). We can use the `local_dir=\"path/to/folder\"` attribute of the `hf_hub_download` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vscode/README_files/README.md'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_hub_download(repo_id=\"google/pegasus-xsum\", filename=\"README.md\",local_dir=\"/home/vscode/README_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model cards\n",
    "Even better than the above, there are ways to only query the Model card metadata that is usually on the README.md file, using the [ModelCard](https://huggingface.co/docs/huggingface_hub/main/en/guides/model-cards) function. \n",
    "\n",
    "Now the problem with this approach is clear, not all the metadata can be found on the Model card, a lot of useful information can be extracted from the README.md file.\n",
    "\n",
    "Even with the previous setback in can said that this functionality really helps to proccess the data better. \n",
    "\n",
    "Now I don't know if it is worth to spend one of the 10k limited requests we have every minute on a preprocessing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Pegasus Models\n",
      "See Docs: [here](https://huggingface.co/transformers/master/model_doc/pegasus.html)\n",
      "\n",
      "Original TF 1 code [here](https://github.com/google-research/pegasus)\n",
      "\n",
      "Authors: Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019\n",
      "\n",
      "Maintained by: [@sshleifer](https://twitter.com/sam_shleifer)\n",
      "\n",
      "Task: Summarization\n",
      "\n",
      "The following is copied from the authors' README.\n",
      "\n",
      "# Mixed & Stochastic Checkpoints\n",
      "\n",
      "We train a pegasus model with sampled gap sentence ratios on both C4 and HugeNews, and stochastically sample important sentences. The updated the results are reported in this table.\n",
      "\n",
      "| dataset | C4 | HugeNews | Mixed & Stochastic|\n",
      "| ---- | ---- | ---- | ----|\n",
      "| xsum | 45.20/22.06/36.99 | 47.21/24.56/39.25 | 47.60/24.83/39.64|\n",
      "| cnn_dailymail | 43.90/21.20/40.76 | 44.17/21.47/41.11 | 44.16/21.56/41.30|\n",
      "| newsroom | 45.07/33.39/41.28 | 45.15/33.51/41.33 | 45.98/34.20/42.18|\n",
      "| multi_news | 46.74/17.95/24.26 | 47.52/18.72/24.91 | 47.65/18.75/24.95|\n",
      "| gigaword | 38.75/19.96/36.14 | 39.12/19.86/36.24 | 39.65/20.47/36.76|\n",
      "| wikihow | 43.07/19.70/34.79 | 41.35/18.51/33.42 | 46.39/22.12/38.41 *|\n",
      "| reddit_tifu | 26.54/8.94/21.64 | 26.63/9.01/21.60 | 27.99/9.81/22.94|\n",
      "| big_patent | 53.63/33.16/42.25 | 53.41/32.89/42.07 | 52.29/33.08/41.66 *|\n",
      "| arxiv | 44.70/17.27/25.80 | 44.67/17.18/25.73 | 44.21/16.95/25.67|\n",
      "| pubmed | 45.49/19.90/27.69 | 45.09/19.56/27.42 | 45.97/20.15/28.25|\n",
      "| aeslc | 37.69/21.85/36.84 | 37.40/21.22/36.45 | 37.68/21.25/36.51|\n",
      "| billsum | 57.20/39.56/45.80 | 57.31/40.19/45.82 | 59.67/41.58/47.59|\n",
      "\n",
      "The \"Mixed & Stochastic\" model has the following changes:\n",
      "- trained on both C4 and HugeNews (dataset mixture is weighted by their number of examples). \n",
      "- trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).\n",
      "- the model uniformly sample a gap sentence ratio between 15% and 45%.\n",
      "- importance sentences are sampled using a 20% uniform noise to importance scores.\n",
      "- the sentencepiece tokenizer is updated to be able to encode newline character.\n",
      "\n",
      "\n",
      "(*) the numbers of wikihow and big_patent datasets are not comparable because of change in tokenization and data:\n",
      "- wikihow dataset contains newline characters which is useful for paragraph segmentation, the C4 and HugeNews model's sentencepiece tokenizer doesn't encode newline and loose this information.\n",
      "- we update the BigPatent dataset to preserve casing, some format cleanings are also changed, please refer to change in TFDS.\n",
      "\n",
      "\n",
      "The \"Mixed & Stochastic\" model has the following changes (from pegasus-large in the paper):\n",
      "\n",
      "\n",
      "trained on both C4 and HugeNews (dataset mixture is weighted by their number of examples).\n",
      "trained for 1.5M instead of 500k (we observe slower convergence on pretraining perplexity).\n",
      "the model uniformly sample a gap sentence ratio between 15% and 45%.\n",
      "importance sentences are sampled using a 20% uniform noise to importance scores.\n",
      "the sentencepiece tokenizer is updated to be able to encode newline character.\n",
      "\n",
      "\n",
      "Citation\n",
      "```\n",
      "\n",
      "\n",
      "@misc{zhang2019pegasus,\n",
      "    title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},\n",
      "    author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},\n",
      "    year={2019},\n",
      "    eprint={1912.08777},\n",
      "    archivePrefix={arXiv},\n",
      "    primaryClass={cs.CL}\n",
      "}\n",
      "```\n",
      "language: en\n",
      "tags:\n",
      "- summarization\n",
      "model-index:\n",
      "- name: google/pegasus-xsum\n",
      "  results:\n",
      "  - task:\n",
      "      type: summarization\n",
      "      name: Summarization\n",
      "    dataset:\n",
      "      name: samsum\n",
      "      type: samsum\n",
      "      config: samsum\n",
      "      split: train\n",
      "    metrics:\n",
      "    - type: rouge\n",
      "      value: 21.8096\n",
      "      name: ROUGE-1\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 4.2525\n",
      "      name: ROUGE-2\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 17.4469\n",
      "      name: ROUGE-L\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 18.8907\n",
      "      name: ROUGE-LSUM\n",
      "      verified: true\n",
      "    - type: loss\n",
      "      value: 3.0317161083221436\n",
      "      name: loss\n",
      "      verified: true\n",
      "    - type: gen_len\n",
      "      value: 20.3122\n",
      "      name: gen_len\n",
      "      verified: true\n",
      "  - task:\n",
      "      type: summarization\n",
      "      name: Summarization\n",
      "    dataset:\n",
      "      name: xsum\n",
      "      type: xsum\n",
      "      config: default\n",
      "      split: test\n",
      "    metrics:\n",
      "    - type: rouge\n",
      "      value: 46.8623\n",
      "      name: ROUGE-1\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 24.4533\n",
      "      name: ROUGE-2\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 39.0548\n",
      "      name: ROUGE-L\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 39.0994\n",
      "      name: ROUGE-LSUM\n",
      "      verified: true\n",
      "    - type: loss\n",
      "      value: 1.5717021226882935\n",
      "      name: loss\n",
      "      verified: true\n",
      "    - type: gen_len\n",
      "      value: 22.8821\n",
      "      name: gen_len\n",
      "      verified: true\n",
      "  - task:\n",
      "      type: summarization\n",
      "      name: Summarization\n",
      "    dataset:\n",
      "      name: cnn_dailymail\n",
      "      type: cnn_dailymail\n",
      "      config: 3.0.0\n",
      "      split: test\n",
      "    metrics:\n",
      "    - type: rouge\n",
      "      value: 22.2062\n",
      "      name: ROUGE-1\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 7.6701\n",
      "      name: ROUGE-2\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 15.4046\n",
      "      name: ROUGE-L\n",
      "      verified: true\n",
      "    - type: rouge\n",
      "      value: 19.2182\n",
      "      name: ROUGE-LSUM\n",
      "      verified: true\n",
      "    - type: loss\n",
      "      value: 2.681241273880005\n",
      "      name: loss\n",
      "      verified: true\n",
      "    - type: gen_len\n",
      "      value: 25.0234\n",
      "      name: gen_len\n",
      "      verified: true\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import ModelCard\n",
    "\n",
    "card = ModelCard.load('google/pegasus-xsum')\n",
    "\n",
    "#Get the textual part of the README.md file of the model \n",
    "print(card.text)\n",
    "\n",
    "#Get the metadata part of README.md file of the model \n",
    "print(card.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` card.data ` : Returns a ModelCardData instance with the model card’s metadata. \n",
    "Call `.to_dict()` on this instance to get the representation as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'en', 'tags': ['summarization'], 'model-index': [{'name': 'google/pegasus-xsum', 'results': [{'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'samsum', 'type': 'samsum', 'config': 'samsum', 'split': 'train'}, 'metrics': [{'type': 'rouge', 'value': 21.8096, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 4.2525, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 17.4469, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 18.8907, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 3.0317161083221436, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 20.3122, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'xsum', 'type': 'xsum', 'config': 'default', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 46.8623, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 24.4533, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 39.0548, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 39.0994, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 1.5717021226882935, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 22.8821, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'cnn_dailymail', 'type': 'cnn_dailymail', 'config': '3.0.0', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 22.2062, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 7.6701, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 15.4046, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 19.2182, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 2.681241273880005, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 25.0234, 'name': 'gen_len', 'verified': True}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "print(card.data.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models\n",
    "\n",
    "Now the next step is how to get the names of the model repositories. After that, it would be nice to figure out a way to get the names of models based on a time period.\n",
    "\n",
    "Let's explore the [list_models](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.list_models) function.\n",
    "\n",
    "We can access relevant information of the model by using the properties of the [ModelInfo](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.hf_api.ModelInfo) Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert/albert-base-v1 2022-03-02 23:29:04+00:00\n",
      "albert/albert-base-v2 2022-03-02 23:29:04+00:00\n",
      "albert/albert-large-v1 2022-03-02 23:29:04+00:00\n",
      "albert/albert-large-v2 2022-03-02 23:29:04+00:00\n",
      "albert/albert-xlarge-v1 2022-03-02 23:29:04+00:00\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# List all models\n",
    "model_list = api.list_models()\n",
    "\n",
    "i = 5\n",
    "\n",
    "for model in model_list:\n",
    "    if(i==0):\n",
    "        break\n",
    "    i-=1\n",
    "    print(model.id ,model.created_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order in the lists\n",
    "\n",
    "After around 30000 of the initial models, different dates start to appear, and they look to be orderd in an accedent pattern.\n",
    "The initial date appears to be 2022-03-02 23:29:04+00:00, and then another batch with 2022-03-02 23:29:05+00:00 appears as well.\n",
    "Retreiving the general information from each model seems to be fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{datetime.datetime(2022, 3, 2, 23, 29, 4, tzinfo=datetime.timezone.utc), datetime.datetime(2022, 3, 2, 23, 29, 5, tzinfo=datetime.timezone.utc)}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "model_list = api.list_models()\n",
    "i = 30000\n",
    "unique_dates = set()\n",
    "for model in model_list:\n",
    "    if(i==0):\n",
    "        break\n",
    "    i-=1\n",
    "    unique_dates.add(model.created_at)\n",
    "\n",
    "print(unique_dates)\n",
    "print(len(unique_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892\n",
      "['2022-03-09 11:52:08+00:00', '2022-03-09 13:30:22+00:00', '2022-03-03 15:53:14+00:00', '2022-03-04 12:49:07+00:00', '2022-03-07 00:22:21+00:00', '2022-03-08 14:22:06+00:00', '2022-03-03 04:53:15+00:00', '2022-03-06 20:35:13+00:00', '2022-03-05 20:52:57+00:00', '2022-03-07 09:51:17+00:00']\n"
     ]
    }
   ],
   "source": [
    "model_list = api.list_models()\n",
    "i = 31000\n",
    "unique_dates = set()\n",
    "for model in model_list:\n",
    "    if(i==0):\n",
    "        break\n",
    "    i-=1\n",
    "    unique_dates.add(model.created_at)\n",
    "\n",
    "print(len(unique_dates))\n",
    "last_10_dates = list(unique_dates)[-10:]\n",
    "print([str(x) for x in last_10_dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the methods\n",
    "\n",
    "We can apply the knowledge of the model cards to the list of model information we got in the last section.\n",
    "\n",
    "The `ModelInfo` object returned by `list_models()` has the following restrictions:\n",
    "\n",
    "    Most attributes of this class are optional. This is because the data returned by the Hub depends on the query made. In general, the more specific the query, the more information is returned. On the contrary, when listing models using list_models() only a subset of the attributes are returned.\n",
    "\n",
    "The above means that, the data from the model card is not brought when the `list_models()` method is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = api.list_models()\n",
    "i = 3\n",
    "found_metadata_list = []\n",
    "\n",
    "for model in model_list:\n",
    "    if(i==0):\n",
    "        break\n",
    "    if(model.card_data!=None):\n",
    "        found_metadata_list.append(model)\n",
    "        print(model.id)\n",
    "        print(model.card_data)\n",
    "        i-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to use the ModelCard.load() function to bring the metadata information.\n",
    "Here an error occurs where, if the model does not have a model card, the function throws the following:\n",
    "\n",
    "    EntryNotFoundError: 404 Client Error. (Request ID: Root=1-65bab81f-55f2b8de5181a4be4982085b;0edd9b55-c840-4d40-a05e-d03930f3addf)\n",
    "\n",
    "    Entry Not Found for url: https://huggingface.co/zhuqing/bert-base-uncased-mumsnet-all-0/resolve/main/README.md.\n",
    "\n",
    "You need to import the error from the HF utils libraries and handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████| 1.83k/1.83k [00:00<00:00, 6.28MB/s]\n",
      "/home/vscode/.local/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ziedsb19/tunbert_zied\n",
      "{'language': 'en', 'tags': ['summarization'], 'model-index': [{'name': 'google/pegasus-xsum', 'results': [{'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'samsum', 'type': 'samsum', 'config': 'samsum', 'split': 'train'}, 'metrics': [{'type': 'rouge', 'value': 21.8096, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 4.2525, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 17.4469, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 18.8907, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 3.0317161083221436, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 20.3122, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'xsum', 'type': 'xsum', 'config': 'default', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 46.8623, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 24.4533, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 39.0548, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 39.0994, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 1.5717021226882935, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 22.8821, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'cnn_dailymail', 'type': 'cnn_dailymail', 'config': '3.0.0', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 22.2062, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 7.6701, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 15.4046, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 19.2182, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 2.681241273880005, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 25.0234, 'name': 'gen_len', 'verified': True}]}]}]}\n",
      "Error loading model card\n",
      "Error loading model card\n",
      "Error loading model card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████| 57.0/57.0 [00:00<00:00, 191kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zinary/DialoGPT-small-rick-new\n",
      "{'language': 'en', 'tags': ['summarization'], 'model-index': [{'name': 'google/pegasus-xsum', 'results': [{'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'samsum', 'type': 'samsum', 'config': 'samsum', 'split': 'train'}, 'metrics': [{'type': 'rouge', 'value': 21.8096, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 4.2525, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 17.4469, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 18.8907, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 3.0317161083221436, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 20.3122, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'xsum', 'type': 'xsum', 'config': 'default', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 46.8623, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 24.4533, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 39.0548, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 39.0994, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 1.5717021226882935, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 22.8821, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'cnn_dailymail', 'type': 'cnn_dailymail', 'config': '3.0.0', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 22.2062, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 7.6701, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 15.4046, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 19.2182, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 2.681241273880005, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 25.0234, 'name': 'gen_len', 'verified': True}]}]}]}\n",
      "Error loading model card\n",
      "Error loading model card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████| 31.0/31.0 [00:00<00:00, 73.0kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ziqingyang/XLMRobertaBaseForXNLI-en\n",
      "{'language': 'en', 'tags': ['summarization'], 'model-index': [{'name': 'google/pegasus-xsum', 'results': [{'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'samsum', 'type': 'samsum', 'config': 'samsum', 'split': 'train'}, 'metrics': [{'type': 'rouge', 'value': 21.8096, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 4.2525, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 17.4469, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 18.8907, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 3.0317161083221436, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 20.3122, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'xsum', 'type': 'xsum', 'config': 'default', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 46.8623, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 24.4533, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 39.0548, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 39.0994, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 1.5717021226882935, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 22.8821, 'name': 'gen_len', 'verified': True}]}, {'task': {'type': 'summarization', 'name': 'Summarization'}, 'dataset': {'name': 'cnn_dailymail', 'type': 'cnn_dailymail', 'config': '3.0.0', 'split': 'test'}, 'metrics': [{'type': 'rouge', 'value': 22.2062, 'name': 'ROUGE-1', 'verified': True}, {'type': 'rouge', 'value': 7.6701, 'name': 'ROUGE-2', 'verified': True}, {'type': 'rouge', 'value': 15.4046, 'name': 'ROUGE-L', 'verified': True}, {'type': 'rouge', 'value': 19.2182, 'name': 'ROUGE-LSUM', 'verified': True}, {'type': 'loss', 'value': 2.681241273880005, 'name': 'loss', 'verified': True}, {'type': 'gen_len', 'value': 25.0234, 'name': 'gen_len', 'verified': True}]}]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub.utils import EntryNotFoundError\n",
    "\n",
    "model_list = api.list_models()\n",
    "i = 3\n",
    "start = 30000\n",
    "found_metadata_list = []\n",
    "\n",
    "for model in model_list:\n",
    "    if(start>0):\n",
    "        start-=1\n",
    "        continue\n",
    "    \n",
    "    if(i==0):\n",
    "        break\n",
    "    \n",
    "    model_card_data = None\n",
    "    try:\n",
    "        model_card_data = ModelCard.load(model.id)\n",
    "    except EntryNotFoundError:\n",
    "        print(f\"Error loading model card\")\n",
    "    \n",
    "    if(model_card_data!=None):\n",
    "        print()\n",
    "        print(model.id)\n",
    "        print(card.data.to_dict())\n",
    "        i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EntryNotFoundError' from 'huggingface_hub' (/home/vscode/.local/lib/python3.11/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntryNotFoundError\n",
      "\u001b[1;32m      3\u001b[0m model_list \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mlist_models()\n",
      "\u001b[1;32m      4\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'EntryNotFoundError' from 'huggingface_hub' (/home/vscode/.local/lib/python3.11/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import EntryNotFoundError\n",
    "\n",
    "model_list = api.list_models()\n",
    "i = 3\n",
    "start = 30000\n",
    "found_metadata_list = []\n",
    "\n",
    "for model in model_list:\n",
    "    if(start>0):\n",
    "        start-=1\n",
    "        continue\n",
    "    \n",
    "    if(i==0):\n",
    "        break\n",
    "    \n",
    "    model_card_data = None\n",
    "    try:\n",
    "        model_card_data = ModelCard.load(model.id)\n",
    "    except HTTPError:\n",
    "        print(f\"Error loading model card\")\n",
    "    \n",
    "    if(model_card_data!=None):\n",
    "        print()\n",
    "        print(model.id)\n",
    "        print(card.data.to_dict())\n",
    "        i-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster downloads\n",
    "If you are running on a machine with high bandwidth, you can increase your download speed with [hf_transfer](https://github.com/huggingface/hf_transfer), a Rust-based library developed to speed up file transfers with the Hub. To enable it:\n",
    "\n",
    "- Specify the hf_transfer extra when installing huggingface_hub (e.g. pip install huggingface_hub[hf_transfer]).\n",
    "- Set HF_HUB_ENABLE_HF_TRANSFER=1 as an environment variable.\n",
    "\n",
    "Test will have to be made to actually determine how worth it it is to use this feature, given that the README.md files weight between 3KB to 10KB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
