@prefix ns1: <https://w3id.org/codemeta/> .
@prefix ns2: <http://w3id.org/fair4ml/> .
@prefix ns3: <http://mlcommons.org/croissant/> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix schema: <https://schema.org/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<http://mlentory.zbmed.de/mlentory_graph/017f19456e7a37fe3c1ad51165020b62cc12497d5d20651e073e3520a5099eb9> a schema:DefinedTerm ;
    schema:description "Answers questions based on tabular data."^^xsd:string ;
    schema:name "Table Question Answering"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/02ddc4a56bdf9cea710d08eea8b6e5605737db4db25ae48b044d427b753d6665> a ns2:ML_Model ;
    ns2:ethicalLegalSocial """Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Limitations: Limitations:
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
Limitations - List: - This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections"""^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the model and its derivatives is not specified in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model and its derivatives may not be used"^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/97b8857097478874bd8e9cb141256c1acde4789374ce4d72be5580b0da08f403> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/black-forest-labs/FLUX.1-dev> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-07-31T21:13:44+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-08-16T14:38:19+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-07-31T21:13:44+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- en
license: other
license_name: flux-1-dev-non-commercial-license
license_link: LICENSE.md
extra_gated_prompt: By clicking "Agree", you agree to the [FluxDev Non-Commercial
  License Agreement](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)
  and acknowledge the [Acceptable Use Policy](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/POLICY.md).
tags:
- text-to-image
- image-generation
- flux
---

![FLUX.1 [dev] Grid](./dev_grid.jpg)

`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.
For more information, please read our [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/).

# Key Features
1. Cutting-edge output quality, second only to our state-of-the-art model `FLUX.1 [pro]`.
2. Competitive prompt following, matching the performance of closed source alternatives .
3. Trained using guidance distillation, making `FLUX.1 [dev]` more efficient.
4. Open weights to drive new scientific research, and empower artists to develop innovative workflows.
5. Generated outputs can be used for personal, scientific, and commercial purposes as described in the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).

# Usage
We provide a reference implementation of `FLUX.1 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux).
Developers and creatives looking to build on top of `FLUX.1 [dev]` are encouraged to use this as a starting point.

## API Endpoints
The FLUX.1 models are also available via API from the following sources
- [bfl.ml](https://docs.bfl.ml/) (currently `FLUX.1 [pro]`)
- [replicate.com](https://replicate.com/collections/flux)
- [fal.ai](https://fal.ai/models/fal-ai/flux/dev)
- [mystic.ai](https://www.mystic.ai/black-forest-labs/flux1-dev)

## ComfyUI
`FLUX.1 [dev]` is also available in [Comfy UI](https://github.com/comfyanonymous/ComfyUI) for local inference with a node-based workflow.

## Diffusers

To use `FLUX.1 [dev]` with the 🧨 diffusers python library, first install or upgrade diffusers

```shell
pip install -U diffusers
```

Then you can use `FluxPipeline` to run the model

```python
import torch
from diffusers import FluxPipeline

pipe = FluxPipeline.from_pretrained("black-forest-labs/FLUX.1-dev", torch_dtype=torch.bfloat16)
pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power

prompt = "A cat holding a sign that says hello world"
image = pipe(
    prompt,
    height=1024,
    width=1024,
    guidance_scale=3.5,
    num_inference_steps=50,
    max_sequence_length=512,
    generator=torch.Generator("cpu").manual_seed(0)
).images[0]
image.save("flux-dev.png")
```

To learn more check out the [diffusers](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux) documentation

---
# Limitations
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.

# Out-of-Scope Use
The model and its derivatives may not be used

- In any way that violates any applicable national, federal, state, local or international law or regulation.
- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.
- To generate or disseminate verifiably false information and/or content with the purpose of harming others.
- To generate or disseminate personal identifiable information that can be used to harm an individual.
- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.
- To create non-consensual nudity or illegal pornographic content.
- For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.
- Generating or facilitating large-scale disinformation campaigns.

# License
This model falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)."""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/black-forest-labs/FLUX.1-dev/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/black-forest-labs/FLUX.1-dev"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/03a8010a33b7e6718a7b9231b9c21ef9f07a99b444199c8533c744e5ddf1b77a>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/c7504a968f417dfe1555958db6caabff413bf4f321bbb9c477d4ff3a9352a0e4>,
        <http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9>,
        <http://mlentory.zbmed.de/mlentory_graph/f57740f97820ddb55f90898c07a60a736e9a9030d5eac125af5c595ad0638c8f>,
        "diffusers:FluxPipeline"^^xsd:string,
        "en"^^xsd:string,
        "license:other"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "other"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the FLUX.1 models are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- en
license: other
license_name: flux-1-dev-non-commercial-license
license_link: LICENSE.md
extra_gated_prompt: By clicking "Agree", you agree to the [FluxDev Non-Commercial
  License Agreement](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)
  and acknowledge the [Acceptable Use Policy](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/POLICY.md).
tags:
- text-to-image
- image-generation
- flux
---

![FLUX.1 [dev] Grid](./dev_grid.jpg)

`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.
For more information, please read our [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/).

# Key Features
1. Cutting-edge output quality, second only to our state-of-the-art model `FLUX.1 [pro]`.
2. Competitive prompt following, matching the performance of closed source alternatives .
3. Trained using guidance distillation, making `FLUX.1 [dev]` more efficient.
4. Open weights to drive new scientific research, and empower artists to develop innovative workflows.
5. Generated outputs can be used for personal, scientific, and commercial purposes as described in the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).

# Usage
We provide a reference implementation of `FLUX.1 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux).
Developers and creatives looking to build on top of `FLUX.1 [dev]` are encouraged to use this as a starting point.

## API Endpoints
The FLUX.1 models are also available via API from the following sources
- [bfl.ml](https://docs.bfl.ml/) (currently `FLUX.1 [pro]`)
- [replicate.com](https://replicate.com/collections/flux)
- [fal.ai](https://fal.ai/models/fal-ai/flux/dev)
- [mystic.ai](https://www.mystic.ai/black-forest-labs/flux1-dev)

## ComfyUI
`FLUX.1 [dev]` is also available in [Comfy UI](https://github.com/comfyanonymous/ComfyUI) for local inference with a node-based workflow.

## Diffusers

To use `FLUX.1 [dev]` with the 🧨 diffusers python library, first install or upgrade diffusers

```shell
pip install -U diffusers
```

Then you can use `FluxPipeline` to run the model

```python
import torch
from diffusers import FluxPipeline

pipe = FluxPipeline.from_pretrained("black-forest-labs/FLUX.1-dev", torch_dtype=torch.bfloat16)
pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power

prompt = "A cat holding a sign that says hello world"
image = pipe(
    prompt,
    height=1024,
    width=1024,
    guidance_scale=3.5,
    num_inference_steps=50,
    max_sequence_length=512,
    generator=torch.Generator("cpu").manual_seed(0)
).images[0]
image.save("flux-dev.png")
```

To learn more check out the [diffusers](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux) documentation

---
# Limitations
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.

# Out-of-Scope Use
The model and its derivatives may not be used

- In any way that violates any applicable national, federal, state, local or international law or regulation.
- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.
- To generate or disseminate verifiably false information and/or content with the purpose of harming others.
- To generate or disseminate personal identifiable information that can be used to harm an individual.
- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.
- To create non-consensual nudity or illegal pornographic content.
- For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.
- Generating or facilitating large-scale disinformation campaigns.

# License
This model falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)."""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/black-forest-labs/FLUX.1-dev> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/black-forest-labs/FLUX.1-dev/discussions> ;
    ns1:readme <https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/065802aa405dd5caa9286a5a6e0267edf6732a1bcd0951678d1981262d67d805> a schema:Organization .

<http://mlentory.zbmed.de/mlentory_graph/091fb189582881dd287806cb48e7e20f2caa92bd980601e911f4a2a101787f04> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found"^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/812d8303ed5720ece9efd43abaf31605194f7c778b2e76978276fbdaaa6d41be> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the model is for text generation, specifically for generating text based on user's tweets."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/f64e9e2107d219122c90233f1c6d719235a80c4f8a73e0cda62a43c1b4d56c70> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/812d8303ed5720ece9efd43abaf31605194f7c778b2e76978276fbdaaa6d41be> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/812d8303ed5720ece9efd43abaf31605194f7c778b2e76978276fbdaaa6d41be> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/huggingartists/fascinoma> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found"^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "The information provided does not contain the name of the contributor in the given context."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2022-03-02T23:29:05+00:00"^^xsd:dateTime ;
    schema:dateModified "2021-08-04T07:45:42+00:00"^^xsd:dateTime ;
    schema:datePublished "2022-03-02T23:29:05+00:00"^^xsd:dateTime ;
    schema:description """---
language: en
datasets:
- huggingartists/fascinoma
tags:
- huggingartists
- lyrics
- lm-head
- causal-lm
widget:
- text: "I am"
---

<div class="inline-flex flex-col" style="line-height: 1.5;">
    <div class="flex">
        <div
			style="display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;https://assets.genius.com/images/default_avatar_300.png?1627659427&#39;)">
        </div>
    </div>
    <div style="text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800">🤖 HuggingArtists Model 🤖</div>
    <div style="text-align: center; font-size: 16px; font-weight: 800">Fascinoma</div>
    <a href="https://genius.com/artists/fascinoma">
    	<div style="text-align: center; font-size: 14px;">@fascinoma</div>
    </a>
</div>

I was made with [huggingartists](https://github.com/AlekseyKorshuk/huggingartists).

Create your own bot based on your favorite artist with [the demo](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)!

## How does it work?

To understand how the model was developed, check the [W&B report](https://wandb.ai/huggingartists/huggingartists/reportlist).

## Training data

The model was trained on lyrics from Fascinoma.

Dataset is available [here](https://huggingface.co/datasets/huggingartists/fascinoma).
And can be used with:

```python
from datasets import load_dataset

dataset = load_dataset("huggingartists/fascinoma")
```

Or with Transformers library:

```python
from transformers import AutoTokenizer, AutoModelWithLMHead
  
tokenizer = AutoTokenizer.from_pretrained("huggingartists/fascinoma")

model = AutoModelWithLMHead.from_pretrained("huggingartists/fascinoma")
```

[Explore the data](https://wandb.ai/huggingartists/huggingartists/runs/za989b3u/artifacts), which is tracked with [W&B artifacts](https://docs.wandb.com/artifacts) at every step of the pipeline.

## Training procedure

The model is based on a pre-trained [GPT-2](https://huggingface.co/gpt2) which is fine-tuned on Fascinoma's lyrics.

Hyperparameters and metrics are recorded in the [W&B training run](https://wandb.ai/huggingartists/huggingartists/runs/kklye04t) for full transparency and reproducibility.

At the end of training, [the final model](https://wandb.ai/huggingartists/huggingartists/runs/kklye04t/artifacts) is logged and versioned.

## How to use

You can use this model directly with a pipeline for text generation:

```python
from transformers import pipeline
generator = pipeline('text-generation',
                     model='huggingartists/fascinoma')
generator("I am", num_return_sequences=5)
```

## Limitations and bias

The model suffers from [the same limitations and bias as GPT-2](https://huggingface.co/gpt2#limitations-and-bias).

In addition, the data present in the user's tweets further affects the text generated by the model.

## About

*Built by Aleksey Korshuk*

[![Follow](https://img.shields.io/github/followers/AlekseyKorshuk?style=social)](https://github.com/AlekseyKorshuk)

For more details, visit the project repository.

[![GitHub stars](https://img.shields.io/github/stars/AlekseyKorshuk/huggingartists?style=social)](https://github.com/AlekseyKorshuk/huggingartists)
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/huggingartists/fascinoma/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/huggingartists/fascinoma"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/3cf1ab8574d81c00233c30bd64e2562c80c2c4b9ba8c824b6f4e12904b13eac0>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/9029477f3163928162d1e889607fce71fdf85631fdee3951bb37114ce0203c57>,
        <http://mlentory.zbmed.de/mlentory_graph/9b3d9b28e7c2070531e1d563836c54690a539654c89cf38e6a27635202a9f512>,
        <http://mlentory.zbmed.de/mlentory_graph/9c51ba772888465129d6bda3b0b520a3a16960b8c108a8cd2276eb4fb85ee5f2>,
        <http://mlentory.zbmed.de/mlentory_graph/c2cc99051d7ed76ff3bce040b0417e9769c6200e327ebd61cfddefbb7b100262>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/e812918f10f81efd761fb2c9c424c9f4d524482d4df92c83016dbd98b6dbf69e>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "dataset:huggingartists/fascinoma"^^xsd:string,
        "en"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The context provided does not contain any information about the processor requirements for the application."^^xsd:string ;
    schema:releaseNotes """---
language: en
datasets:
- huggingartists/fascinoma
tags:
- huggingartists
- lyrics
- lm-head
- causal-lm
widget:
- text: "I am"
---

<div class="inline-flex flex-col" style="line-height: 1.5;">
    <div class="flex">
        <div
			style="display:DISPLAY_1; margin-left: auto; margin-right: auto; width: 92px; height:92px; border-radius: 50%; background-size: cover; background-image: url(&#39;https://assets.genius.com/images/default_avatar_300.png?1627659427&#39;)">
        </div>
    </div>
    <div style="text-align: center; margin-top: 3px; font-size: 16px; font-weight: 800">🤖 HuggingArtists Model 🤖</div>
    <div style="text-align: center; font-size: 16px; font-weight: 800">Fascinoma</div>
    <a href="https://genius.com/artists/fascinoma">
    	<div style="text-align: center; font-size: 14px;">@fascinoma</div>
    </a>
</div>

I was made with [huggingartists](https://github.com/AlekseyKorshuk/huggingartists).

Create your own bot based on your favorite artist with [the demo](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb)!

## How does it work?

To understand how the model was developed, check the [W&B report](https://wandb.ai/huggingartists/huggingartists/reportlist).

## Training data

The model was trained on lyrics from Fascinoma.

Dataset is available [here](https://huggingface.co/datasets/huggingartists/fascinoma).
And can be used with:

```python
from datasets import load_dataset

dataset = load_dataset("huggingartists/fascinoma")
```

Or with Transformers library:

```python
from transformers import AutoTokenizer, AutoModelWithLMHead
  
tokenizer = AutoTokenizer.from_pretrained("huggingartists/fascinoma")

model = AutoModelWithLMHead.from_pretrained("huggingartists/fascinoma")
```

[Explore the data](https://wandb.ai/huggingartists/huggingartists/runs/za989b3u/artifacts), which is tracked with [W&B artifacts](https://docs.wandb.com/artifacts) at every step of the pipeline.

## Training procedure

The model is based on a pre-trained [GPT-2](https://huggingface.co/gpt2) which is fine-tuned on Fascinoma's lyrics.

Hyperparameters and metrics are recorded in the [W&B training run](https://wandb.ai/huggingartists/huggingartists/runs/kklye04t) for full transparency and reproducibility.

At the end of training, [the final model](https://wandb.ai/huggingartists/huggingartists/runs/kklye04t/artifacts) is logged and versioned.

## How to use

You can use this model directly with a pipeline for text generation:

```python
from transformers import pipeline
generator = pipeline('text-generation',
                     model='huggingartists/fascinoma')
generator("I am", num_return_sequences=5)
```

## Limitations and bias

The model suffers from [the same limitations and bias as GPT-2](https://huggingface.co/gpt2#limitations-and-bias).

In addition, the data present in the user's tweets further affects the text generated by the model.

## About

*Built by Aleksey Korshuk*

[![Follow](https://img.shields.io/github/followers/AlekseyKorshuk?style=social)](https://github.com/AlekseyKorshuk)

For more details, visit the project repository.

[![GitHub stars](https://img.shields.io/github/stars/AlekseyKorshuk/huggingartists?style=social)](https://github.com/AlekseyKorshuk/huggingartists)
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/huggingartists/fascinoma> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/huggingartists/fascinoma/discussions> ;
    ns1:readme <https://huggingface.co/huggingartists/fascinoma/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/0bfdbeb5ecc4b734a83b3ab57917acf06cd48a0060a2dbfdf5f54ba2a1cfd167> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "The context provided does not contain any information about the ethical, legal, or social aspects of the model. Therefore, the exact phrase \"Information not found\" should be used as the response."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/151c5c18796e684c7f0c77ded700ed4efddb5eb060b8924db2428896c9cebd7c> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the model is to be made aware of the risks, biases, and limitations of the model."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/a464d85abf29c8b2f8dc299e9fe390a547c8450e42b7ab0ccaac88fdcaeaaa84> ;
    ns2:modelCategory "Information not found"^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/4407cda96af0afeaf4c1c5aaacc2cddb4eccb07bfeb1a5dded4a788a19846707> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/151c5c18796e684c7f0c77ded700ed4efddb5eb060b8924db2428896c9cebd7c> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/151c5c18796e684c7f0c77ded700ed4efddb5eb060b8924db2428896c9cebd7c> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/distilbert/distilbert-base-multilingual-cased> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-05-06T13:46:54+00:00"^^xsd:dateTime ;
    schema:datePublished "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- multilingual
- af
- sq
- ar
- an
- hy
- ast
- az
- ba
- eu
- bar
- be
- bn
- inc
- bs
- br
- bg
- my
- ca
- ceb
- ce
- zh
- cv
- hr
- cs
- da
- nl
- en
- et
- fi
- fr
- gl
- ka
- de
- el
- gu
- ht
- he
- hi
- hu
- is
- io
- id
- ga
- it
- ja
- jv
- kn
- kk
- ky
- ko
- la
- lv
- lt
- roa
- nds
- lm
- mk
- mg
- ms
- ml
- mr
- mn
- min
- ne
- new
- nb
- nn
- oc
- fa
- pms
- pl
- pt
- pa
- ro
- ru
- sco
- sr
- hr
- scn
- sk
- sl
- aze
- es
- su
- sw
- sv
- tl
- tg
- th
- ta
- tt
- te
- tr
- uk
- ud
- uz
- vi
- vo
- war
- cy
- fry
- pnb
- yo
license: apache-2.0
datasets:
- wikipedia
---

# Model Card for DistilBERT base multilingual (cased)

# Table of Contents

1. [Model Details](#model-details)
2. [Uses](#uses)
3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)
4. [Training Details](#training-details)
5. [Evaluation](#evaluation)
6. [Environmental Impact](#environmental-impact)
7. [Citation](#citation)
8. [How To Get Started With the Model](#how-to-get-started-with-the-model)

# Model Details

## Model Description

This model is a distilled version of the [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased/). The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is cased: it does make a difference between english and English.

The model is trained on the concatenation of Wikipedia in 104 different languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).
The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).
On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.

We encourage potential users of this model to check out the [BERT base multilingual model card](https://huggingface.co/bert-base-multilingual-cased) to learn more about usage, limitations and potential biases.

- **Developed by:** Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face)
- **Model type:** Transformer-based language model
- **Language(s) (NLP):** 104 languages; see full list [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)
- **License:** Apache 2.0
- **Related Models:** [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased)
- **Resources for more information:** 
  - [GitHub Repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)
  - [Associated Paper](https://arxiv.org/abs/1910.01108)

# Uses

## Direct Use and Downstream Use

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.

## Out of Scope Use

The model should not be used to intentionally create hostile or alienating environments for people. The model was not trained to be factual or true representations of people or events, and therefore using the models to generate such content is out-of-scope for the abilities of this model.

# Bias, Risks, and Limitations

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.

## Recommendations

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.

# Training Details

- The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages
- The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.
- Further information about the training procedure and data is included in the [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) model card.

# Evaluation

The model developers report the following accuracy results for DistilmBERT (see [GitHub Repo](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)): 

> Here are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):

| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |
| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|
| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |
| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |
| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |

# Environmental Impact

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** More information needed
- **Hours used:** More information needed
- **Cloud Provider:** More information needed
- **Compute Region:** More information needed
- **Carbon Emitted:** More information needed

# Citation

```bibtex
@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}
```

APA
- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

# How to Get Started With the Model

You can use the model directly with a pipeline for masked language modeling: 

```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')
>>> unmasker("Hello I'm a [MASK] model.")

[{'score': 0.040800247341394424,
  'sequence': "Hello I'm a virtual model.",
  'token': 37859,
  'token_str': 'virtual'},
 {'score': 0.020015988498926163,
  'sequence': "Hello I'm a big model.",
  'token': 22185,
  'token_str': 'big'},
 {'score': 0.018680453300476074,
  'sequence': "Hello I'm a Hello model.",
  'token': 31178,
  'token_str': 'Hello'},
 {'score': 0.017396586015820503,
  'sequence': "Hello I'm a model model.",
  'token': 13192,
  'token_str': 'model'},
 {'score': 0.014229810796678066,
  'sequence': "Hello I'm a perfect model.",
  'token': 43477,
  'token_str': 'perfect'}]
```
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/distilbert/distilbert-base-multilingual-cased/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/distilbert/distilbert-base-multilingual-cased"^^xsd:string ;
    schema:inLanguage "af"^^xsd:string,
        "ar"^^xsd:string,
        "az"^^xsd:string,
        "ba"^^xsd:string,
        "be"^^xsd:string,
        "bg"^^xsd:string,
        "bn"^^xsd:string,
        "br"^^xsd:string,
        "bs"^^xsd:string,
        "ca"^^xsd:string,
        "cs"^^xsd:string,
        "cy"^^xsd:string,
        "da"^^xsd:string,
        "de"^^xsd:string,
        "el"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "et"^^xsd:string,
        "eu"^^xsd:string,
        "fa"^^xsd:string,
        "fi"^^xsd:string,
        "fr"^^xsd:string,
        "gl"^^xsd:string,
        "gu"^^xsd:string,
        "he"^^xsd:string,
        "hi"^^xsd:string,
        "hr"^^xsd:string,
        "ht"^^xsd:string,
        "hu"^^xsd:string,
        "hy"^^xsd:string,
        "id"^^xsd:string,
        "is"^^xsd:string,
        "it"^^xsd:string,
        "ja"^^xsd:string,
        "ka"^^xsd:string,
        "kk"^^xsd:string,
        "kn"^^xsd:string,
        "ko"^^xsd:string,
        "la"^^xsd:string,
        "lt"^^xsd:string,
        "lv"^^xsd:string,
        "mg"^^xsd:string,
        "mk"^^xsd:string,
        "ml"^^xsd:string,
        "mn"^^xsd:string,
        "mr"^^xsd:string,
        "ms"^^xsd:string,
        "my"^^xsd:string,
        "ne"^^xsd:string,
        "nl"^^xsd:string,
        "nn"^^xsd:string,
        "oc"^^xsd:string,
        "pa"^^xsd:string,
        "pl"^^xsd:string,
        "pt"^^xsd:string,
        "ro"^^xsd:string,
        "ru"^^xsd:string,
        "sk"^^xsd:string,
        "sl"^^xsd:string,
        "sq"^^xsd:string,
        "sr"^^xsd:string,
        "su"^^xsd:string,
        "sv"^^xsd:string,
        "sw"^^xsd:string,
        "ta"^^xsd:string,
        "te"^^xsd:string,
        "tg"^^xsd:string,
        "th"^^xsd:string,
        "tl"^^xsd:string,
        "tr"^^xsd:string,
        "tt"^^xsd:string,
        "uk"^^xsd:string,
        "uz"^^xsd:string,
        "vi"^^xsd:string,
        "yo"^^xsd:string,
        "zh"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/02d5fe1af629b32020e3f74d866aa66a5001c17461f60e2ed7bb4a7224a40870>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1aa60aa9719223ac8750af1366d4a56166214022d0d069d1412225070d1275b9>,
        <http://mlentory.zbmed.de/mlentory_graph/2f6977083cec992fd87ac4e63effc5723d1af4356346054007b916659b276565>,
        <http://mlentory.zbmed.de/mlentory_graph/309322bdc7e7441b9f4608224edba0ae93e18d4f079f4721e80b9c830a543e9a>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/4b6121cea7734a3579dc04999a3b0a03ea19f5392b2dc776f08f69fe60adadd6>,
        <http://mlentory.zbmed.de/mlentory_graph/4e5443728f51b2323bf3a1c4a6a6d8612484cbca0a8b0beeb5fb0258d1df5196>,
        <http://mlentory.zbmed.de/mlentory_graph/5753fbc4186ff7da2a349a9e6c953346d81338871b3af41695d0e0707dee467e>,
        <http://mlentory.zbmed.de/mlentory_graph/5e2174aea879a4806552b465a7caa98d3f94a7296126ee9f011b524911ed47cf>,
        <http://mlentory.zbmed.de/mlentory_graph/5f05cc43f75ab65c5a42a72971f2505a9613d2882a1d30b66b318a693985a9da>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/8a7c0f53b968dbfecd34bbb37846ae3e47ed7434b4f28d23160e0df0bb79d5b3>,
        <http://mlentory.zbmed.de/mlentory_graph/a8eae50bf40de8621698ed7e4943c7c010b710e5ab9ac9aae114dfab8527f239>,
        <http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21>,
        <http://mlentory.zbmed.de/mlentory_graph/ae6f8948011de4b50edd72fa4ba450988b135ac2718cd958ee93b06bd34289aa>,
        <http://mlentory.zbmed.de/mlentory_graph/b191c4fd48c1c510469ee9111aee8a38ee126676e8f72937db5b30de066f635a>,
        <http://mlentory.zbmed.de/mlentory_graph/b27ff40fa7a2058157ffd2256f890a419f0528607ed9c66983c96e9f5aa17aa9>,
        <http://mlentory.zbmed.de/mlentory_graph/b3764ab684e398f1cc0ce31596f300e8c8475b38bbf865201bca5e9aa964d8d8>,
        <http://mlentory.zbmed.de/mlentory_graph/d35473e099c177351c833e6dad49614522e57381763a759b112129fe6233ef94>,
        <http://mlentory.zbmed.de/mlentory_graph/d97786edc8d2091920dae92570c4a793d367ed8ec0dff0c869b4d21fcd2e02e2>,
        <http://mlentory.zbmed.de/mlentory_graph/de8e27b7ddf241399ac5fa9721bb05a2ccbcd267141d9c8ccd10f0131ae1559b>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "af"^^xsd:string,
        "an"^^xsd:string,
        "ar"^^xsd:string,
        "arxiv:1910.01108"^^xsd:string,
        "arxiv:1910.09700"^^xsd:string,
        "az"^^xsd:string,
        "ba"^^xsd:string,
        "be"^^xsd:string,
        "bg"^^xsd:string,
        "bn"^^xsd:string,
        "br"^^xsd:string,
        "bs"^^xsd:string,
        "ca"^^xsd:string,
        "ce"^^xsd:string,
        "cs"^^xsd:string,
        "cv"^^xsd:string,
        "cy"^^xsd:string,
        "da"^^xsd:string,
        "dataset:wikipedia"^^xsd:string,
        "de"^^xsd:string,
        "el"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "et"^^xsd:string,
        "eu"^^xsd:string,
        "fa"^^xsd:string,
        "fi"^^xsd:string,
        "fr"^^xsd:string,
        "ga"^^xsd:string,
        "gl"^^xsd:string,
        "gu"^^xsd:string,
        "he"^^xsd:string,
        "hi"^^xsd:string,
        "hr"^^xsd:string,
        "ht"^^xsd:string,
        "hu"^^xsd:string,
        "hy"^^xsd:string,
        "id"^^xsd:string,
        "io"^^xsd:string,
        "is"^^xsd:string,
        "it"^^xsd:string,
        "ja"^^xsd:string,
        "jv"^^xsd:string,
        "ka"^^xsd:string,
        "kk"^^xsd:string,
        "kn"^^xsd:string,
        "ko"^^xsd:string,
        "ky"^^xsd:string,
        "la"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "lm"^^xsd:string,
        "lt"^^xsd:string,
        "lv"^^xsd:string,
        "mg"^^xsd:string,
        "mk"^^xsd:string,
        "ml"^^xsd:string,
        "mn"^^xsd:string,
        "mr"^^xsd:string,
        "ms"^^xsd:string,
        "my"^^xsd:string,
        "nb"^^xsd:string,
        "ne"^^xsd:string,
        "nl"^^xsd:string,
        "nn"^^xsd:string,
        "oc"^^xsd:string,
        "pa"^^xsd:string,
        "pl"^^xsd:string,
        "pt"^^xsd:string,
        "region:us"^^xsd:string,
        "ro"^^xsd:string,
        "ru"^^xsd:string,
        "sk"^^xsd:string,
        "sl"^^xsd:string,
        "sq"^^xsd:string,
        "sr"^^xsd:string,
        "su"^^xsd:string,
        "sv"^^xsd:string,
        "sw"^^xsd:string,
        "ta"^^xsd:string,
        "te"^^xsd:string,
        "tf"^^xsd:string,
        "tg"^^xsd:string,
        "th"^^xsd:string,
        "tl"^^xsd:string,
        "tr"^^xsd:string,
        "tt"^^xsd:string,
        "ud"^^xsd:string,
        "uk"^^xsd:string,
        "uz"^^xsd:string,
        "vi"^^xsd:string,
        "vo"^^xsd:string,
        "yo"^^xsd:string,
        "zh"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- multilingual
- af
- sq
- ar
- an
- hy
- ast
- az
- ba
- eu
- bar
- be
- bn
- inc
- bs
- br
- bg
- my
- ca
- ceb
- ce
- zh
- cv
- hr
- cs
- da
- nl
- en
- et
- fi
- fr
- gl
- ka
- de
- el
- gu
- ht
- he
- hi
- hu
- is
- io
- id
- ga
- it
- ja
- jv
- kn
- kk
- ky
- ko
- la
- lv
- lt
- roa
- nds
- lm
- mk
- mg
- ms
- ml
- mr
- mn
- min
- ne
- new
- nb
- nn
- oc
- fa
- pms
- pl
- pt
- pa
- ro
- ru
- sco
- sr
- hr
- scn
- sk
- sl
- aze
- es
- su
- sw
- sv
- tl
- tg
- th
- ta
- tt
- te
- tr
- uk
- ud
- uz
- vi
- vo
- war
- cy
- fry
- pnb
- yo
license: apache-2.0
datasets:
- wikipedia
---

# Model Card for DistilBERT base multilingual (cased)

# Table of Contents

1. [Model Details](#model-details)
2. [Uses](#uses)
3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)
4. [Training Details](#training-details)
5. [Evaluation](#evaluation)
6. [Environmental Impact](#environmental-impact)
7. [Citation](#citation)
8. [How To Get Started With the Model](#how-to-get-started-with-the-model)

# Model Details

## Model Description

This model is a distilled version of the [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased/). The code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is cased: it does make a difference between english and English.

The model is trained on the concatenation of Wikipedia in 104 different languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).
The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).
On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.

We encourage potential users of this model to check out the [BERT base multilingual model card](https://huggingface.co/bert-base-multilingual-cased) to learn more about usage, limitations and potential biases.

- **Developed by:** Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face)
- **Model type:** Transformer-based language model
- **Language(s) (NLP):** 104 languages; see full list [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)
- **License:** Apache 2.0
- **Related Models:** [BERT base multilingual model](https://huggingface.co/bert-base-multilingual-cased)
- **Resources for more information:** 
  - [GitHub Repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)
  - [Associated Paper](https://arxiv.org/abs/1910.01108)

# Uses

## Direct Use and Downstream Use

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.

## Out of Scope Use

The model should not be used to intentionally create hostile or alienating environments for people. The model was not trained to be factual or true representations of people or events, and therefore using the models to generate such content is out-of-scope for the abilities of this model.

# Bias, Risks, and Limitations

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.

## Recommendations

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.

# Training Details

- The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages
- The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.
- Further information about the training procedure and data is included in the [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) model card.

# Evaluation

The model developers report the following accuracy results for DistilmBERT (see [GitHub Repo](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)): 

> Here are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):

| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |
| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|
| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |
| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |
| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |

# Environmental Impact

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** More information needed
- **Hours used:** More information needed
- **Cloud Provider:** More information needed
- **Compute Region:** More information needed
- **Carbon Emitted:** More information needed

# Citation

```bibtex
@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}
```

APA
- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

# How to Get Started With the Model

You can use the model directly with a pipeline for masked language modeling: 

```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='distilbert-base-multilingual-cased')
>>> unmasker("Hello I'm a [MASK] model.")

[{'score': 0.040800247341394424,
  'sequence': "Hello I'm a virtual model.",
  'token': 37859,
  'token_str': 'virtual'},
 {'score': 0.020015988498926163,
  'sequence': "Hello I'm a big model.",
  'token': 22185,
  'token_str': 'big'},
 {'score': 0.018680453300476074,
  'sequence': "Hello I'm a Hello model.",
  'token': 31178,
  'token_str': 'Hello'},
 {'score': 0.017396586015820503,
  'sequence': "Hello I'm a model model.",
  'token': 13192,
  'token_str': 'model'},
 {'score': 0.014229810796678066,
  'sequence': "Hello I'm a perfect model.",
  'token': 43477,
  'token_str': 'perfect'}]
```
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/distilbert/distilbert-base-multilingual-cased> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found"^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/distilbert/distilbert-base-multilingual-cased/discussions> ;
    ns1:readme <https://huggingface.co/distilbert/distilbert-base-multilingual-cased/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/24d02c4c9af718667adb32f98890ccae3daa567e09e890fbcc6baa38ee130dd0>,
        <http://mlentory.zbmed.de/mlentory_graph/96b96e1fc1370c6454f55ef5bb235e3108a8e1b32665ef0b72dd54ba220b3a53> .

<http://mlentory.zbmed.de/mlentory_graph/10d3f55f1230f749cbd1e521ed5244663048c671dca10cbf1e74df0ca279d64f> a schema:Person .

<http://mlentory.zbmed.de/mlentory_graph/11123d251fd66f42c5867a6f93966ad782deb34f2571cccd7a7dc776b02e0d0b> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/4895f1e40a05c534b7f3237f31f4536bbcbb6cad57b678d08591d76270974380> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/85f0e69a70ce025a206d6ddbf78d989a3cabbdae215f2045df9b67b21a9d6780> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/1e997bed4477ee580a83cb75321b0ee5013e0e193f27b20ac16d78679b4a78a9> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/margati/distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2023-12-19T09:06:41+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-12-19T09:07:00+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-12-19T09:06:41+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model: distilbert-base-multilingual-cased
tags:
- generated_from_keras_callback
model-index:
- name: distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12
  results: []
---

<!-- This model card has been generated automatically according to the information Keras had access to. You should
probably proofread and complete it, then remove this comment. -->

# distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12

This model is a fine-tuned version of [distilbert-base-multilingual-cased](https://huggingface.co/distilbert-base-multilingual-cased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Train Loss: 0.0535
- Validation Loss: 1.3927
- Train Accuracy: 0.6869
- Epoch: 14

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- optimizer: {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': True, 'is_legacy_optimizer': False, 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 6660, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}, 'registered_name': None}, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False}
- training_precision: float32

### Training results

| Train Loss | Validation Loss | Train Accuracy | Epoch |
|:----------:|:---------------:|:--------------:|:-----:|
| 0.6916     | 0.6779          | 0.5859         | 0     |
| 0.6895     | 0.6660          | 0.6162         | 1     |
| 0.6505     | 0.6476          | 0.6566         | 2     |
| 0.5595     | 0.6096          | 0.7374         | 3     |
| 0.4751     | 0.7793          | 0.5960         | 4     |
| 0.3377     | 0.8518          | 0.6768         | 5     |
| 0.2418     | 1.0199          | 0.6465         | 6     |
| 0.1604     | 1.1340          | 0.6667         | 7     |
| 0.1399     | 1.1893          | 0.6465         | 8     |
| 0.1198     | 0.9966          | 0.6465         | 9     |
| 0.0854     | 1.2855          | 0.6768         | 10    |
| 0.0747     | 1.2972          | 0.6566         | 11    |
| 0.0594     | 1.3570          | 0.6970         | 12    |
| 0.0561     | 1.4063          | 0.6566         | 13    |
| 0.0535     | 1.3927          | 0.6869         | 14    |


### Framework versions

- Transformers 4.35.2
- TensorFlow 2.15.0
- Datasets 2.15.0
- Tokenizers 0.15.0
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/margati/distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/margati/distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/7ad8435aa27d409e8692d8f6043742210150fe3cf420b9abaad32ffe8a63b7c7>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/8dda7455295f0a266a228afac693c1d9f5e021304637fea5132533854c579893>,
        <http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "base_model:distilbert/distilbert-base-multilingual-cased"^^xsd:string,
        "base_model:finetune:distilbert/distilbert-base-multilingual-cased"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string,
        "tf"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model: distilbert-base-multilingual-cased
tags:
- generated_from_keras_callback
model-index:
- name: distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12
  results: []
---

<!-- This model card has been generated automatically according to the information Keras had access to. You should
probably proofread and complete it, then remove this comment. -->

# distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12

This model is a fine-tuned version of [distilbert-base-multilingual-cased](https://huggingface.co/distilbert-base-multilingual-cased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Train Loss: 0.0535
- Validation Loss: 1.3927
- Train Accuracy: 0.6869
- Epoch: 14

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- optimizer: {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': True, 'is_legacy_optimizer': False, 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 6660, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}, 'registered_name': None}, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False}
- training_precision: float32

### Training results

| Train Loss | Validation Loss | Train Accuracy | Epoch |
|:----------:|:---------------:|:--------------:|:-----:|
| 0.6916     | 0.6779          | 0.5859         | 0     |
| 0.6895     | 0.6660          | 0.6162         | 1     |
| 0.6505     | 0.6476          | 0.6566         | 2     |
| 0.5595     | 0.6096          | 0.7374         | 3     |
| 0.4751     | 0.7793          | 0.5960         | 4     |
| 0.3377     | 0.8518          | 0.6768         | 5     |
| 0.2418     | 1.0199          | 0.6465         | 6     |
| 0.1604     | 1.1340          | 0.6667         | 7     |
| 0.1399     | 1.1893          | 0.6465         | 8     |
| 0.1198     | 0.9966          | 0.6465         | 9     |
| 0.0854     | 1.2855          | 0.6768         | 10    |
| 0.0747     | 1.2972          | 0.6566         | 11    |
| 0.0594     | 1.3570          | 0.6970         | 12    |
| 0.0561     | 1.4063          | 0.6566         | 13    |
| 0.0535     | 1.3927          | 0.6869         | 14    |


### Framework versions

- Transformers 4.35.2
- TensorFlow 2.15.0
- Datasets 2.15.0
- Tokenizers 0.15.0
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/margati/distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/margati/distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12/discussions> ;
    ns1:readme <https://huggingface.co/margati/distilbert_base_multilingual_cased_ru_action_min_chunks_works_19_12/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/119bd2aa6d51fb076ddf89b0c5c2536f1550ff5bf549b11a6c834340b0979a14> a schema:DefinedTerm ;
    schema:description "Library for span-based named entity recognition using transformer models."^^xsd:string ;
    schema:name "SpanMarker"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/12855f71ca9ab0f85c6609885b9e912174c81e608da47e6448d968f757f5ab44> a schema:DefinedTerm ;
    schema:description "Library for efficient text classification and word representation learning."^^xsd:string ;
    schema:name "fastText"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/130c4e79e13cad2f6e545cf38ded77772d678341efa1b42e8414e3d3ac4ba7b6> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp is not explicitly stated in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category in the given text is \"speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp\"."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/e84176b0758aa4e827b29420a9d219dd2ca6c52f17244c62fb550d73c263a33a> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions """To get started with the model, follow these steps:

1. Install the necessary dependencies and set up the environment.
2. Load the model and tokenizer.
3. Prepare the input data for the model.
4. Run the model inference and obtain the output.
5. Interpret the output and use it for the desired task.

Uses > Direct Use:

The model can be used directly for various tasks such as text generation, question answering, and summarization. To use the model for a specific task, follow these steps:

1. Prepare the input data for the task.
2. Run the model inference and obtain the output.
3. Interpret the output and use it for the desired task.

Information not found."""^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-01-12T07:12:38+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-01-12T07:17:41+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-01-12T07:12:38+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
tags:
- merge
- mergekit
- mistral
- 7b
- lazymergekit
- mistralai/Mistral-7B-Instruct-v0.2
- uukuguy/speechless-mistral-six-in-one-7b-orth-1.0
---

# speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp

speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp is a merge of the following models:
* [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
* [uukuguy/speechless-mistral-six-in-one-7b-orth-1.0](https://huggingface.co/uukuguy/speechless-mistral-six-in-one-7b-orth-1.0)

## 🧩 Configuration

```yaml
slices:
  - sources:
      - model: mistralai/Mistral-7B-Instruct-v0.2
        layer_range: [0, 32]
      - model: uukuguy/speechless-mistral-six-in-one-7b-orth-1.0
        layer_range: [0, 32]
merge_method: slerp
base_model: mistralai/Mistral-7B-Instruct-v0.2
parameters:
  t:
    - filter: self_attn
      value: [0, 0.5, 0.3, 0.7, 1]
    - filter: mlp
      value: [1, 0.5, 0.7, 0.3, 0]
    - value: 0.5
dtype: bfloat16
```


## 💻 Usage


```python
!pip install -qU transformers accelerate

from transformers import AutoTokenizer
import transformers
import torch

model = "MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp"
messages = [{"role": "user", "content": "What is a large language model?"}]

tokenizer = AutoTokenizer.from_pretrained(model)
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
pipeline = transformers.pipeline(
"text-generation",
model=model,
torch_dtype=torch.float16,
device_map="auto",
)

outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/93108d142ec8aa4540d03f40aa1cbc3eb5d79368400a280197fb230643230e00>,
        <http://mlentory.zbmed.de/mlentory_graph/a4045d1920b7eb5463ba285d7a39bd0751bf81195ee8a40f0e2d0607458bc0fe>,
        <http://mlentory.zbmed.de/mlentory_graph/b0c80490465c422162366747d4bb193299977b51cacddfa724ffc1e2de63c94b>,
        <http://mlentory.zbmed.de/mlentory_graph/b6726f6435b32ab359d8c48bd5f24d3180c4c1772a769d50d3b4c8cf8f0ecd46>,
        <http://mlentory.zbmed.de/mlentory_graph/c16d609c230b6e73c505c48462613ec3adc58ea075e6951dd0863657a07b00d3>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "7b"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
tags:
- merge
- mergekit
- mistral
- 7b
- lazymergekit
- mistralai/Mistral-7B-Instruct-v0.2
- uukuguy/speechless-mistral-six-in-one-7b-orth-1.0
---

# speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp

speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp is a merge of the following models:
* [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
* [uukuguy/speechless-mistral-six-in-one-7b-orth-1.0](https://huggingface.co/uukuguy/speechless-mistral-six-in-one-7b-orth-1.0)

## 🧩 Configuration

```yaml
slices:
  - sources:
      - model: mistralai/Mistral-7B-Instruct-v0.2
        layer_range: [0, 32]
      - model: uukuguy/speechless-mistral-six-in-one-7b-orth-1.0
        layer_range: [0, 32]
merge_method: slerp
base_model: mistralai/Mistral-7B-Instruct-v0.2
parameters:
  t:
    - filter: self_attn
      value: [0, 0.5, 0.3, 0.7, 1]
    - filter: mlp
      value: [1, 0.5, 0.7, 0.3, 0]
    - value: 0.5
dtype: bfloat16
```


## 💻 Usage


```python
!pip install -qU transformers accelerate

from transformers import AutoTokenizer
import transformers
import torch

model = "MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp"
messages = [{"role": "user", "content": "What is a large language model?"}]

tokenizer = AutoTokenizer.from_pretrained(model)
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
pipeline = transformers.pipeline(
"text-generation",
model=model,
torch_dtype=torch.float16,
device_map="auto",
)

outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp/discussions> ;
    ns1:readme <https://huggingface.co/MaziyarPanahi/speechless-mistral-six-in-one-7b-orth-1.0-Mistral-7B-Instruct-v0.2-slerp/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/131ddf8ef1045dc8bb71356d0ce2e1b589e6b924f855508c053046ecca64925a> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/3b4b981961ee604488ce9db2b6cea45a4fb777fb6ee8ef76bb274e14ff6cbdf4> ;
    ns2:modelCategory "Information not found"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/141e91e9e300f34f3844e30be636e1f5f85cf982186ff356d60256998ce834b9> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/arellewen/REINFORCE_CartPole-v1> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2025-02-28T11:07:41+00:00"^^xsd:dateTime ;
    schema:dateModified "2025-02-28T12:45:38+00:00"^^xsd:dateTime ;
    schema:datePublished "2025-02-28T11:07:41+00:00"^^xsd:dateTime ;
    schema:description """---
tags:
- CartPole-v1
- reinforce
- reinforcement-learning
- custom-implementation
- deep-rl-class
model-index:
- name: REINFORCE_CartPole-v1
  results:
  - task:
      type: reinforcement-learning
      name: reinforcement-learning
    dataset:
      name: CartPole-v1
      type: CartPole-v1
    metrics:
    - type: mean_reward
      value: 427.75 +/- 171.99
      name: mean_reward
      verified: false
---

  # **Reinforce** Agent playing **CartPole-v1**
  This is a trained model of a **Reinforce** agent playing **CartPole-v1** .
  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction
  """^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/arellewen/REINFORCE_CartPole-v1/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/arellewen/REINFORCE_CartPole-v1"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04b73aa26b48d6f1e64d5819ec986a901d3b49c7f37b35f4798db992ac65c200>,
        <http://mlentory.zbmed.de/mlentory_graph/13e6e4095523310627190177e08c0206ca97e76f4c4565c15c2b35270daa2588>,
        <http://mlentory.zbmed.de/mlentory_graph/16a23f6c98f42b86e5d4d52f3aac9a772d36e2f700ba7863b81456774dd81577>,
        <http://mlentory.zbmed.de/mlentory_graph/594a6f3cf66106516dc4e80f44dbe528cdcf0fb397d75a65d1b3b2a81dfc0f92>,
        <http://mlentory.zbmed.de/mlentory_graph/dbe08386414358e8b5b6ded6d9d2c773a7bdb23b4885d932efffcbf2c43b03e5>,
        <http://mlentory.zbmed.de/mlentory_graph/f5983965e0a2f764b153dc7a793f4806ba17fe7509997b4e09e9e39852c780af>,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
tags:
- CartPole-v1
- reinforce
- reinforcement-learning
- custom-implementation
- deep-rl-class
model-index:
- name: REINFORCE_CartPole-v1
  results:
  - task:
      type: reinforcement-learning
      name: reinforcement-learning
    dataset:
      name: CartPole-v1
      type: CartPole-v1
    metrics:
    - type: mean_reward
      value: 427.75 +/- 171.99
      name: mean_reward
      verified: false
---

  # **Reinforce** Agent playing **CartPole-v1**
  This is a trained model of a **Reinforce** agent playing **CartPole-v1** .
  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction
  """^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/arellewen/REINFORCE_CartPole-v1> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found"^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/arellewen/REINFORCE_CartPole-v1/discussions> ;
    ns1:readme <https://huggingface.co/arellewen/REINFORCE_CartPole-v1/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/145a1db14402ffd2d346a7bf50e3e9e8c75169383a079480e9ba655e5e2fab8a> a schema:DefinedTerm ;
    schema:description "Unity package for running neural networks in real-time applications and games."^^xsd:string ;
    schema:name "unity-sentis"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/150af95a9d97bb6756653ba43e03dd5be57b9b9fd27fc73a9e60ec97513fb0d1> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the model is for photorealism and producing sfw and nsfw images of decent quality. It is recommended for use on platforms like Mage.Space and Boosty."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model is available on Mage.Space (main sponsor)."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/11b7c117f8cd33c560f4862966654c32eb570fe472aa173c6a54749d61d20636> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/SG161222/RealVisXL_V2.0> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "The model is available on Mage.Space (main sponsor)."^^xsd:string ;
    schema:contributor "The model card authors are not mentioned in the provided context."^^xsd:string ;
    schema:copyrightHolder "The copyright holder is not mentioned in the provided context."^^xsd:string ;
    schema:dateCreated "2023-09-26T05:43:16+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-10-08T16:35:39+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-09-26T05:43:16+00:00"^^xsd:dateTime ;
    schema:description """---
license: openrail++
---
<strong>Check my exclusive models on Mage: </strong><a href="https://www.mage.space/play/4371756b27bf52e7a1146dc6fe2d969c" rel="noopener noreferrer nofollow"><strong>ParagonXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/df67a9f27f19629a98cb0fb619d1949a" rel="noopener noreferrer nofollow"><strong>NovaXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/d8db06ae964310acb4e090eec03984df" rel="noopener noreferrer nofollow"><strong>NovaXL Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/541da1e10976ab82976a5cacc770a413" rel="noopener noreferrer nofollow"><strong>NovaXL V2</strong></a><strong> / </strong><a href="https://www.mage.space/play/a56d2680c464ef25b8c66df126b3f706" rel="noopener noreferrer nofollow"><strong>NovaXL Pony</strong></a><strong> / </strong><a href="https://www.mage.space/play/b0ab6733c3be2408c93523d57a605371" rel="noopener noreferrer nofollow"><strong>NovaXL Pony Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/e3b01cd493ed86ed8e4708751b1c9165" rel="noopener noreferrer nofollow"><strong>RealDreamXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/ef062fc389c3f8723002428290c1158c" rel="noopener noreferrer nofollow"><strong>RealDreamXL Lightning</strong></a></p>
<b>This model is available on <a href="https://www.mage.space/">Mage.Space</a> (main sponsor)</b><br>
<b>You can support me directly on Boosty - https://boosty.to/sg_161222</b><br>

<b>It's important! Read it!</b><br>
The model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.<br>

The model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.<br>
CivitAI Page: https://civitai.com/models/139562/realvisxl-v20<br>

<b>Recommended Negative Prompt:</b><br>
(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth<br>
<b>or another negative prompt</b><br>

<b>Recommended Generation Parameters:</b><br>
Sampling Steps: 15-30<br>
Sampling Method: DPM++ SDE Karras<br>

<b>Recommended Hires Fix Parameters:</b><br>
Hires steps: 10+<br>
Upscaler: 4x-UltraSharp upscaler / or another<br>
Denoising strength: 0.1 - 0.5<br>
Upscale by: 1.1-2.0<br>"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/SG161222/RealVisXL_V2.0/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/SG161222/RealVisXL_V2.0"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9>,
        "diffusers:StableDiffusionXLPipeline"^^xsd:string,
        "license:openrail++"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "openrail++"^^xsd:string ;
    schema:maintainer "The maintainer of the model is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: openrail++
---
<strong>Check my exclusive models on Mage: </strong><a href="https://www.mage.space/play/4371756b27bf52e7a1146dc6fe2d969c" rel="noopener noreferrer nofollow"><strong>ParagonXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/df67a9f27f19629a98cb0fb619d1949a" rel="noopener noreferrer nofollow"><strong>NovaXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/d8db06ae964310acb4e090eec03984df" rel="noopener noreferrer nofollow"><strong>NovaXL Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/541da1e10976ab82976a5cacc770a413" rel="noopener noreferrer nofollow"><strong>NovaXL V2</strong></a><strong> / </strong><a href="https://www.mage.space/play/a56d2680c464ef25b8c66df126b3f706" rel="noopener noreferrer nofollow"><strong>NovaXL Pony</strong></a><strong> / </strong><a href="https://www.mage.space/play/b0ab6733c3be2408c93523d57a605371" rel="noopener noreferrer nofollow"><strong>NovaXL Pony Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/e3b01cd493ed86ed8e4708751b1c9165" rel="noopener noreferrer nofollow"><strong>RealDreamXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/ef062fc389c3f8723002428290c1158c" rel="noopener noreferrer nofollow"><strong>RealDreamXL Lightning</strong></a></p>
<b>This model is available on <a href="https://www.mage.space/">Mage.Space</a> (main sponsor)</b><br>
<b>You can support me directly on Boosty - https://boosty.to/sg_161222</b><br>

<b>It's important! Read it!</b><br>
The model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.<br>

The model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.<br>
CivitAI Page: https://civitai.com/models/139562/realvisxl-v20<br>

<b>Recommended Negative Prompt:</b><br>
(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth<br>
<b>or another negative prompt</b><br>

<b>Recommended Generation Parameters:</b><br>
Sampling Steps: 15-30<br>
Sampling Method: DPM++ SDE Karras<br>

<b>Recommended Hires Fix Parameters:</b><br>
Hires steps: 10+<br>
Upscaler: 4x-UltraSharp upscaler / or another<br>
Denoising strength: 0.1 - 0.5<br>
Upscale by: 1.1-2.0<br>"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/SG161222/RealVisXL_V2.0> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/SG161222/RealVisXL_V2.0/discussions> ;
    ns1:readme <https://huggingface.co/SG161222/RealVisXL_V2.0/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/157e8761577b237679066841ff2867452145bd89d99b81b7423a40a44b8344ef> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the reference implementation of `FLUX.1 [dev]` is for developers and creatives looking to build on top of `FLUX.1 [dev]`. The text suggests that this implementation is a starting point for building on top of `FLUX.1 [dev]`, and that developers and creatives should use this as a reference to build their own projects."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/97b8857097478874bd8e9cb141256c1acde4789374ce4d72be5580b0da08f403> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2025-01-18T01:59:38+00:00"^^xsd:dateTime ;
    schema:dateModified "2025-03-14T02:07:39+00:00"^^xsd:dateTime ;
    schema:datePublished "2025-01-18T01:59:38+00:00"^^xsd:dateTime ;
    schema:description """---
license: other
license_name: flux-1-dev-non-commercial-license
license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md
extra_gated_prompt: By clicking "Agree", you agree to the [FluxDev Non-Commercial
  License Agreement](https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx/blob/main/LICENSE.md)
  and acknowledge the [Acceptable Use Policy](https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx/blob/main/POLICY.md).
tags:
- text-to-image
- image-generation
- flux
---

![FLUX.1 [dev] Grid](./dev_grid.jpg)

`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.
For more information, please read our [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/).

This repository holds [ONNX](https://github.com/onnx/onnx) exports of FLUX.1 [dev] in BF16, FP8, and FP4 precision. 
The main repository of this model can be found [here](https://huggingface.co/black-forest-labs/FLUX.1-dev).

# Usage
We provide a reference implementation of `FLUX.1 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux).
Developers and creatives looking to build on top of `FLUX.1 [dev]` are encouraged to use this as a starting point."""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/03a8010a33b7e6718a7b9231b9c21ef9f07a99b444199c8533c744e5ddf1b77a>,
        <http://mlentory.zbmed.de/mlentory_graph/4e5443728f51b2323bf3a1c4a6a6d8612484cbca0a8b0beeb5fb0258d1df5196>,
        <http://mlentory.zbmed.de/mlentory_graph/c7504a968f417dfe1555958db6caabff413bf4f321bbb9c477d4ff3a9352a0e4>,
        <http://mlentory.zbmed.de/mlentory_graph/f57740f97820ddb55f90898c07a60a736e9a9030d5eac125af5c595ad0638c8f>,
        "license:other"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "other"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the reference implementation of `FLUX.1 [dev]` are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: other
license_name: flux-1-dev-non-commercial-license
license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md
extra_gated_prompt: By clicking "Agree", you agree to the [FluxDev Non-Commercial
  License Agreement](https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx/blob/main/LICENSE.md)
  and acknowledge the [Acceptable Use Policy](https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx/blob/main/POLICY.md).
tags:
- text-to-image
- image-generation
- flux
---

![FLUX.1 [dev] Grid](./dev_grid.jpg)

`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.
For more information, please read our [blog post](https://blackforestlabs.ai/announcing-black-forest-labs/).

This repository holds [ONNX](https://github.com/onnx/onnx) exports of FLUX.1 [dev] in BF16, FP8, and FP4 precision. 
The main repository of this model can be found [here](https://huggingface.co/black-forest-labs/FLUX.1-dev).

# Usage
We provide a reference implementation of `FLUX.1 [dev]`, as well as sampling code, in a dedicated [github repository](https://github.com/black-forest-labs/flux).
Developers and creatives looking to build on top of `FLUX.1 [dev]` are encouraged to use this as a starting point."""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx/discussions> ;
    ns1:readme <https://huggingface.co/black-forest-labs/FLUX.1-dev-onnx/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/15ceeb8703c6ba669411911c62acbbac38f4d1235e8ac6118537ac1e6d75ae84> a schema:DefinedTerm ;
    schema:description "High-throughput asynchronous reinforcement learning framework for training RL agents."^^xsd:string ;
    schema:name "sample-factory"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/1759562ba5af131b8ba1cdb4e06a2cee5c1ebff35971610e6ef2eb585b413ef7> a schema:DefinedTerm ;
    schema:description "Identifies when someone is speaking in an audio stream."^^xsd:string ;
    schema:name "Voice Activity Detection"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/192e3c37cdad044da28b01edd768819d9dc9ebf5642a4dd3e0cf9564058b6d2a> a schema:DefinedTerm ;
    schema:description "Models quantized to 8-bit precision, balancing performance and memory efficiency."^^xsd:string ;
    schema:name "8-bit precision"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/195dccb843b7724d0cb10808bc4e04746e617b113f58332e52204a472b62e958> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/e50ce491d4abac67bd512ffe733d1ed10412db83802a3b15e0cc40b0bcc5203a> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/85fa58f915b932926d1bb143b0bca80547964254fc92a39517d34e5b7250f9bd> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/grimjim/mistralai-Mistral-7B-v0.3> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "The context provided does not contain any information about contributors or related sections."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-05-23T20:01:10+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-10-21T16:45:18+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-23T20:01:10+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model:
- mistralai/Mistral-7B-v0.3
pipeline_tag: text-generation
library_name: transformers
---

# Model Card for Mistral-7B-v0.3

The Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-v0.2/edit/main/README.md)
- Extended vocabulary to 32768

## Installation

It is recommended to use `mistralai/Mistral-7B-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Demo

After installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.

```
mistral-demo $HOME/mistral_models/7B-v0.3
```

Should give something along the following lines:

```
This is a test of the emergency broadcast system. This is only a test.

If this were a real emergency, you would be told what to do.

This is a test
=====================
This is another test of the new blogging software. I’m not sure if I’m going to keep it or not. I’m not sure if I’m going to keep
=====================
This is a third test, mistral AI is very good at testing. 🙂

This is a third test, mistral AI is very good at testing. 🙂

This
=====================
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "mistralai/Mistral-7B-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id)
inputs = tokenizer("Hello my name is", return_tensors="pt")

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/grimjim/mistralai-Mistral-7B-v0.3/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/grimjim/mistralai-Mistral-7B-v0.3"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:finetune:mistralai/Mistral-7B-v0.3"^^xsd:string,
        "base_model:mistralai/Mistral-7B-v0.3"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by Mistral-7B-v0.3 are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The text does not provide specific information about the processor requirements for running the Mistral-7B-v0.3 model."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model:
- mistralai/Mistral-7B-v0.3
pipeline_tag: text-generation
library_name: transformers
---

# Model Card for Mistral-7B-v0.3

The Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-v0.2/edit/main/README.md)
- Extended vocabulary to 32768

## Installation

It is recommended to use `mistralai/Mistral-7B-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Demo

After installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.

```
mistral-demo $HOME/mistral_models/7B-v0.3
```

Should give something along the following lines:

```
This is a test of the emergency broadcast system. This is only a test.

If this were a real emergency, you would be told what to do.

This is a test
=====================
This is another test of the new blogging software. I’m not sure if I’m going to keep it or not. I’m not sure if I’m going to keep
=====================
This is a third test, mistral AI is very good at testing. 🙂

This is a third test, mistral AI is very good at testing. 🙂

This
=====================
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "mistralai/Mistral-7B-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id)
inputs = tokenizer("Hello my name is", return_tensors="pt")

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/grimjim/mistralai-Mistral-7B-v0.3> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/grimjim/mistralai-Mistral-7B-v0.3/discussions> ;
    ns1:readme <https://huggingface.co/grimjim/mistralai-Mistral-7B-v0.3/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/21a4b3b0fe861021548d89e241eb815b3c147c6d12ec9eade6925be45bc78650> a schema:DefinedTerm ;
    schema:description "Transforms audio signals from one form to another."^^xsd:string ;
    schema:name "Audio to Audio"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2221cfd9924f3af781e4ea213f95b4891059e39c1e825e3fb806f2526dc905f7> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/40b331abbeb7a5faa9ee0a36b7b9b9c0748500ab183a2ef8af0f673230b1f45a> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """The intended use of the LoRA DreamBooth - KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl is not explicitly stated in the given context. However, it can be inferred that the purpose of this LoRA adaptation is to enhance the quality of images generated using the RealVisXL_V2.0 model. The text mentions that the weights were trained on a photo of NoahSanchez using DreamBooth, which suggests that the intended use is for generating images of NoahSanchez or similar subjects. 

To determine the exact intended use, more information would be needed, such as the specific instructions or guidelines provided by the creators of the LoRA adaptation."""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/a1943d610be97366c0b632005eeea8e493269f372ceb5ffb4ab1aefe2fde3d1f> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "The text does not provide any information about the validated On."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl> ;
    schema:author "The author of this content is KyriaAnnwyn."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "The context provided does not contain any information about the conditions of access to the LoRA DreamBooth - KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl model."^^xsd:string ;
    schema:contributor "The contributor in the given text is \"KyriaAnnwyn\"."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2023-11-10T10:55:34+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-11-10T11:19:17+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-11-10T10:55:34+00:00"^^xsd:dateTime ;
    schema:description """
---
license: openrail++
base_model: SG161222/RealVisXL_V2.0
instance_prompt: a photo of NoahSanchez
tags:
- stable-diffusion-xl
- stable-diffusion-xl-diffusers
- text-to-image
- diffusers
- lora
inference: true
---
    
# LoRA DreamBooth - KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl

These are LoRA adaption weights for SG161222/RealVisXL_V2.0. The weights were trained on a photo of NoahSanchez using [DreamBooth](https://dreambooth.github.io/). You can find some example images in the following. 

![img_0](./image_0.png)
![img_1](./image_1.png)
![img_2](./image_2.png)
![img_3](./image_3.png)


LoRA for the text encoder was enabled: True.

Special VAE used for training: None.
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl/discussions> ;
    schema:distribution "The distribution in the text is not explicitly mentioned."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0b6c077d0e7e512fe4a09f8d65944ef93c47c0c822ba512f9d69a0205fbc4e04>,
        <http://mlentory.zbmed.de/mlentory_graph/12f271f5159c42562f3014bee76f49cdb0b6af26a1e072c41d27917bbe668147>,
        <http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9>,
        <http://mlentory.zbmed.de/mlentory_graph/ec80fcbc48ed63fb19725597c40817bdccd1afeefab08c05e9e4df8a60b2d5eb>,
        <http://mlentory.zbmed.de/mlentory_graph/f57740f97820ddb55f90898c07a60a736e9a9030d5eac125af5c595ad0638c8f>,
        "base_model:SG161222/RealVisXL_V2.0"^^xsd:string,
        "base_model:adapter:SG161222/RealVisXL_V2.0"^^xsd:string,
        "license:openrail++"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "openrail++"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "The context provided does not contain any information about the processor requirements for running the application."^^xsd:string ;
    schema:releaseNotes """
---
license: openrail++
base_model: SG161222/RealVisXL_V2.0
instance_prompt: a photo of NoahSanchez
tags:
- stable-diffusion-xl
- stable-diffusion-xl-diffusers
- text-to-image
- diffusers
- lora
inference: true
---
    
# LoRA DreamBooth - KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl

These are LoRA adaption weights for SG161222/RealVisXL_V2.0. The weights were trained on a photo of NoahSanchez using [DreamBooth](https://dreambooth.github.io/). You can find some example images in the following. 

![img_0](./image_0.png)
![img_1](./image_1.png)
![img_2](./image_2.png)
![img_3](./image_3.png)


LoRA for the text encoder was enabled: True.

Special VAE used for training: None.
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "The context provided does not contain any information about storage requirements."^^xsd:string ;
    schema:url <https://huggingface.co/KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl/discussions> ;
    ns1:readme <https://huggingface.co/KyriaAnnwyn/lora-trained-NoahSanchez_RV2_novae_long-xl/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2260d23c57c23706360f274bdd86ac9084ee32774e7ae08d0bcb7a185091eeb7> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The context provided does not contain information about the intended use of the creative work."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category in the provided text is not explicitly mentioned."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/40c6f438362f1a92103a016e23ec429f4b4dc082c4d0b6b7f94f07db46fe1706> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions """To get started with the model, follow these steps:

1. Add your dataset to the notebook.
2. Click "Run All" to execute the notebook.
3. You will receive a 2x faster finetuned model which can be exported to GGUF, vLLM, or uploaded to Hugging Face.

The model supports the following models:

- Llama-3 8b
- Gemma 7b
- Mistral 7b
- Llama-2 7b
- TinyLlama
- CodeLlama 34b A100
- Mistral 7b 1xT4
- DPO - Zephyr

The notebook is beginner-friendly, and you can use it for ShareGPT ChatML / Vicuna templates, raw text, or DPO."""^^xsd:string ;
    ns2:validatedOn "The text does not provide information about the validated On."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/RichardErkhov/unsloth_-_llama-3-8b-Instruct-bnb-4bit-4bits> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "The context provided does not contain information about the conditions of access to the property."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-05-03T10:30:33+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-05-03T10:43:14+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-03T10:30:33+00:00"^^xsd:dateTime ;
    schema:description """---
{}
---
Quantization made by Richard Erkhov.

[Github](https://github.com/RichardErkhov)

[Discord](https://discord.gg/pvy7H8DZMG)

[Request more models](https://github.com/RichardErkhov/quant_request)


llama-3-8b-Instruct-bnb-4bit - bnb 4bits
- Model creator: https://huggingface.co/unsloth/
- Original model: https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit/




Original model description:
---
language:
- en
license: apache-2.0
library_name: transformers
tags:
- unsloth
- transformers
- llama
- llama-3
---

# Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!

Directly quantized 4bit model with `bitsandbytes`.

We have a Google Colab Tesla T4 notebook for Llama-3 8b here: https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png" width="200"/>](https://discord.gg/u54VK8m8tk)
[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy%20me%20a%20coffee%20button.png" width="200"/>](https://ko-fi.com/unsloth)
[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)

## ✨ Finetune for Free

All notebooks are **beginner friendly**! Add your dataset, click "Run All", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.

| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
| **Llama-3 8b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2.4x faster | 58% less |
| **Gemma 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 58% less |
| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |
| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |
| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |
| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |
| **Mistral 7b** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\\* | 62% less |
| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |

- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.

"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/RichardErkhov/unsloth_-_llama-3-8b-Instruct-bnb-4bit-4bits/discussions> ;
    schema:distribution "The distribution in the text is not explicitly mentioned. However, it is mentioned that the model can be exported to GGUF, vLLM, or uploaded to Hugging Face."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/RichardErkhov/unsloth_-_llama-3-8b-Instruct-bnb-4bit-4bits"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0813ac63520e9dbe708eec9898230a9dbdb1c48971b07fa1ad53fb6dfaa7a564>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/2976f620ce615218c4caaaa64d292b6c1cc2785cda2fe2fcc1fabefc0c73faef>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "The maintainer of the item is not mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
{}
---
Quantization made by Richard Erkhov.

[Github](https://github.com/RichardErkhov)

[Discord](https://discord.gg/pvy7H8DZMG)

[Request more models](https://github.com/RichardErkhov/quant_request)


llama-3-8b-Instruct-bnb-4bit - bnb 4bits
- Model creator: https://huggingface.co/unsloth/
- Original model: https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit/




Original model description:
---
language:
- en
license: apache-2.0
library_name: transformers
tags:
- unsloth
- transformers
- llama
- llama-3
---

# Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth!

Directly quantized 4bit model with `bitsandbytes`.

We have a Google Colab Tesla T4 notebook for Llama-3 8b here: https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png" width="200"/>](https://discord.gg/u54VK8m8tk)
[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy%20me%20a%20coffee%20button.png" width="200"/>](https://ko-fi.com/unsloth)
[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)

## ✨ Finetune for Free

All notebooks are **beginner friendly**! Add your dataset, click "Run All", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.

| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
| **Llama-3 8b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing)               | 2.4x faster | 58% less |
| **Gemma 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)               | 2.4x faster | 58% less |
| **Mistral 7b**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |
| **Llama-2 7b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)               | 2.2x faster | 43% less |
| **TinyLlama**  | [▶️ Start on Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)              | 3.9x faster | 74% less |
| **CodeLlama 34b** A100   | [▶️ Start on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)              | 1.9x faster | 27% less |
| **Mistral 7b** 1xT4  | [▶️ Start on Kaggle](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) | 5x faster\\* | 62% less |
| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |

- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.

"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/RichardErkhov/unsloth_-_llama-3-8b-Instruct-bnb-4bit-4bits> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/RichardErkhov/unsloth_-_llama-3-8b-Instruct-bnb-4bit-4bits/discussions> ;
    ns1:readme <https://huggingface.co/RichardErkhov/unsloth_-_llama-3-8b-Instruct-bnb-4bit-4bits/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/25a48b7b32ec31051c00ea96f30d1637f216eb0ed7620da77d06c4637ab5da7f> a schema:DefinedTerm ;
    schema:description "Categorizes images into predefined classes or labels."^^xsd:string ;
    schema:name "Image Classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/266c130bc128debb0db48b7a2e58c6423c7bafb2445fcccd68e2da192f3487d1> a schema:DefinedTerm ;
    schema:description "Converts visual content from images into textual representations."^^xsd:string ;
    schema:name "Image toText"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/293ad034f899dc681b5be10c715f25c59fa7f5261f229b66d796246e10927354> a schema:DefinedTerm ;
    schema:description "Unity plugin that enables games and simulations to serve as environments for training intelligent agents."^^xsd:string ;
    schema:name "ml-agents"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2a6252a686be67870057a99fee8f242bac2e674e5271387aeef2947408fdc614> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/0293985a9647a36f09130bb7b83e928f80516acf3bd2d2fe7e311efc911f3984>,
        <http://mlentory.zbmed.de/mlentory_graph/20227b77ce090a21afa4ed043d9908132d4def622f31e14f1f3ef52047648a8c>,
        <http://mlentory.zbmed.de/mlentory_graph/3208536fd5f5414cfa274461aae830a0b3b39e3ba2e9e782979c75d3f8277fec>,
        <http://mlentory.zbmed.de/mlentory_graph/f5b41a8e8b49e0f38dadd1a8bb57cddfa9aec7e09638c87319e76718ceb45770> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of TinyLlama-1.1B is not explicitly stated in the provided context. However, it is mentioned that TinyLlama can be plugged and played in many open-source projects built upon Llama, and it is compact with only 1.1B parameters, making it suitable for applications demanding a restricted computation and memory footprint."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Model Details"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/ed3cf312800bc7233c923d178ecf3bb18ab45b2969d4c652c0e1b0abbce9cc39> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/0293985a9647a36f09130bb7b83e928f80516acf3bd2d2fe7e311efc911f3984>,
        <http://mlentory.zbmed.de/mlentory_graph/20227b77ce090a21afa4ed043d9908132d4def622f31e14f1f3ef52047648a8c>,
        <http://mlentory.zbmed.de/mlentory_graph/3208536fd5f5414cfa274461aae830a0b3b39e3ba2e9e782979c75d3f8277fec>,
        <http://mlentory.zbmed.de/mlentory_graph/f5b41a8e8b49e0f38dadd1a8bb57cddfa9aec7e09638c87319e76718ceb45770> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/0293985a9647a36f09130bb7b83e928f80516acf3bd2d2fe7e311efc911f3984>,
        <http://mlentory.zbmed.de/mlentory_graph/20227b77ce090a21afa4ed043d9908132d4def622f31e14f1f3ef52047648a8c>,
        <http://mlentory.zbmed.de/mlentory_graph/3208536fd5f5414cfa274461aae830a0b3b39e3ba2e9e782979c75d3f8277fec>,
        <http://mlentory.zbmed.de/mlentory_graph/f5b41a8e8b49e0f38dadd1a8bb57cddfa9aec7e09638c87319e76718ceb45770> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "The copyright holder is not mentioned in the provided context."^^xsd:string ;
    schema:dateCreated "2023-12-30T06:27:30+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-03-17T05:07:08+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-12-30T06:27:30+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
datasets:
- cerebras/SlimPajama-627B
- bigcode/starcoderdata
- HuggingFaceH4/ultrachat_200k
- HuggingFaceH4/ultrafeedback_binarized
language:
- en
widget:
- example_title: Fibonacci (Python)
  messages:
  - role: system
    content: You are a chatbot who can help code!
  - role: user
    content: Write me a function to calculate the first 10 digits of the fibonacci
      sequence in Python and print it out to the CLI.
---
<div align="center">

# TinyLlama-1.1B
</div>

https://github.com/jzhang38/TinyLlama

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. 


We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### This Model
This is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was " initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. 
We then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4." 


#### How to use
You will need the transformers>=4.34
Do check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.

```python
# Install transformers from source - only needed for versions <= v4.34
# pip install git+https://github.com/huggingface/transformers.git
# pip install accelerate

import torch
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")

# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
# <|system|>
# You are a friendly chatbot who always responds in the style of a pirate.</s>
# <|user|>
# How many helicopters can a human eat in one sitting?</s>
# <|assistant|>
# ...
```"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "dataset:HuggingFaceH4/ultrachat_200k"^^xsd:string,
        "dataset:HuggingFaceH4/ultrafeedback_binarized"^^xsd:string,
        "dataset:bigcode/starcoderdata"^^xsd:string,
        "dataset:cerebras/SlimPajama-627B"^^xsd:string,
        "en"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "The maintainer of TinyLlama-1.1B is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
datasets:
- cerebras/SlimPajama-627B
- bigcode/starcoderdata
- HuggingFaceH4/ultrachat_200k
- HuggingFaceH4/ultrafeedback_binarized
language:
- en
widget:
- example_title: Fibonacci (Python)
  messages:
  - role: system
    content: You are a chatbot who can help code!
  - role: user
    content: Write me a function to calculate the first 10 digits of the fibonacci
      sequence in Python and print it out to the CLI.
---
<div align="center">

# TinyLlama-1.1B
</div>

https://github.com/jzhang38/TinyLlama

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. 


We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### This Model
This is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was " initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. 
We then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4." 


#### How to use
You will need the transformers>=4.34
Do check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.

```python
# Install transformers from source - only needed for versions <= v4.34
# pip install git+https://github.com/huggingface/transformers.git
# pip install accelerate

import torch
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")

# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
# <|system|>
# You are a friendly chatbot who always responds in the style of a pirate.</s>
# <|user|>
# How many helicopters can a human eat in one sitting?</s>
# <|assistant|>
# ...
```"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements """The software requirements mentioned in the context are:

1. transformers>=4.34
2. Check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information."""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/discussions> ;
    ns1:readme <https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2ae536499f90e7e5edcd87dddf0d258801cae8e7a2f82d8ce02361ef54e77c05> a schema:DefinedTerm ;
    schema:description "Apple's framework for integrating machine learning models into iOS, macOS, watchOS, and tvOS apps."^^xsd:string ;
    schema:name "Core ML"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2f45196854d69c3bcc21304aceb64e2a3db2d17613399d7d3c7a21611573a4d5> a schema:DefinedTerm ;
    schema:description "Models with documented carbon footprint information related to their training process."^^xsd:string ;
    schema:name "Carbon Emissions"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/30dd2efcaa6fa16117ccc754f8b9bf72cdf3db3f4aeb4f775f736afa625b9d39> a schema:Organization .

<http://mlentory.zbmed.de/mlentory_graph/3135ad7c615e157f80ecab1b96d4167d2e6c147b2e563721a652a5b56845418c> a schema:DefinedTerm ;
    schema:description "Collection of SOTA computer vision models, layers, utilities, and optimizers for training and inferencing."^^xsd:string ;
    schema:name "timm"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/316fd1b6c56f67729f7e0880d91953804a9af869ce76b2906f26f3d534937931> a schema:DefinedTerm ;
    schema:description "Topic modeling technique that leverages BERT embeddings and clustering for document topic extraction."^^xsd:string ;
    schema:name "BERTopic"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/35d7d276230081bdfd86d06ca958126011d6a7e381452a488c2bfa8fcc512fdb> a schema:DefinedTerm ;
    schema:description "Architecture where multiple specialized sub-models (experts) are conditionally activated based on input."^^xsd:string ;
    schema:name "Mixture of Experts"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/3c0ba30e405e7b0497398c5156da4d4a2c5b29033be2b8a3adb5f32f9e4f7471> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/5f8c97c4aaca5a8c6afac1954a68be2dcb40698e64871f37e5fe5dc5849a3aab>,
        <http://mlentory.zbmed.de/mlentory_graph/746e796ec31ad9ad2b517555a4e91b2e3dfff187021fd0255aae50476624430d> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/d8f2d587f7a2538c0b27b071b8686eeb4ca1c2d23df31d6e6808ce3d984a15b0> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/ILKT/2024-06-24_22-31-18_epoch_17> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found"^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-06-25T01:52:17+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-06-28T14:52:59+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-06-25T01:52:17+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- en
- pl
model-index:
- name: 2024-06-24_22-31-18_epoch_17
  results:
  - dataset:
      config: default
      name: MTEB AllegroReviews
      revision: b89853e6de927b0e3bfa8ecc0e56fe4e02ceafc6
      split: test
      type: PL-MTEB/allegro-reviews
    metrics:
    - type: accuracy
      value: 23.111332007952285
    - type: f1
      value: 20.922233617200952
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB CBD
      revision: 36ddb419bcffe6a5374c3891957912892916f28d
      split: test
      type: PL-MTEB/cbd
    metrics:
    - type: accuracy
      value: 55.010000000000005
    - type: ap
      value: 15.264405069688278
    - type: f1
      value: 46.42734568792598
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB CDSC-E
      revision: 0a3d4aa409b22f80eb22cbf59b492637637b536d
      split: test
      type: PL-MTEB/cdsce-pairclassification
    metrics: []
    task:
      type: PairClassification
  - dataset:
      config: default
      name: MTEB CDSC-R
      revision: 1cd6abbb00df7d14be3dbd76a7dcc64b3a79a7cd
      split: test
      type: PL-MTEB/cdscr-sts
    metrics: []
    task:
      type: STS
  - dataset:
      config: default
      name: MTEB EightTagsClustering
      revision: 78b962b130c6690659c65abf67bf1c2f030606b6
      split: test
      type: PL-MTEB/8tags-clustering
    metrics:
    - type: v_measure
      value: 11.749984888594936
    - type: v_measure_std
      value: 2.153175630562388
    task:
      type: Clustering
  - dataset:
      config: pl
      name: MTEB MassiveIntentClassification (pl)
      revision: 4672e20407010da34463acc759c162ca9734bca6
      split: test
      type: mteb/amazon_massive_intent
    metrics:
    - type: accuracy
      value: 24.10221923335575
    - type: f1
      value: 21.743191256542133
    task:
      type: Classification
  - dataset:
      config: pl
      name: MTEB MassiveIntentClassification (pl)
      revision: 4672e20407010da34463acc759c162ca9734bca6
      split: validation
      type: mteb/amazon_massive_intent
    metrics:
    - type: accuracy
      value: 24.181013280865713
    - type: f1
      value: 21.417194623179693
    task:
      type: Classification
  - dataset:
      config: pl
      name: MTEB MassiveScenarioClassification (pl)
      revision: fad2c6e8459f9e1c45d9315f4953d921437d70f8
      split: test
      type: mteb/amazon_massive_scenario
    metrics:
    - type: accuracy
      value: 31.11297915265635
    - type: f1
      value: 29.01600098649581
    task:
      type: Classification
  - dataset:
      config: pl
      name: MTEB MassiveScenarioClassification (pl)
      revision: fad2c6e8459f9e1c45d9315f4953d921437d70f8
      split: validation
      type: mteb/amazon_massive_scenario
    metrics:
    - type: accuracy
      value: 30.570585341859324
    - type: f1
      value: 28.997180237535552
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB PAC
      revision: fc69d1c153a8ccdcf1eef52f4e2a27f88782f543
      split: test
      type: laugustyniak/abusive-clauses-pl
    metrics:
    - type: accuracy
      value: 61.76947581812917
    - type: ap
      value: 72.61605166006719
    - type: f1
      value: 58.70062063338849
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB PSC
      revision: d05a294af9e1d3ff2bfb6b714e08a24a6cabc669
      split: test
      type: PL-MTEB/psc-pairclassification
    metrics: []
    task:
      type: PairClassification
  - dataset:
      config: default
      name: MTEB PlscClusteringP2P
      revision: 8436dd4c05222778013d6642ee2f3fa1722bca9b
      split: test
      type: PL-MTEB/plsc-clustering-p2p
    metrics:
    - type: v_measure
      value: 36.630974271573855
    task:
      type: Clustering
  - dataset:
      config: default
      name: MTEB PlscClusteringS2S
      revision: 39bcadbac6b1eddad7c1a0a176119ce58060289a
      split: test
      type: PL-MTEB/plsc-clustering-s2s
    metrics:
    - type: v_measure
      value: 32.37393038036762
    task:
      type: Clustering
  - dataset:
      config: default
      name: MTEB PolEmo2.0-IN
      revision: d90724373c70959f17d2331ad51fb60c71176b03
      split: test
      type: PL-MTEB/polemo2_in
    metrics:
    - type: accuracy
      value: 45.581717451523545
    - type: f1
      value: 46.52096155620845
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB PolEmo2.0-OUT
      revision: 6a21ab8716e255ab1867265f8b396105e8aa63d4
      split: test
      type: PL-MTEB/polemo2_out
    metrics:
    - type: accuracy
      value: 18.643724696356273
    - type: f1
      value: 15.461041528691336
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB SICK-E-PL
      revision: 71bba34b0ece6c56dfcf46d9758a27f7a90f17e9
      split: test
      type: PL-MTEB/sicke-pl-pairclassification
    metrics: []
    task:
      type: PairClassification
  - dataset:
      config: default
      name: MTEB SICK-R-PL
      revision: fd5c2441b7eeff8676768036142af4cfa42c1339
      split: test
      type: PL-MTEB/sickr-pl-sts
    metrics: []
    task:
      type: STS
  - dataset:
      config: pl
      name: MTEB STS22 (pl)
      revision: de9d86b3b84231dc21f76c7b7af1f28e2f57f6e3
      split: test
      type: mteb/sts22-crosslingual-sts
    metrics: []
    task:
      type: STS
  - dataset:
      config: pl
      name: MTEB STSBenchmarkMultilingualSTS (pl)
      revision: 29afa2569dcedaaa2fe6a3dcfebab33d28b82e8c
      split: dev
      type: mteb/stsb_multi_mt
    metrics: []
    task:
      type: STS
  - dataset:
      config: pl
      name: MTEB STSBenchmarkMultilingualSTS (pl)
      revision: 29afa2569dcedaaa2fe6a3dcfebab33d28b82e8c
      split: test
      type: mteb/stsb_multi_mt
    metrics: []
    task:
      type: STS
pipeline_tag: sentence-similarity
tags:
- sentence-transformers
- sentence-similarity
- mteb
- feature-extraction
---
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/ILKT/2024-06-24_22-31-18_epoch_17/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/ILKT/2024-06-24_22-31-18_epoch_17"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string,
        "pl"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/13e6e4095523310627190177e08c0206ca97e76f4c4565c15c2b35270daa2588>,
        <http://mlentory.zbmed.de/mlentory_graph/55bd15a0af8bef99be0b0832411b5afbbb83779bc0cd1795dffee7c3285baa1f>,
        <http://mlentory.zbmed.de/mlentory_graph/55c2ef29f4fc34014e7c41f2439cd686ebcc0bd4d1a02299b7491148b03bd06b>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/bb5a8d5565bb8b4e7a168aaa8cac657040496dc48337408cd7960e62d51eb863>,
        <http://mlentory.zbmed.de/mlentory_graph/ca449e33dbfb2eb8a94ef8f31fe870b192443910298d3cb6ca462aab9710b804>,
        <http://mlentory.zbmed.de/mlentory_graph/e05dcf019c16b58ee18632feff8d896599b435fed50864784dfbbf57cfeb9f88>,
        <http://mlentory.zbmed.de/mlentory_graph/e2cb5f7dba65387f6ecd56e7a642d33df8fca1197b2ac918cd629a6cd9b294b2>,
        "en"^^xsd:string,
        "pl"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- en
- pl
model-index:
- name: 2024-06-24_22-31-18_epoch_17
  results:
  - dataset:
      config: default
      name: MTEB AllegroReviews
      revision: b89853e6de927b0e3bfa8ecc0e56fe4e02ceafc6
      split: test
      type: PL-MTEB/allegro-reviews
    metrics:
    - type: accuracy
      value: 23.111332007952285
    - type: f1
      value: 20.922233617200952
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB CBD
      revision: 36ddb419bcffe6a5374c3891957912892916f28d
      split: test
      type: PL-MTEB/cbd
    metrics:
    - type: accuracy
      value: 55.010000000000005
    - type: ap
      value: 15.264405069688278
    - type: f1
      value: 46.42734568792598
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB CDSC-E
      revision: 0a3d4aa409b22f80eb22cbf59b492637637b536d
      split: test
      type: PL-MTEB/cdsce-pairclassification
    metrics: []
    task:
      type: PairClassification
  - dataset:
      config: default
      name: MTEB CDSC-R
      revision: 1cd6abbb00df7d14be3dbd76a7dcc64b3a79a7cd
      split: test
      type: PL-MTEB/cdscr-sts
    metrics: []
    task:
      type: STS
  - dataset:
      config: default
      name: MTEB EightTagsClustering
      revision: 78b962b130c6690659c65abf67bf1c2f030606b6
      split: test
      type: PL-MTEB/8tags-clustering
    metrics:
    - type: v_measure
      value: 11.749984888594936
    - type: v_measure_std
      value: 2.153175630562388
    task:
      type: Clustering
  - dataset:
      config: pl
      name: MTEB MassiveIntentClassification (pl)
      revision: 4672e20407010da34463acc759c162ca9734bca6
      split: test
      type: mteb/amazon_massive_intent
    metrics:
    - type: accuracy
      value: 24.10221923335575
    - type: f1
      value: 21.743191256542133
    task:
      type: Classification
  - dataset:
      config: pl
      name: MTEB MassiveIntentClassification (pl)
      revision: 4672e20407010da34463acc759c162ca9734bca6
      split: validation
      type: mteb/amazon_massive_intent
    metrics:
    - type: accuracy
      value: 24.181013280865713
    - type: f1
      value: 21.417194623179693
    task:
      type: Classification
  - dataset:
      config: pl
      name: MTEB MassiveScenarioClassification (pl)
      revision: fad2c6e8459f9e1c45d9315f4953d921437d70f8
      split: test
      type: mteb/amazon_massive_scenario
    metrics:
    - type: accuracy
      value: 31.11297915265635
    - type: f1
      value: 29.01600098649581
    task:
      type: Classification
  - dataset:
      config: pl
      name: MTEB MassiveScenarioClassification (pl)
      revision: fad2c6e8459f9e1c45d9315f4953d921437d70f8
      split: validation
      type: mteb/amazon_massive_scenario
    metrics:
    - type: accuracy
      value: 30.570585341859324
    - type: f1
      value: 28.997180237535552
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB PAC
      revision: fc69d1c153a8ccdcf1eef52f4e2a27f88782f543
      split: test
      type: laugustyniak/abusive-clauses-pl
    metrics:
    - type: accuracy
      value: 61.76947581812917
    - type: ap
      value: 72.61605166006719
    - type: f1
      value: 58.70062063338849
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB PSC
      revision: d05a294af9e1d3ff2bfb6b714e08a24a6cabc669
      split: test
      type: PL-MTEB/psc-pairclassification
    metrics: []
    task:
      type: PairClassification
  - dataset:
      config: default
      name: MTEB PlscClusteringP2P
      revision: 8436dd4c05222778013d6642ee2f3fa1722bca9b
      split: test
      type: PL-MTEB/plsc-clustering-p2p
    metrics:
    - type: v_measure
      value: 36.630974271573855
    task:
      type: Clustering
  - dataset:
      config: default
      name: MTEB PlscClusteringS2S
      revision: 39bcadbac6b1eddad7c1a0a176119ce58060289a
      split: test
      type: PL-MTEB/plsc-clustering-s2s
    metrics:
    - type: v_measure
      value: 32.37393038036762
    task:
      type: Clustering
  - dataset:
      config: default
      name: MTEB PolEmo2.0-IN
      revision: d90724373c70959f17d2331ad51fb60c71176b03
      split: test
      type: PL-MTEB/polemo2_in
    metrics:
    - type: accuracy
      value: 45.581717451523545
    - type: f1
      value: 46.52096155620845
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB PolEmo2.0-OUT
      revision: 6a21ab8716e255ab1867265f8b396105e8aa63d4
      split: test
      type: PL-MTEB/polemo2_out
    metrics:
    - type: accuracy
      value: 18.643724696356273
    - type: f1
      value: 15.461041528691336
    task:
      type: Classification
  - dataset:
      config: default
      name: MTEB SICK-E-PL
      revision: 71bba34b0ece6c56dfcf46d9758a27f7a90f17e9
      split: test
      type: PL-MTEB/sicke-pl-pairclassification
    metrics: []
    task:
      type: PairClassification
  - dataset:
      config: default
      name: MTEB SICK-R-PL
      revision: fd5c2441b7eeff8676768036142af4cfa42c1339
      split: test
      type: PL-MTEB/sickr-pl-sts
    metrics: []
    task:
      type: STS
  - dataset:
      config: pl
      name: MTEB STS22 (pl)
      revision: de9d86b3b84231dc21f76c7b7af1f28e2f57f6e3
      split: test
      type: mteb/sts22-crosslingual-sts
    metrics: []
    task:
      type: STS
  - dataset:
      config: pl
      name: MTEB STSBenchmarkMultilingualSTS (pl)
      revision: 29afa2569dcedaaa2fe6a3dcfebab33d28b82e8c
      split: dev
      type: mteb/stsb_multi_mt
    metrics: []
    task:
      type: STS
  - dataset:
      config: pl
      name: MTEB STSBenchmarkMultilingualSTS (pl)
      revision: 29afa2569dcedaaa2fe6a3dcfebab33d28b82e8c
      split: test
      type: mteb/stsb_multi_mt
    metrics: []
    task:
      type: STS
pipeline_tag: sentence-similarity
tags:
- sentence-transformers
- sentence-similarity
- mteb
- feature-extraction
---
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/ILKT/2024-06-24_22-31-18_epoch_17> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/ILKT/2024-06-24_22-31-18_epoch_17/discussions> ;
    ns1:readme <https://huggingface.co/ILKT/2024-06-24_22-31-18_epoch_17/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/3ce8c80e76939926894342f3b717024c1ac4ec97dd6870163fc8fd016a588480> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "The context provided does not contain any information about ethical, legal, or social aspects. Therefore, the answer is \"Information not found\"."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/402c5284d04ce29828735d46f4b53890fbf3cfc9f7c900018e7bea70762dc667>,
        <http://mlentory.zbmed.de/mlentory_graph/daa54867c3884268e9ee1292bae85ade578214c2f224fae26233cc78919a1a20> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of DistilBERT base uncased finetuned SST-2 is for topic classification."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/85f0e69a70ce025a206d6ddbf78d989a3cabbdae215f2045df9b67b21a9d6780> ;
    ns2:modelCategory "Model Details"^^xsd:string ;
    ns2:modelRisks "Based on the provided context, the potential risks associated with the model are not explicitly stated. However, it does mention that the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model. Additionally, it mentions that the model could produce biased predictions that target underrepresented populations, as observed in experiments."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/4407cda96af0afeaf4c1c5aaacc2cddb4eccb07bfeb1a5dded4a788a19846707> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/402c5284d04ce29828735d46f4b53890fbf3cfc9f7c900018e7bea70762dc667>,
        <http://mlentory.zbmed.de/mlentory_graph/daa54867c3884268e9ee1292bae85ade578214c2f224fae26233cc78919a1a20> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/402c5284d04ce29828735d46f4b53890fbf3cfc9f7c900018e7bea70762dc667>,
        <http://mlentory.zbmed.de/mlentory_graph/daa54867c3884268e9ee1292bae85ade578214c2f224fae26233cc78919a1a20> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "The context provided does not contain any information about the validated On."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-12-19T16:29:37+00:00"^^xsd:dateTime ;
    schema:datePublished "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:description """---
language: en
license: apache-2.0
datasets:
- sst2
- glue
model-index:
- name: distilbert-base-uncased-finetuned-sst-2-english
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: glue
      type: glue
      config: sst2
      split: validation
    metrics:
    - type: accuracy
      value: 0.9105504587155964
      name: Accuracy
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg
    - type: precision
      value: 0.8978260869565218
      name: Precision
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA
    - type: recall
      value: 0.9301801801801802
      name: Recall
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ
    - type: auc
      value: 0.9716626673402374
      name: AUC
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ
    - type: f1
      value: 0.9137168141592922
      name: F1
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA
    - type: loss
      value: 0.39013850688934326
      name: loss
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: sst2
      type: sst2
      config: default
      split: train
    metrics:
    - type: accuracy
      value: 0.9885521685548412
      name: Accuracy
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA
    - type: precision
      value: 0.9881965062029833
      name: Precision Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw
    - type: precision
      value: 0.9885521685548412
      name: Precision Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ
    - type: precision
      value: 0.9885639626373408
      name: Precision Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg
    - type: recall
      value: 0.9886145346602994
      name: Recall Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA
    - type: recall
      value: 0.9885521685548412
      name: Recall Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ
    - type: recall
      value: 0.9885521685548412
      name: Recall Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw
    - type: f1
      value: 0.9884019815052447
      name: F1 Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ
    - type: f1
      value: 0.9885521685548412
      name: F1 Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ
    - type: f1
      value: 0.9885546181087554
      name: F1 Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA
    - type: loss
      value: 0.040652573108673096
      name: loss
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg
---

# DistilBERT base uncased finetuned SST-2

## Table of Contents
- [Model Details](#model-details)
- [How to Get Started With the Model](#how-to-get-started-with-the-model)
- [Uses](#uses)
- [Risks, Limitations and Biases](#risks-limitations-and-biases)
- [Training](#training)

## Model Details
**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.
This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).
- **Developed by:** Hugging Face
- **Model Type:** Text Classification
- **Language(s):** English
- **License:** Apache-2.0
- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).
- **Resources for more information:**
    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)
    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)

## How to Get Started With the Model

Example of single-label classification:
​​
```python
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]

```

## Uses

#### Direct Use

This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.

#### Misuse and Out-of-scope Use
The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.


## Risks, Limitations and Biases

Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.

For instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aurélien Géron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.

<img src="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg" alt="Map of positive probabilities per country." width="500"/>

We strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).



# Training


#### Training Data


The authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.

#### Training Procedure

###### Fine-tuning hyper-parameters


- learning_rate = 1e-5
- batch_size = 32
- warmup = 600
- max_seq_length = 128
- num_train_epochs = 3.0


"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/13e6e4095523310627190177e08c0206ca97e76f4c4565c15c2b35270daa2588>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/4e5443728f51b2323bf3a1c4a6a6d8612484cbca0a8b0beeb5fb0258d1df5196>,
        <http://mlentory.zbmed.de/mlentory_graph/7ad8435aa27d409e8692d8f6043742210150fe3cf420b9abaad32ffe8a63b7c7>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21>,
        <http://mlentory.zbmed.de/mlentory_graph/bc53f85f6e664d1c6ece992a9d80b2e0705d1b3c75028563adb2ed9f36aedec9>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "arxiv:1910.01108"^^xsd:string,
        "dataset:glue"^^xsd:string,
        "dataset:sst2"^^xsd:string,
        "doi:10.57967/hf/0181"^^xsd:string,
        "en"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string,
        "tf"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language: en
license: apache-2.0
datasets:
- sst2
- glue
model-index:
- name: distilbert-base-uncased-finetuned-sst-2-english
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: glue
      type: glue
      config: sst2
      split: validation
    metrics:
    - type: accuracy
      value: 0.9105504587155964
      name: Accuracy
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg
    - type: precision
      value: 0.8978260869565218
      name: Precision
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA
    - type: recall
      value: 0.9301801801801802
      name: Recall
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ
    - type: auc
      value: 0.9716626673402374
      name: AUC
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ
    - type: f1
      value: 0.9137168141592922
      name: F1
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA
    - type: loss
      value: 0.39013850688934326
      name: loss
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: sst2
      type: sst2
      config: default
      split: train
    metrics:
    - type: accuracy
      value: 0.9885521685548412
      name: Accuracy
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA
    - type: precision
      value: 0.9881965062029833
      name: Precision Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw
    - type: precision
      value: 0.9885521685548412
      name: Precision Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ
    - type: precision
      value: 0.9885639626373408
      name: Precision Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg
    - type: recall
      value: 0.9886145346602994
      name: Recall Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA
    - type: recall
      value: 0.9885521685548412
      name: Recall Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ
    - type: recall
      value: 0.9885521685548412
      name: Recall Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw
    - type: f1
      value: 0.9884019815052447
      name: F1 Macro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ
    - type: f1
      value: 0.9885521685548412
      name: F1 Micro
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ
    - type: f1
      value: 0.9885546181087554
      name: F1 Weighted
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA
    - type: loss
      value: 0.040652573108673096
      name: loss
      verified: true
      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg
---

# DistilBERT base uncased finetuned SST-2

## Table of Contents
- [Model Details](#model-details)
- [How to Get Started With the Model](#how-to-get-started-with-the-model)
- [Uses](#uses)
- [Risks, Limitations and Biases](#risks-limitations-and-biases)
- [Training](#training)

## Model Details
**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.
This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).
- **Developed by:** Hugging Face
- **Model Type:** Text Classification
- **Language(s):** English
- **License:** Apache-2.0
- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).
- **Resources for more information:**
    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)
    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)

## How to Get Started With the Model

Example of single-label classification:
​​
```python
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]

```

## Uses

#### Direct Use

This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.

#### Misuse and Out-of-scope Use
The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.


## Risks, Limitations and Biases

Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.

For instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aurélien Géron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.

<img src="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg" alt="Map of positive probabilities per country." width="500"/>

We strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).



# Training


#### Training Data


The authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.

#### Training Procedure

###### Fine-tuning hyper-parameters


- learning_rate = 1e-5
- batch_size = 32
- warmup = 600
- max_seq_length = 128
- num_train_epochs = 3.0


"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/discussions> ;
    ns1:readme <https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/96b96e1fc1370c6454f55ef5bb235e3108a8e1b32665ef0b72dd54ba220b3a53> .

<http://mlentory.zbmed.de/mlentory_graph/3db362f2d160db0ec9efef6e48b6ba6fc466784d4fdf38393a2818c9c974f93e> a schema:DefinedTerm ;
    schema:description "Deep learning library that simplifies training neural networks using modern best practices."^^xsd:string ;
    schema:name "fastai"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/3df48d276cc7907b46eaab79aee7aa3a9221afd335628ee051da496d12c30e67> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/034a881c083ce582a4998bf12129f65097a143b5a5e28119457461204aaa636d> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of nercomlower-bert-base-spanish-wwm-cased is not explicitly stated in the provided context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/cb9bd15fb27d24d93383343fa6ee37e644b0a41da399de954b0a5cf93c7b3501> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/020eb0d4713d856b7707c9066e6bedcd042487c7ce89143c25e68a5cf634de2f> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/034a881c083ce582a4998bf12129f65097a143b5a5e28119457461204aaa636d> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/034a881c083ce582a4998bf12129f65097a143b5a5e28119457461204aaa636d> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "The text does not provide information about the validated On."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/simonestradasch/nercomlower-bert-base-spanish-wwm-cased> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2023-07-11T01:30:38+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-07-11T01:40:59+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-07-11T01:30:38+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- es
tags:
- generated_from_trainer
datasets:
- simonestradasch/NERcomp2lower
model-index:
- name: nercomlower-bert-base-spanish-wwm-cased
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# nercomlower-bert-base-spanish-wwm-cased

This model is a fine-tuned version of [dccuchile/bert-base-spanish-wwm-cased](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) on the simonestradasch/NERcomp2lower dataset.
It achieves the following results on the evaluation set:
- Loss: 0.2448
- Body Part Precision: 0.7140
- Body Part Recall: 0.7676
- Body Part F1: 0.7398
- Body Part Number: 413
- Disease Precision: 0.7505
- Disease Recall: 0.7805
- Disease F1: 0.7652
- Disease Number: 975
- Family Member Precision: 0.875
- Family Member Recall: 0.9333
- Family Member F1: 0.9032
- Family Member Number: 30
- Medication Precision: 0.8764
- Medication Recall: 0.8387
- Medication F1: 0.8571
- Medication Number: 93
- Procedure Precision: 0.6571
- Procedure Recall: 0.6656
- Procedure F1: 0.6613
- Procedure Number: 311
- Overall Precision: 0.7344
- Overall Recall: 0.7634
- Overall F1: 0.7487
- Overall Accuracy: 0.9277

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 13
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss | Body Part Precision | Body Part Recall | Body Part F1 | Body Part Number | Disease Precision | Disease Recall | Disease F1 | Disease Number | Family Member Precision | Family Member Recall | Family Member F1 | Family Member Number | Medication Precision | Medication Recall | Medication F1 | Medication Number | Procedure Precision | Procedure Recall | Procedure F1 | Procedure Number | Overall Precision | Overall Recall | Overall F1 | Overall Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:-------------------:|:----------------:|:------------:|:----------------:|:-----------------:|:--------------:|:----------:|:--------------:|:-----------------------:|:--------------------:|:----------------:|:--------------------:|:--------------------:|:-----------------:|:-------------:|:-----------------:|:-------------------:|:----------------:|:------------:|:----------------:|:-----------------:|:--------------:|:----------:|:----------------:|
| 0.355         | 1.0   | 1004 | 0.2520          | 0.7073              | 0.8015           | 0.7514       | 413              | 0.7485            | 0.7477         | 0.7481     | 975            | 0.8710                  | 0.9                  | 0.8852           | 30                   | 0.7196               | 0.8280            | 0.77          | 93                | 0.5804              | 0.6270           | 0.6028       | 311              | 0.7093            | 0.7459         | 0.7271     | 0.9219           |
| 0.1869        | 2.0   | 2008 | 0.2448          | 0.7140              | 0.7676           | 0.7398       | 413              | 0.7505            | 0.7805         | 0.7652     | 975            | 0.875                   | 0.9333               | 0.9032           | 30                   | 0.8764               | 0.8387            | 0.8571        | 93                | 0.6571              | 0.6656           | 0.6613       | 311              | 0.7344            | 0.7634         | 0.7487     | 0.9277           |


### Framework versions

- Transformers 4.30.2
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/simonestradasch/nercomlower-bert-base-spanish-wwm-cased/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/simonestradasch/nercomlower-bert-base-spanish-wwm-cased"^^xsd:string ;
    schema:inLanguage "es"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0c3d01ed44dc2940ae38707146a02c16bc80cc8637ce48ac5f3125b7c6fb2982>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/461c5e3d5b0d3aa22f2cc8a7718ca67259b586f0e9f9c26fdf31874cab0e3a4f>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/974df5e3234c3601080dac4fa96eebf5d19d40df41e9e30cb61084d9fbefea1e>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "dataset:simonestradasch/NERcomp2lower"^^xsd:string,
        "es"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- es
tags:
- generated_from_trainer
datasets:
- simonestradasch/NERcomp2lower
model-index:
- name: nercomlower-bert-base-spanish-wwm-cased
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# nercomlower-bert-base-spanish-wwm-cased

This model is a fine-tuned version of [dccuchile/bert-base-spanish-wwm-cased](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) on the simonestradasch/NERcomp2lower dataset.
It achieves the following results on the evaluation set:
- Loss: 0.2448
- Body Part Precision: 0.7140
- Body Part Recall: 0.7676
- Body Part F1: 0.7398
- Body Part Number: 413
- Disease Precision: 0.7505
- Disease Recall: 0.7805
- Disease F1: 0.7652
- Disease Number: 975
- Family Member Precision: 0.875
- Family Member Recall: 0.9333
- Family Member F1: 0.9032
- Family Member Number: 30
- Medication Precision: 0.8764
- Medication Recall: 0.8387
- Medication F1: 0.8571
- Medication Number: 93
- Procedure Precision: 0.6571
- Procedure Recall: 0.6656
- Procedure F1: 0.6613
- Procedure Number: 311
- Overall Precision: 0.7344
- Overall Recall: 0.7634
- Overall F1: 0.7487
- Overall Accuracy: 0.9277

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 13
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss | Body Part Precision | Body Part Recall | Body Part F1 | Body Part Number | Disease Precision | Disease Recall | Disease F1 | Disease Number | Family Member Precision | Family Member Recall | Family Member F1 | Family Member Number | Medication Precision | Medication Recall | Medication F1 | Medication Number | Procedure Precision | Procedure Recall | Procedure F1 | Procedure Number | Overall Precision | Overall Recall | Overall F1 | Overall Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:-------------------:|:----------------:|:------------:|:----------------:|:-----------------:|:--------------:|:----------:|:--------------:|:-----------------------:|:--------------------:|:----------------:|:--------------------:|:--------------------:|:-----------------:|:-------------:|:-----------------:|:-------------------:|:----------------:|:------------:|:----------------:|:-----------------:|:--------------:|:----------:|:----------------:|
| 0.355         | 1.0   | 1004 | 0.2520          | 0.7073              | 0.8015           | 0.7514       | 413              | 0.7485            | 0.7477         | 0.7481     | 975            | 0.8710                  | 0.9                  | 0.8852           | 30                   | 0.7196               | 0.8280            | 0.77          | 93                | 0.5804              | 0.6270           | 0.6028       | 311              | 0.7093            | 0.7459         | 0.7271     | 0.9219           |
| 0.1869        | 2.0   | 2008 | 0.2448          | 0.7140              | 0.7676           | 0.7398       | 413              | 0.7505            | 0.7805         | 0.7652     | 975            | 0.875                   | 0.9333               | 0.9032           | 30                   | 0.8764               | 0.8387            | 0.8571        | 93                | 0.6571              | 0.6656           | 0.6613       | 311              | 0.7344            | 0.7634         | 0.7487     | 0.9277           |


### Framework versions

- Transformers 4.30.2
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/simonestradasch/nercomlower-bert-base-spanish-wwm-cased> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/simonestradasch/nercomlower-bert-base-spanish-wwm-cased/discussions> ;
    ns1:readme <https://huggingface.co/simonestradasch/nercomlower-bert-base-spanish-wwm-cased/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/40e5b840897abb6f44300acdb8df19e9ba787840ae3899b5cff54ee848bd4df6> a schema:DefinedTerm ;
    schema:description "Predicts the depth of objects in images for 3D scene understanding."^^xsd:string ;
    schema:name "Depth Estimation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/427579e1091b3a9a3bf55b54c4167319ee1e7fd37d7a1389722fcb73d6a8901c> a schema:DefinedTerm ;
    schema:description "Classifies images into categories not seen during training."^^xsd:string ;
    schema:name "Zero Shot Image Classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/435f6e89d90ad4bd6be870aa7e1a66a9ce6bb4c201cdc1adf4aec7db47ff1148> a schema:DefinedTerm ;
    schema:description "Generates audio content from textual descriptions."^^xsd:string ;
    schema:name "Text to Audio"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/43625c04f736f13b1f357a0d9357ed3dbc8e216e352796b03fb0fa69aea1f93b> a schema:DefinedTerm ;
    schema:description "Models based on XLM-RoBERTa, a cross-lingual version of RoBERTa pretrained on text from 100 languages for multilingual understanding."^^xsd:string ;
    schema:name "xlm-roberta"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/45ef55138ac28324915f59dee5c5614894be22d29e2a8283b76835bcb91e3e87> a schema:DefinedTerm ;
    schema:description "Answers natural language questions about images."^^xsd:string ;
    schema:name "Visual Question Answering"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/46fd5133284e3f6058330c31c628a6854b5eeb608d471e58c7052d9365f5acf4> a schema:DefinedTerm ;
    schema:description "Extracts meaningful features from images for downstream tasks."^^xsd:string ;
    schema:name "Image Feature Extraction"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/48572c53d6fcb57f374c750375f91bc885a37864711f13f93b9e1c6813bb99d9> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/3c14fb791ef79e48f95d78b8f9f8e233e1773421508cb537054e54ee68fac0fe> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """Information not found.

Context:
TinyPoliticaLlama-1.1B: TinyPoliticaLlama-1.1B is a SFT fine-tune of [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) and a sister model of [h4rz3rk4s3/TinyParlaMintLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyParlaMintLlama-1.1B) and [h4rz3rk4s3/TinyNewsLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyNewsLlama-1.1B).
This model was fine-tuned for ~24h on one A100 40GB on ~225M tokens on the training corpora of both her sister models.
The goal of this project is to study the potential for improving the domain-specific (in this case political) knowledge of small (<3B) LLMs by concentrating the training datasets TF-IDF in respect to the underlying Topics found in the origianl Dataset.
The used training data contains speeches from the **Austrian**, **Danish**, **French**, **British**, **Hungarian**, **Dutch**, **Norwegian**, **Polish**, **Swedish** and **Turkish** Parliament, as well as political news articles from **The New York Times**, **USA Today** and **The Washington Times**.
TinyPoliticaLlama-1.1B > 💻 Usage - Par. 1: TinyPoliticaLlama-1.1B > 💻 Usage:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
TinyPoliticaLlama-1.1B: TinyPoliticaLlama-1.1B is a SFT fine-tune of [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) and a sister model of [h4rz3rk4s3/TinyParlaMintLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyParlaMintLlama-1.1B) and [h4rz3rk4s3/TinyNewsLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyNewsLlama-1.1B).
This model was fine-tuned for ~24h on one A100 40GB on ~225M tokens on the training corpora of both her sister models.
The goal of this project is to study the potential for improving the domain-specific (in this case political) knowledge of small (<3B) LLMs by concentrating the training datasets TF-IDF in respect to the underlying Topics found in the origianl Dataset.
The used training data contains speeches from the **Austrian**, **Danish**, **French**, **British**, **Hungarian**, **Dutch**, **Norwegian**, **Polish**, **Swedish** and **Turkish** Parliament, as well as political news articles from **The New York Times**, **USA Today** and **The Washington Times**.
TinyPoliticaLlama-1.1B > 💻 Usage - Par. 1: TinyPoliticaLlama-1.1B > 💻 Usage:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
TinyPoliticaLlama-1.1B: TinyPoliticaLlama-1.1B is a SFT fine-tune of [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) and a sister model of [h4rz3rk4s3/TinyParlaMintLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyParlaMintLlama-1.1B) and [h4rz3rk4s3/TinyNewsLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyNewsLlama-1.1B).
This model was fine-tuned for ~24h on one A100 40GB on ~225M tokens on the training corpora of both her sister models.
The goal of this project is to study the potential for improving the domain-specific (in this case political) knowledge of small (<3B) LLMs by concentrating the training datasets TF-IDF in respect to the underlying Topics found in the origianl Dataset.
The used training data contains speeches from the **Austrian**, **Danish**, **French**, **British**, **Hungarian**, **Dutch**, **Norwegian**, **Polish**, **Swedish** and **Turkish** Parliament, as well as political news articles from **The New York Times**, **USA Today** and **The Washington Times**.
TinyPoliticaLlama-1.1B > 💻 Usage - Par. 1: TinyPoliticaLlama-1.1B > 💻 Usage:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
TinyPoliticaLlama-1.1B: TinyPoliticaLlama-1.1B is a SFT fine-tune of [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) and a sister model of [h4rz3rk4s3/TinyParlaMintLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyParlaMintLlama-1.1B) and [h4rz3rk4s3/TinyNewsLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyNewsLlama-1.1B).
This model was fine-tuned for ~24h on one A100 40GB on ~225M tokens on the training corpora of both her sister models.
The goal of this project is to study the potential for improving the domain-specific (in this case political) knowledge of small (<3B) LLMs by concentrating the training datasets TF-IDF in respect to the underlying Topics found in the origianl Dataset.
The used training data contains speeches from the **Austrian**, **Danish**, **French**, **British**, **Hungarian**, **Dutch**, **Norwegian**, **Polish**, **Swedish** and **Turkish** Parliament, as well as political news articles from **The New York Times**, **USA Today** and **The Washington Times**.
TinyPoliticaLlama-1.1B > 💻 Usage - Par. 1: TinyPoliticaLlama-1.1B > 💻 Usage:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
TinyPoliticaLlama-1.1B: TinyPolitica"""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The context provided does not contain information about the model category."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/a78f959c215edcbfd7197d95d2f6cbb591f9e010a628c097ae2631a706e27277> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "The usage instructions for TinyPoliticaLlama-1.1B are not explicitly mentioned in the provided context. However, it does mention that the model was fine-tuned for ~24h on one A100 40GB on ~225M tokens on the training corpora of both her sister models. The goal of this project is to study the potential for improving the domain-specific (in this case political) knowledge of small (<3B) LLMs by concentrating the training datasets TF-IDF in respect to the underlying Topics found in the original Dataset. The used training data contains speeches from the Austrian, Danish, French, British, Hungarian, Dutch, Norwegian, Polish, Swedish and Turkish Parliament, as well as political news articles from The New York Times, USA Today and The Washington Times."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/h4rz3rk4s3/TinyPoliticaLlama-1.1B> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-03-08T14:53:34+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-03-08T14:57:34+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-03-08T14:53:34+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
tags:
- TinyLlama
- QLoRA
- Politics
- EU
- News
- sft
base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
---

# TinyPoliticaLlama-1.1B

TinyPoliticaLlama-1.1B is a SFT fine-tune of [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) and a sister model of [h4rz3rk4s3/TinyParlaMintLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyParlaMintLlama-1.1B) and [h4rz3rk4s3/TinyNewsLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyNewsLlama-1.1B).
This model was fine-tuned for ~24h on one A100 40GB on ~225M tokens on the training corpora of both her sister models.

The goal of this project is to study the potential for improving the domain-specific (in this case political) knowledge of small (<3B) LLMs by concentrating the training datasets TF-IDF in respect to the underlying Topics found in the origianl Dataset.

The used training data contains speeches from the **Austrian**, **Danish**, **French**, **British**, **Hungarian**, **Dutch**, **Norwegian**, **Polish**, **Swedish** and **Turkish** Parliament, as well as political news articles from **The New York Times**, **USA Today** and **The Washington Times**.


## 💻 Usage

```python
!pip install -qU transformers accelerate
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator
import transformers
import torch
model = "h4rz3rk4s3/TinyPoliticaLlama-1.1B"
messages = [
    {
        "role": "system",
        "content": "You are an experienced journalist in the political domain and an expert of European politics.",
    },
    {"role": "user", "content": "Write a short article explaining how the French yellow vest protests started, how they developed over time and how the French Government reacted to the protests."},
]

tokenizer = AutoTokenizer.from_pretrained(model)
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model = AutoModelForCausalLM.from_pretrained(
    model, trust_remote_code=True, device_map={"": Accelerator().process_index}
)

pipeline = transformers.pipeline(
    "text-generation",
    tokenizer=tokenizer,
    model=model,
    torch_dtype=torch.float16,
    device_map={"": Accelerator().process_index},
)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/h4rz3rk4s3/TinyPoliticaLlama-1.1B/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/h4rz3rk4s3/TinyPoliticaLlama-1.1B"^^xsd:string ;
    schema:inLanguage "EU"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0437d97d6f71569092a72cc3c2b5ce5cc4751919c8ab349cb684ab8c6a9cbb5d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/7a3700557a9a837d2acbe1b2901694ff05af496b82967766e041c3ca88374601>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/83d1ebc775fee7fb0a812e8dca395fdcac7c0fe605940807d90a6d698dd912b8>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/e2bd5001a46246a7e8474accffaa2931c529f86701173c10c48e9befe9197517>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        <http://mlentory.zbmed.de/mlentory_graph/fbc5ef54567df2b9fee883b665a53480149a7458fe3709e106706905b8466e2c>,
        "EU"^^xsd:string,
        "base_model:TinyLlama/TinyLlama-1.1B-Chat-v1.0"^^xsd:string,
        "base_model:finetune:TinyLlama/TinyLlama-1.1B-Chat-v1.0"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "The maintainer of TinyPoliticaLlama-1.1B is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
tags:
- TinyLlama
- QLoRA
- Politics
- EU
- News
- sft
base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
---

# TinyPoliticaLlama-1.1B

TinyPoliticaLlama-1.1B is a SFT fine-tune of [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) and a sister model of [h4rz3rk4s3/TinyParlaMintLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyParlaMintLlama-1.1B) and [h4rz3rk4s3/TinyNewsLlama-1.1B](https://huggingface.co/h4rz3rk4s3/TinyNewsLlama-1.1B).
This model was fine-tuned for ~24h on one A100 40GB on ~225M tokens on the training corpora of both her sister models.

The goal of this project is to study the potential for improving the domain-specific (in this case political) knowledge of small (<3B) LLMs by concentrating the training datasets TF-IDF in respect to the underlying Topics found in the origianl Dataset.

The used training data contains speeches from the **Austrian**, **Danish**, **French**, **British**, **Hungarian**, **Dutch**, **Norwegian**, **Polish**, **Swedish** and **Turkish** Parliament, as well as political news articles from **The New York Times**, **USA Today** and **The Washington Times**.


## 💻 Usage

```python
!pip install -qU transformers accelerate
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator
import transformers
import torch
model = "h4rz3rk4s3/TinyPoliticaLlama-1.1B"
messages = [
    {
        "role": "system",
        "content": "You are an experienced journalist in the political domain and an expert of European politics.",
    },
    {"role": "user", "content": "Write a short article explaining how the French yellow vest protests started, how they developed over time and how the French Government reacted to the protests."},
]

tokenizer = AutoTokenizer.from_pretrained(model)
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model = AutoModelForCausalLM.from_pretrained(
    model, trust_remote_code=True, device_map={"": Accelerator().process_index}
)

pipeline = transformers.pipeline(
    "text-generation",
    tokenizer=tokenizer,
    model=model,
    torch_dtype=torch.float16,
    device_map={"": Accelerator().process_index},
)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/h4rz3rk4s3/TinyPoliticaLlama-1.1B> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/h4rz3rk4s3/TinyPoliticaLlama-1.1B/discussions> ;
    ns1:readme <https://huggingface.co/h4rz3rk4s3/TinyPoliticaLlama-1.1B/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/4acb0244517b271c7719eadb638a9049f7dfa43a14bf6eb119d83c2dbc6d856f> a schema:DefinedTerm ;
    schema:description "Sequence modeling toolkit for training custom models for translation, summarization, and other text generation tasks."^^xsd:string ;
    schema:name "Fairseq"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/4b38b1aea7963feedd990e2bb7e533146682962e27838393f9a2859b371ec148> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/527f8fccacd13d63f02189d2818818a852504fa8cd1b02edcdc062cb6ee12c5d> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended uses and limitations of full_vanilla_doff_dpo_iter_1 are not explicitly stated in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/c372f0bca81211b35cd0974aabe8500d5ca4479cddab1685b44f24bd5523dc9e> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess """Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_1 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections"""^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-07-03T14:46:51+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-07-03T17:24:25+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-07-03T14:46:51+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model: alignment-handbook/zephyr-7b-sft-full
tags:
- alignment-handbook
- generated_from_trainer
- trl
- dpo
datasets:
- updated
- original
model-index:
- name: full_vanilla_doff_dpo_iter_1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# full_vanilla_doff_dpo_iter_1

This model is a fine-tuned version of [alignment-handbook/zephyr-7b-sft-full](https://huggingface.co/alignment-handbook/zephyr-7b-sft-full) on the updated and the original datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2+cu121
- Datasets 2.14.6
- Tokenizers 0.15.2
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/267e04ff20e54a667a0c17e69b7dd3f07b14ad451d8a605d3b5b771693ba73ba>,
        <http://mlentory.zbmed.de/mlentory_graph/28f6fc79227e13d54a24f63fe2a8971b6f281ac3ca778d4303f84baacbc18a18>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/89b5833251f8f95309bb1d81c7cbd5e686d9f1e1a94b81869c2661ff9301fb64>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:alignment-handbook/zephyr-7b-sft-full"^^xsd:string,
        "base_model:finetune:alignment-handbook/zephyr-7b-sft-full"^^xsd:string,
        "dataset:original"^^xsd:string,
        "dataset:updated"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model: alignment-handbook/zephyr-7b-sft-full
tags:
- alignment-handbook
- generated_from_trainer
- trl
- dpo
datasets:
- updated
- original
model-index:
- name: full_vanilla_doff_dpo_iter_1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# full_vanilla_doff_dpo_iter_1

This model is a fine-tuned version of [alignment-handbook/zephyr-7b-sft-full](https://huggingface.co/alignment-handbook/zephyr-7b-sft-full) on the updated and the original datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2+cu121
- Datasets 2.14.6
- Tokenizers 0.15.2
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1/discussions> ;
    ns1:readme <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/4eaeba2a8985fdb21e18ca1e967f067a345c0498ed10fc3e5e9ba95e39876cab> a schema:DefinedTerm ;
    schema:description "Categorizes rows in tabular data into predefined classes."^^xsd:string ;
    schema:name "Tabular Classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5294100683a6737ae3cdcdb512f4b8459993c27fd8f86917ce5e595178c28f6d> a schema:DefinedTerm ;
    schema:description "Extracts answers from documents based on questions."^^xsd:string ;
    schema:name "Document Question Answering"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/55f0c4818716a5c86a051b41b27450e6f00f7e81d8a89b0f3bd6928719ce08e4> a schema:DefinedTerm ;
    schema:description "Library for lightweight pipelining in Python, used for saving/loading Python objects and parallelizing loops."^^xsd:string ;
    schema:name "Joblib"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5670ef7e87a4fbee3fa95024561f672fca6cc6a159a62e20ed73b98d267a72bd> a schema:DefinedTerm ;
    schema:description "JavaScript library for running transformer models in browsers and Node.js environments."^^xsd:string ;
    schema:name "Transformers.js"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/589eadf6e68dee6f27b8675b1b4667c3e0a99e6bc28677420c40fbb41312854b> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/7b94abd1a94f42759242e24f381b59455f00250bb42fe5e8479187485a809d0f> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended"""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/85fa58f915b932926d1bb143b0bca80547964254fc92a39517d34e5b7250f9bd> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "The model card for Mistral-7B-Instruct-v0.3 does not provide specific usage instructions. However, it does mention that the model is a quick demonstration of how the base model can be fine-tuned for compelling performance. It also highlights the lack of moderation mechanisms and the community's interest in developing ways to make the model respect guardrails for deployment in moderated environments. The model card does not provide any direct usage instructions."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/grimjim/mistralai-Mistral-7B-Instruct-v0.3> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-05-23T20:01:31+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-10-21T16:42:58+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-23T20:01:31+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model: mistralai/Mistral-7B-Instruct-v0.3
pipeline_tag: text-generation
library_name: transformers
---

# Model Card for Mistral-7B-Instruct-v0.3

The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)
- Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling

## Installation

It is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-Instruct-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Chat

After installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using

```
mistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256
```

### Instruct following

```py
from mistral_inference.model import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(messages=[UserMessage(content="Explain Machine Learning to me in a nutshell.")])

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

### Function calling

```py
from mistral_common.protocol.instruct.tool_calls import Function, Tool
from mistral_inference.model import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(
    tools=[
        Tool(
            function=Function(
                name="get_current_weather",
                description="Get the current weather",
                parameters={
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "The temperature unit to use. Infer this from the users location.",
                        },
                    },
                    "required": ["location", "format"],
                },
            )
        )
    ],
    messages=[
        UserMessage(content="What's the weather like today in Paris?"),
        ],
)

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import pipeline

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]
chatbot = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.3")
chatbot(messages)
```

## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/grimjim/mistralai-Mistral-7B-Instruct-v0.3/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/grimjim/mistralai-Mistral-7B-Instruct-v0.3"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:finetune:mistralai/Mistral-7B-Instruct-v0.3"^^xsd:string,
        "base_model:mistralai/Mistral-7B-Instruct-v0.3"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the Mistral-7B-Instruct-v0.3 model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The text does not provide information about the processor requirements for running the Mistral-7B-Instruct-v0.3 model."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model: mistralai/Mistral-7B-Instruct-v0.3
pipeline_tag: text-generation
library_name: transformers
---

# Model Card for Mistral-7B-Instruct-v0.3

The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)
- Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling

## Installation

It is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-Instruct-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Chat

After installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using

```
mistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256
```

### Instruct following

```py
from mistral_inference.model import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(messages=[UserMessage(content="Explain Machine Learning to me in a nutshell.")])

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

### Function calling

```py
from mistral_common.protocol.instruct.tool_calls import Function, Tool
from mistral_inference.model import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(
    tools=[
        Tool(
            function=Function(
                name="get_current_weather",
                description="Get the current weather",
                parameters={
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "The temperature unit to use. Infer this from the users location.",
                        },
                    },
                    "required": ["location", "format"],
                },
            )
        )
    ],
    messages=[
        UserMessage(content="What's the weather like today in Paris?"),
        ],
)

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import pipeline

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]
chatbot = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.3")
chatbot(messages)
```

## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "The text does not provide specific software requirements for the Mistral-7B-Instruct-v0.3 model. It only mentions that the model is a quick demonstration of the base model being easily fine-tuned for compelling performance and does not have moderation mechanisms. It also mentions the intention to engage with the community on ways to make the model respect guardrails for deployment in moderated environments."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/grimjim/mistralai-Mistral-7B-Instruct-v0.3> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/grimjim/mistralai-Mistral-7B-Instruct-v0.3/discussions> ;
    ns1:readme <https://huggingface.co/grimjim/mistralai-Mistral-7B-Instruct-v0.3/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5931df18dd0fa4a7991df9c03e52fe5074eee570f6c3e7b480947ec85d10c3a1> a schema:DefinedTerm ;
    schema:description "Toolkit for optimizing and deploying deep learning models across Intel hardware."^^xsd:string ;
    schema:name "OpenVINO"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/59b663660275b892172a573e140e5eeec1a75e54c1838aea8d6f78d0f6286eba> a schema:DefinedTerm ;
    schema:description "Models that can be fine-tuned using HuggingFace's AutoTrain service without requiring manual code."^^xsd:string ;
    schema:name "AutoTrain Compatible"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5c6c7a9e81be05e126c2ff79f262747bcaa13dce4de3db78679e947f541d5911> a schema:DefinedTerm ;
    schema:description "Industrial-strength NLP library with pre-trained models and support for 65+ languages."^^xsd:string ;
    schema:name "spaCy"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5c8c2755212903689b56f39d7ba199e4d9118ce211f93686db5b92f68ec68dc1> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found"^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found"^^xsd:string ;
    ns2:intendedUse "The intended use of the **PPO** agent playing **LunarLander-v2** is not explicitly stated in the given context. However, it can be inferred that the agent is being used for training purposes, as it is mentioned that it is a trained model. The agent is likely being used to demonstrate the capabilities of the stable-baselines3 library and to showcase the effectiveness of the **PPO** algorithm in solving the **LunarLander-v2** environment."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/3b4b981961ee604488ce9db2b6cea45a4fb777fb6ee8ef76bb274e14ff6cbdf4> ;
    ns2:modelCategory "Information not found"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/54ded1dc61acf1026f0859f850cee38bfbf5da27e35bcd3a46458a7b2e81985f> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/mlewinski/ppo-LunarLander-v2> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found"^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-02-09T21:41:54+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-02-09T22:14:11+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-02-09T21:41:54+00:00"^^xsd:dateTime ;
    schema:description """---
library_name: stable-baselines3
tags:
- LunarLander-v2
- deep-reinforcement-learning
- reinforcement-learning
- stable-baselines3
model-index:
- name: PPO
  results:
  - task:
      type: reinforcement-learning
      name: reinforcement-learning
    dataset:
      name: LunarLander-v2
      type: LunarLander-v2
    metrics:
    - type: mean_reward
      value: 251.95 +/- 13.83
      name: mean_reward
      verified: false
---

# **PPO** Agent playing **LunarLander-v2**
This is a trained model of a **PPO** agent playing **LunarLander-v2**
using the [stable-baselines3 library](https://github.com/DLR-RM/stable-baselines3).

## Usage (with Stable-baselines3)
TODO: Add your code


```python
from stable_baselines3 import ...
from huggingface_sb3 import load_from_hub

...
```
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/mlewinski/ppo-LunarLander-v2/discussions> ;
    schema:distribution "Information not found"^^xsd:string ;
    schema:funding "Information not found"^^xsd:string ;
    schema:identifier "https://huggingface.co/mlewinski/ppo-LunarLander-v2"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/13e6e4095523310627190177e08c0206ca97e76f4c4565c15c2b35270daa2588>,
        <http://mlentory.zbmed.de/mlentory_graph/2529572666d8ae917752f418f0b008b820cf821a3ba355927ca159454740dce7>,
        <http://mlentory.zbmed.de/mlentory_graph/594a6f3cf66106516dc4e80f44dbe528cdcf0fb397d75a65d1b3b2a81dfc0f92>,
        <http://mlentory.zbmed.de/mlentory_graph/8c4a8f327d1fd4a4e809a099e5f7d5a743e4a5c0883b7dbd24a235b2d281ba8e>,
        <http://mlentory.zbmed.de/mlentory_graph/a3c9ca93ab7ec29d067dd25733037f174c1326ef487292e5e1c9eea5cb35feb1>,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "Information not found"^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found"^^xsd:string ;
    schema:releaseNotes """---
library_name: stable-baselines3
tags:
- LunarLander-v2
- deep-reinforcement-learning
- reinforcement-learning
- stable-baselines3
model-index:
- name: PPO
  results:
  - task:
      type: reinforcement-learning
      name: reinforcement-learning
    dataset:
      name: LunarLander-v2
      type: LunarLander-v2
    metrics:
    - type: mean_reward
      value: 251.95 +/- 13.83
      name: mean_reward
      verified: false
---

# **PPO** Agent playing **LunarLander-v2**
This is a trained model of a **PPO** agent playing **LunarLander-v2**
using the [stable-baselines3 library](https://github.com/DLR-RM/stable-baselines3).

## Usage (with Stable-baselines3)
TODO: Add your code


```python
from stable_baselines3 import ...
from huggingface_sb3 import load_from_hub

...
```
"""^^xsd:string ;
    schema:softwareHelp "Information not found"^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found"^^xsd:string ;
    schema:url <https://huggingface.co/mlewinski/ppo-LunarLander-v2> ;
    schema:version "Information not found"^^xsd:string ;
    ns1:buildInstructions """Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3) - Par. 1: **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
**PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3): **PPO** Agent playing **LunarLander-v2** > Usage (with Stable-baselines3):
TODO: Add your code
```python
from stable_baselines3 import...
from huggingface_sb3 import load_from_hub
...
```

Question: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions/documentation.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If"""^^xsd:string ;
    ns1:developmentStatus "Information not found"^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/mlewinski/ppo-LunarLander-v2/discussions> ;
    ns1:readme <https://huggingface.co/mlewinski/ppo-LunarLander-v2/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5c990ca2a5e70f08415c0f4172f510fcd684a2eb231b7fb731c1381dddcfdd76> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Model Details"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/40c6f438362f1a92103a016e23ec429f4b4dc082c4d0b6b7f94f07db46fe1706> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-4bits> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "The copyright holder is not mentioned in the provided context."^^xsd:string ;
    schema:dateCreated "2024-05-02T00:58:20+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-05-02T00:59:37+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-02T00:58:20+00:00"^^xsd:dateTime ;
    schema:description """---
{}
---
Quantization made by Richard Erkhov.

[Github](https://github.com/RichardErkhov)

[Discord](https://discord.gg/pvy7H8DZMG)

[Request more models](https://github.com/RichardErkhov/quant_request)


TinyLlama-1.1B-Chat-v1.0 - bnb 4bits
- Model creator: https://huggingface.co/TinyLlama/
- Original model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/




Original model description:
---
license: apache-2.0
datasets:
- cerebras/SlimPajama-627B
- bigcode/starcoderdata
- HuggingFaceH4/ultrachat_200k
- HuggingFaceH4/ultrafeedback_binarized
language:
- en
widget:
  - example_title: Fibonacci (Python)
    messages:
    - role: system
      content: You are a chatbot who can help code!
    - role: user
      content: Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.
---
<div align="center">

# TinyLlama-1.1B
</div>

https://github.com/jzhang38/TinyLlama

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. 


We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### This Model
This is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was " initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. 
We then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4." 


#### How to use
You will need the transformers>=4.34
Do check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.

```python
# Install transformers from source - only needed for versions <= v4.34
# pip install git+https://github.com/huggingface/transformers.git
# pip install accelerate

import torch
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")

# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
# <|system|>
# You are a friendly chatbot who always responds in the style of a pirate.</s>
# <|user|>
# How many helicopters can a human eat in one sitting?</s>
# <|assistant|>
# ...
```

"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-4bits/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-4bits"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0813ac63520e9dbe708eec9898230a9dbdb1c48971b07fa1ad53fb6dfaa7a564>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/2976f620ce615218c4caaaa64d292b6c1cc2785cda2fe2fcc1fabefc0c73faef>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "The maintainer of TinyLlama-1.1B is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
{}
---
Quantization made by Richard Erkhov.

[Github](https://github.com/RichardErkhov)

[Discord](https://discord.gg/pvy7H8DZMG)

[Request more models](https://github.com/RichardErkhov/quant_request)


TinyLlama-1.1B-Chat-v1.0 - bnb 4bits
- Model creator: https://huggingface.co/TinyLlama/
- Original model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/




Original model description:
---
license: apache-2.0
datasets:
- cerebras/SlimPajama-627B
- bigcode/starcoderdata
- HuggingFaceH4/ultrachat_200k
- HuggingFaceH4/ultrafeedback_binarized
language:
- en
widget:
  - example_title: Fibonacci (Python)
    messages:
    - role: system
      content: You are a chatbot who can help code!
    - role: user
      content: Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.
---
<div align="center">

# TinyLlama-1.1B
</div>

https://github.com/jzhang38/TinyLlama

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. 


We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### This Model
This is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was " initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. 
We then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4." 


#### How to use
You will need the transformers>=4.34
Do check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.

```python
# Install transformers from source - only needed for versions <= v4.34
# pip install git+https://github.com/huggingface/transformers.git
# pip install accelerate

import torch
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")

# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
# <|system|>
# You are a friendly chatbot who always responds in the style of a pirate.</s>
# <|user|>
# How many helicopters can a human eat in one sitting?</s>
# <|assistant|>
# ...
```

"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements """The software requirements mentioned in the context are:

1. transformers>=4.34
2. Check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information."""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-4bits> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-4bits/discussions> ;
    ns1:readme <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-4bits/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5e9f8dc5efdf686d003f27fc61cf3a0b9e5dbed094499e8e3332f5c535d686f3> a schema:DefinedTerm ;
    schema:description "End-to-end speech processing toolkit covering ASR, TTS, speech translation, and speech enhancement."^^xsd:string ;
    schema:name "ESPnet"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6803a9142550ca466078b0794746397f37034f277ce6f45bb7f252be171ab407> a schema:DefinedTerm ;
    schema:description "Creates images without specific text prompts or conditions."^^xsd:string ;
    schema:name "Unconditional Image Generation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6839f153ace4a5cfe1e9a1cdb43c1bfc2260f4c85be0608af5ec84a75a205d2c> a schema:DefinedTerm ;
    schema:description "Converts written text into spoken audio."^^xsd:string ;
    schema:name "Text to Speech"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6b2cb1c31bfc0737f9a78d4dacc6f6619634e3ff69aa403db1d2d5df0cc6302d> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the model is for generating photorealistic images of decent quality, with the ability to produce both sfw and nsfw images. The model is still in the training phase and may contain artifacts and perform poorly in some cases. The model is recommended for use with specific negative prompts and generation parameters, and can be supported through Boosty. The model is not intended for use in any out-of-scope or downstream uses."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/11b7c117f8cd33c560f4862966654c32eb570fe472aa173c6a54749d61d20636> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "The usage instructions for the model are not explicitly mentioned in the provided context. However, the context does mention that the model is aimed at photorealism and can produce sfw and nsfw images of decent quality. It also provides recommended negative prompts and generation parameters for the model."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/SG161222/RealVisXL_V2.01> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "The model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases."^^xsd:string ;
    schema:contributor "The model card authors are not mentioned in the provided context."^^xsd:string ;
    schema:copyrightHolder "The copyright holder is not mentioned in the provided context."^^xsd:string ;
    schema:dateCreated "2023-12-18T15:48:19+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-10-08T16:35:47+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-12-18T15:48:19+00:00"^^xsd:dateTime ;
    schema:description """---
license: openrail++
---
<strong>Check my exclusive models on Mage: </strong><a href="https://www.mage.space/play/4371756b27bf52e7a1146dc6fe2d969c" rel="noopener noreferrer nofollow"><strong>ParagonXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/df67a9f27f19629a98cb0fb619d1949a" rel="noopener noreferrer nofollow"><strong>NovaXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/d8db06ae964310acb4e090eec03984df" rel="noopener noreferrer nofollow"><strong>NovaXL Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/541da1e10976ab82976a5cacc770a413" rel="noopener noreferrer nofollow"><strong>NovaXL V2</strong></a><strong> / </strong><a href="https://www.mage.space/play/a56d2680c464ef25b8c66df126b3f706" rel="noopener noreferrer nofollow"><strong>NovaXL Pony</strong></a><strong> / </strong><a href="https://www.mage.space/play/b0ab6733c3be2408c93523d57a605371" rel="noopener noreferrer nofollow"><strong>NovaXL Pony Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/e3b01cd493ed86ed8e4708751b1c9165" rel="noopener noreferrer nofollow"><strong>RealDreamXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/ef062fc389c3f8723002428290c1158c" rel="noopener noreferrer nofollow"><strong>RealDreamXL Lightning</strong></a></p>
<b>It's important! Read it!</b><br>
The model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.<br>

<b>You can support me directly on Boosty - https://boosty.to/sg_161222</b><br>

The model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.<br>
CivitAI Page: ---<br>

<b>Recommended Negative Prompt:</b><br>
(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth<br>
<b>or another negative prompt</b><br>

<b>Recommended Generation Parameters:</b><br>
Sampling Steps: 7+<br>
Sampling Method: DPM++ SDE Karras<br>
CFG Scale: 1.5-3

<b>Recommended Hires Fix Parameters:</b><br>
Hires steps: 10+<br>
Upscaler: 4x-UltraSharp upscaler / or another<br>
Denoising strength: 0.1 - 0.5<br>
Upscale by: 1.1-2.0<br>"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/SG161222/RealVisXL_V2.01/discussions> ;
    schema:distribution "The context provided does not contain any information about the distribution of the model."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/SG161222/RealVisXL_V2.01"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9>,
        "diffusers:StableDiffusionXLPipeline"^^xsd:string,
        "license:openrail++"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "openrail++"^^xsd:string ;
    schema:maintainer "The maintainer of the model is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: openrail++
---
<strong>Check my exclusive models on Mage: </strong><a href="https://www.mage.space/play/4371756b27bf52e7a1146dc6fe2d969c" rel="noopener noreferrer nofollow"><strong>ParagonXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/df67a9f27f19629a98cb0fb619d1949a" rel="noopener noreferrer nofollow"><strong>NovaXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/d8db06ae964310acb4e090eec03984df" rel="noopener noreferrer nofollow"><strong>NovaXL Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/541da1e10976ab82976a5cacc770a413" rel="noopener noreferrer nofollow"><strong>NovaXL V2</strong></a><strong> / </strong><a href="https://www.mage.space/play/a56d2680c464ef25b8c66df126b3f706" rel="noopener noreferrer nofollow"><strong>NovaXL Pony</strong></a><strong> / </strong><a href="https://www.mage.space/play/b0ab6733c3be2408c93523d57a605371" rel="noopener noreferrer nofollow"><strong>NovaXL Pony Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/e3b01cd493ed86ed8e4708751b1c9165" rel="noopener noreferrer nofollow"><strong>RealDreamXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/ef062fc389c3f8723002428290c1158c" rel="noopener noreferrer nofollow"><strong>RealDreamXL Lightning</strong></a></p>
<b>It's important! Read it!</b><br>
The model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.<br>

<b>You can support me directly on Boosty - https://boosty.to/sg_161222</b><br>

The model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.<br>
CivitAI Page: ---<br>

<b>Recommended Negative Prompt:</b><br>
(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth<br>
<b>or another negative prompt</b><br>

<b>Recommended Generation Parameters:</b><br>
Sampling Steps: 7+<br>
Sampling Method: DPM++ SDE Karras<br>
CFG Scale: 1.5-3

<b>Recommended Hires Fix Parameters:</b><br>
Hires steps: 10+<br>
Upscaler: 4x-UltraSharp upscaler / or another<br>
Denoising strength: 0.1 - 0.5<br>
Upscale by: 1.1-2.0<br>"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/SG161222/RealVisXL_V2.01> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/SG161222/RealVisXL_V2.01/discussions> ;
    ns1:readme <https://huggingface.co/SG161222/RealVisXL_V2.01/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6b72a43d7396497fe9733cb91a9e688db073b7c1c583e2c5fca339bd414a4dac> a schema:DefinedTerm ;
    schema:description "General label indicating the model or dataset has been categorized with specific keywords for discoverability."^^xsd:string ;
    schema:name "tags"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6b75cb265aea0eff37651595203eec38a2a621d81df99e4cfcd5f30cef590384> a schema:DefinedTerm ;
    schema:description "Toolkit for building, training, and fine-tuning GPU-accelerated conversational AI models."^^xsd:string ;
    schema:name "NeMo"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6bae02c2de2ffbaef251f84ae4fca46cf71a08d49204411ce94474ce829738ec> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/9fda77bf870c60518d915563657cd9c230951c55c7abb6395dadcb0c53747922> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the property is stated to enable users to make a decision as to the suitability of this creative work for their intended use."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Model Details"^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/dff307b5bc08947d577a5c7be1c716f51aac0eca11095e8a4735db37d32a3404> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/SidhaarthMurali/llama3.2-1b-mbpp-full> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2025-01-20T09:29:33+00:00"^^xsd:dateTime ;
    schema:dateModified "2025-01-20T10:00:22+00:00"^^xsd:dateTime ;
    schema:datePublished "2025-01-20T09:29:33+00:00"^^xsd:dateTime ;
    schema:description """---
library_name: transformers
license: other
base_model: meta-llama/Llama-3.2-1B
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: mbpp
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# mbpp

This model is a fine-tuned version of [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) on the identity and the mbpp datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3.0

### Training results



### Framework versions

- Transformers 4.46.1
- Pytorch 2.4.0
- Datasets 3.1.0
- Tokenizers 0.20.3
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/SidhaarthMurali/llama3.2-1b-mbpp-full/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/SidhaarthMurali/llama3.2-1b-mbpp-full"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/97a327fc7cf14c750f068bdf3aab8cf61c95e2b04079bd1772d37c385f2b1088>,
        <http://mlentory.zbmed.de/mlentory_graph/b751528b59fc7a0677c52c99b85d41f69866dad58deffdbf1e33a510f27bc2f5>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:finetune:meta-llama/Llama-3.2-1B"^^xsd:string,
        "base_model:meta-llama/Llama-3.2-1B"^^xsd:string,
        "license:other"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "other"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the framework are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
library_name: transformers
license: other
base_model: meta-llama/Llama-3.2-1B
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: mbpp
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# mbpp

This model is a fine-tuned version of [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) on the identity and the mbpp datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3.0

### Training results



### Framework versions

- Transformers 4.46.1
- Pytorch 2.4.0
- Datasets 3.1.0
- Tokenizers 0.20.3
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/SidhaarthMurali/llama3.2-1b-mbpp-full> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/SidhaarthMurali/llama3.2-1b-mbpp-full/discussions> ;
    ns1:readme <https://huggingface.co/SidhaarthMurali/llama3.2-1b-mbpp-full/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6bdae8c542a61819acfd0767eb5e55cafba29955aaefd1261c204618a4d3a487> a schema:DefinedTerm ;
    schema:description "Models with published evaluation metrics, benchmarks, or test results available for review."^^xsd:string ;
    schema:name "Eval Results"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6be7cf7db3f450c9e336fa84141084eae3710a2125412213fc9cb7041323d501> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/bf94889b67ecdd083fffcc97e7e51d92c6ffda086a4eca0658bf573797f1a573> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category is not mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/d4480d490771662bb85676cf29422964810080c65ef1bd8385fa451f7801acdb> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "The dataset used for validating the model is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/sidvash/famus_exh_task2_unsloth_llama-3-8b-Instruct-bnb-4bit-merged_16bit> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-05-28T05:24:51+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-06-22T21:46:06+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-28T05:24:51+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
- sft
base_model: unsloth/llama-3-8b-Instruct-bnb-4bit
---

# Uploaded  model

- **Developed by:** sidvash
- **License:** apache-2.0
- **Finetuned from model :** unsloth/llama-3-8b-Instruct-bnb-4bit

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)

# Task
GIven a document, and an event type from FrameNet, extract all instances of that event type from the document.

# Data
v0.1

This model used FAMuS train set (759 examples):
- Each instance had a gold extracted event + silver generated extracted events of the same type as extracted from Gemini-1.5-pro model with a 10-shot ICL gold annotated examples
- All instances are positive data (i.e. there is at least one instance of the event type present in the data)

More details: TBD
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/sidvash/famus_exh_task2_unsloth_llama-3-8b-Instruct-bnb-4bit-merged_16bit/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/sidvash/famus_exh_task2_unsloth_llama-3-8b-Instruct-bnb-4bit-merged_16bit"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/28f6fc79227e13d54a24f63fe2a8971b6f281ac3ca778d4303f84baacbc18a18>,
        <http://mlentory.zbmed.de/mlentory_graph/547c2a6f152d09c8e3dbd3a857c5481c4c4d77accf781e6fe237d9c49f47d9a7>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/e2bd5001a46246a7e8474accffaa2931c529f86701173c10c48e9befe9197517>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:finetune:unsloth/llama-3-8b-Instruct-bnb-4bit"^^xsd:string,
        "base_model:unsloth/llama-3-8b-Instruct-bnb-4bit"^^xsd:string,
        "en"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the software are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
- sft
base_model: unsloth/llama-3-8b-Instruct-bnb-4bit
---

# Uploaded  model

- **Developed by:** sidvash
- **License:** apache-2.0
- **Finetuned from model :** unsloth/llama-3-8b-Instruct-bnb-4bit

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)

# Task
GIven a document, and an event type from FrameNet, extract all instances of that event type from the document.

# Data
v0.1

This model used FAMuS train set (759 examples):
- Each instance had a gold extracted event + silver generated extracted events of the same type as extracted from Gemini-1.5-pro model with a 10-shot ICL gold annotated examples
- All instances are positive data (i.e. there is at least one instance of the event type present in the data)

More details: TBD
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/sidvash/famus_exh_task2_unsloth_llama-3-8b-Instruct-bnb-4bit-merged_16bit> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is v0.1."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/sidvash/famus_exh_task2_unsloth_llama-3-8b-Instruct-bnb-4bit-merged_16bit/discussions> ;
    ns1:readme <https://huggingface.co/sidvash/famus_exh_task2_unsloth_llama-3-8b-Instruct-bnb-4bit-merged_16bit/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6c9cb322c6bea7b3d73a5bca13215e5b2fe27c2235bee6a8eb01574bfb84d3c1> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/b039d4f4ed1833320cf3d5167c9e63c2e5864d50c719d12ed5a1b61de7086184> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The context provided does not contain information about the intended use of the property."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/85f0e69a70ce025a206d6ddbf78d989a3cabbdae215f2045df9b67b21a9d6780> ;
    ns2:modelCategory "The context provided does not contain information about the model category."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/290a5935ba014c4301a9f430a5f8577ab587e52024ce0e77b49d8296fe2aa3e4> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/fernandals/sentiment_v1> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2023-08-09T14:10:18+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-08-09T16:06:59+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-08-09T14:10:18+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model: distilbert-base-uncased
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: sentiment_v1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sentiment_v1

This model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.4863
- Accuracy: 0.8312

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 0.5858        | 1.0   | 3410 | 0.5747          | 0.7928   |
| 0.4237        | 2.0   | 6820 | 0.4863          | 0.8312   |


### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/fernandals/sentiment_v1/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/fernandals/sentiment_v1"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/7ad8435aa27d409e8692d8f6043742210150fe3cf420b9abaad32ffe8a63b7c7>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/974df5e3234c3601080dac4fa96eebf5d19d40df41e9e30cb61084d9fbefea1e>,
        <http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "base_model:distilbert/distilbert-base-uncased"^^xsd:string,
        "base_model:finetune:distilbert/distilbert-base-uncased"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the software are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model: distilbert-base-uncased
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: sentiment_v1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sentiment_v1

This model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.4863
- Accuracy: 0.8312

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 0.5858        | 1.0   | 3410 | 0.5747          | 0.7928   |
| 0.4237        | 2.0   | 6820 | 0.4863          | 0.8312   |


### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/fernandals/sentiment_v1> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/fernandals/sentiment_v1/discussions> ;
    ns1:readme <https://huggingface.co/fernandals/sentiment_v1/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6fd4d85e3416654bf33d9c5f3803b3e9900ce78a6309e4e6a84b2499ad4b7981> a schema:DefinedTerm ;
    schema:description "Simple framework for state-of-the-art NLP supporting tasks like named entity recognition and part-of-speech tagging."^^xsd:string ;
    schema:name "Flair"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/748580eeca6cf33af0292762713fbe0366225e973dccc66cdb56486f2b6ae341> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/7b94abd1a94f42759242e24f381b59455f00250bb42fe5e8479187485a809d0f> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/fab3808aee8413b2338df69bd63f0825aa16fbd16d9b162e95a945870b98b527> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/mjm4dl/WD_ADR_v1_really_mistral_mbh_new_data_r16_e5_linear_sched> ;
    schema:author "The author of this content is mjm4dl."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "mjm4dl"^^xsd:string ;
    schema:copyrightHolder "The copyright holder in the given text is \"mjm4dl\"."^^xsd:string ;
    schema:dateCreated "2024-08-11T13:03:00+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-08-11T13:05:54+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-08-11T13:03:00+00:00"^^xsd:dateTime ;
    schema:description """---
base_model: mistralai/Mistral-7B-Instruct-v0.3
language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- mistral
- trl
- sft
---

# Uploaded  model

- **Developed by:** mjm4dl
- **License:** apache-2.0
- **Finetuned from model :** mistralai/Mistral-7B-Instruct-v0.3

This mistral model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/mjm4dl/WD_ADR_v1_really_mistral_mbh_new_data_r16_e5_linear_sched/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/mjm4dl/WD_ADR_v1_really_mistral_mbh_new_data_r16_e5_linear_sched"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/28f6fc79227e13d54a24f63fe2a8971b6f281ac3ca778d4303f84baacbc18a18>,
        <http://mlentory.zbmed.de/mlentory_graph/547c2a6f152d09c8e3dbd3a857c5481c4c4d77accf781e6fe237d9c49f47d9a7>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/e2bd5001a46246a7e8474accffaa2931c529f86701173c10c48e9befe9197517>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:finetune:mistralai/Mistral-7B-Instruct-v0.3"^^xsd:string,
        "base_model:mistralai/Mistral-7B-Instruct-v0.3"^^xsd:string,
        "en"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "The maintainer of the model is mjm4dl."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
base_model: mistralai/Mistral-7B-Instruct-v0.3
language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- mistral
- trl
- sft
---

# Uploaded  model

- **Developed by:** mjm4dl
- **License:** apache-2.0
- **Finetuned from model :** mistralai/Mistral-7B-Instruct-v0.3

This mistral model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/mjm4dl/WD_ADR_v1_really_mistral_mbh_new_data_r16_e5_linear_sched> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/mjm4dl/WD_ADR_v1_really_mistral_mbh_new_data_r16_e5_linear_sched/discussions> ;
    ns1:readme <https://huggingface.co/mjm4dl/WD_ADR_v1_really_mistral_mbh_new_data_r16_e5_linear_sched/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/771e059c7890ae196a3227d36771c27f019638d33a819ca19bef2ae302b92b44> a schema:DefinedTerm ;
    schema:description "PyTorch-based audio source separation toolkit for researchers."^^xsd:string ;
    schema:name "Asteroid"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/79765e8db24271bde7a56b897b0b6c389d5be642563652aae56b6253290a801e> a schema:DefinedTerm ;
    schema:description "Classifies content into categories not seen during training."^^xsd:string ;
    schema:name "Zero Shot Classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/7daea98679eae3b5819f476833cada39b566629edb49ee1f55db0016e8ecf118> a schema:DefinedTerm ;
    schema:description "Python NLP library for accurate multilingual text analysis, including tokenization, POS tagging, and dependency parsing."^^xsd:string ;
    schema:name "Stanza"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/7ff927c41d72b6b2c9f13139dc99a851f2132e39d957e0c024b3022c2f194860> a schema:Organization .

<http://mlentory.zbmed.de/mlentory_graph/817a5a96614c358977bdbb4a9236873bedd5b59d312a8729cfc45d640bb61c99> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/558f0f0c846c9732fb258f139d8e03a6a976172ea778c1c3307e3a2eeb496834> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6fa5b295a0420d9a3cc819e672ebdd61720c48761f8c2960d28ff7f7655875be> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "The model Risks are significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/4407cda96af0afeaf4c1c5aaacc2cddb4eccb07bfeb1a5dded4a788a19846707> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/558f0f0c846c9732fb258f139d8e03a6a976172ea778c1c3307e3a2eeb496834> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/558f0f0c846c9732fb258f139d8e03a6a976172ea778c1c3307e3a2eeb496834> ;
    ns2:usageInstructions "To get started with the model, use the code below."^^xsd:string ;
    ns2:validatedOn "The text does not provide any information about the validated On."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "The conditions that affect the availability of, or method(s) of access to, an item are not explicitly mentioned in the given context."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-05-06T13:46:39+00:00"^^xsd:dateTime ;
    schema:datePublished "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:description """---
language: en
datasets:
- squad
widget:
- text: Which name is also used to describe the Amazon rainforest in English?
  context: 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish:
    Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch:
    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is
    a moist broadleaf forest that covers most of the Amazon basin of South America.
    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which
    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This
    region includes territory belonging to nine nations. The majority of the forest
    is contained within Brazil, with 60% of the rainforest, followed by Peru with
    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,
    Guyana, Suriname and French Guiana. States or departments in four nations contain
    "Amazonas" in their names. The Amazon represents over half of the planet''s remaining
    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest
    in the world, with an estimated 390 billion individual trees divided into 16,000
    species.'
- text: How many square kilometers of rainforest is covered in the basin?
  context: 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish:
    Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch:
    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is
    a moist broadleaf forest that covers most of the Amazon basin of South America.
    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which
    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This
    region includes territory belonging to nine nations. The majority of the forest
    is contained within Brazil, with 60% of the rainforest, followed by Peru with
    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,
    Guyana, Suriname and French Guiana. States or departments in four nations contain
    "Amazonas" in their names. The Amazon represents over half of the planet''s remaining
    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest
    in the world, with an estimated 390 billion individual trees divided into 16,000
    species.'
license: apache-2.0
---

# DistilBERT base uncased distilled SQuAD

## Table of Contents
- [Model Details](#model-details)
- [How To Get Started With the Model](#how-to-get-started-with-the-model)
- [Uses](#uses)
- [Risks, Limitations and Biases](#risks-limitations-and-biases)
- [Training](#training)
- [Evaluation](#evaluation)
- [Environmental Impact](#environmental-impact)
- [Technical Specifications](#technical-specifications)
- [Citation Information](#citation-information)
- [Model Card Authors](#model-card-authors)

## Model Details

**Model Description:** The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than *bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.

This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned using (a second step of) knowledge distillation on [SQuAD v1.1](https://huggingface.co/datasets/squad). 

- **Developed by:** Hugging Face
- **Model Type:** Transformer-based language model
- **Language(s):** English 
- **License:** Apache 2.0
- **Related Models:** [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased)
- **Resources for more information:**
  - See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including this model)
  - See [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure

## How to Get Started with the Model 

Use the code below to get started with the model. 

```python
>>> from transformers import pipeline
>>> question_answerer = pipeline("question-answering", model='distilbert-base-uncased-distilled-squad')

>>> context = r\"\"\"
... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a
... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune
... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.
... \"\"\"

>>> result = question_answerer(question="What is a good example of a question answering dataset?",     context=context)
>>> print(
... f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}"
...)

Answer: 'SQuAD dataset', score: 0.4704, start: 147, end: 160
```

Here is how to use this model in PyTorch:

```python
from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')
model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = torch.argmax(outputs.start_logits)
answer_end_index = torch.argmax(outputs.end_logits)

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
```

And in TensorFlow: 

```python
from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-distilled-squad")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased-distilled-squad")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
```

## Uses

This model can be used for question answering.

#### Misuse and Out-of-scope Use

The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.

## Risks, Limitations and Biases

**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:


```python
>>> from transformers import pipeline
>>> question_answerer = pipeline("question-answering", model='distilbert-base-uncased-distilled-squad')

>>> context = r\"\"\"
... Alice is sitting on the bench. Bob is sitting next to her.
... \"\"\"

>>> result = question_answerer(question="Who is the CEO?", context=context)
>>> print(
... f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}"
...)

Answer: 'Bob', score: 0.4183, start: 32, end: 35
```

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.

## Training

#### Training Data

The [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: 

> DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).

To learn more about the SQuAD v1.1 dataset, see the [SQuAD v1.1 data card](https://huggingface.co/datasets/squad).

#### Training Procedure

##### Preprocessing

See the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details.

##### Pretraining

See the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details. 

## Evaluation

As discussed in the [model repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)

> This model reaches a F1 score of 86.9 on the [SQuAD v1.1] dev set (for comparison, Bert bert-base-uncased version reaches a F1 score of 88.5).

## Environmental Impact

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type and hours used based on the [associated paper](https://arxiv.org/pdf/1910.01108.pdf). Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.

- **Hardware Type:** 8 16GB V100 GPUs
- **Hours used:** 90 hours
- **Cloud Provider:** Unknown
- **Compute Region:** Unknown
- **Carbon Emitted:** Unknown

## Technical Specifications

See the [associated paper](https://arxiv.org/abs/1910.01108) for details on the modeling architecture, objective, compute infrastructure, and training details.

## Citation Information

```bibtex
@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}
```

APA: 
- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

## Model Card Authors

This model card was written by the Hugging Face team. 
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad/discussions> ;
    schema:distribution "The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/6b88490a63533b6426e4066081546474864e838077b33450890ba59f06225201>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/97a1e7815c8e610e583f7f2afd2724e05d2efa1398eeb798f80981c0d44b28dc>,
        <http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21>,
        <http://mlentory.zbmed.de/mlentory_graph/d18f907263e8b85d9fa3da91cfbbf936203cf6077cef0d677afd97d2ebd3f68b>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "arxiv:1910.01108"^^xsd:string,
        "arxiv:1910.09700"^^xsd:string,
        "dataset:squad"^^xsd:string,
        "en"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string,
        "tf"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language: en
datasets:
- squad
widget:
- text: Which name is also used to describe the Amazon rainforest in English?
  context: 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish:
    Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch:
    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is
    a moist broadleaf forest that covers most of the Amazon basin of South America.
    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which
    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This
    region includes territory belonging to nine nations. The majority of the forest
    is contained within Brazil, with 60% of the rainforest, followed by Peru with
    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,
    Guyana, Suriname and French Guiana. States or departments in four nations contain
    "Amazonas" in their names. The Amazon represents over half of the planet''s remaining
    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest
    in the world, with an estimated 390 billion individual trees divided into 16,000
    species.'
- text: How many square kilometers of rainforest is covered in the basin?
  context: 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish:
    Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch:
    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is
    a moist broadleaf forest that covers most of the Amazon basin of South America.
    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which
    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This
    region includes territory belonging to nine nations. The majority of the forest
    is contained within Brazil, with 60% of the rainforest, followed by Peru with
    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,
    Guyana, Suriname and French Guiana. States or departments in four nations contain
    "Amazonas" in their names. The Amazon represents over half of the planet''s remaining
    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest
    in the world, with an estimated 390 billion individual trees divided into 16,000
    species.'
license: apache-2.0
---

# DistilBERT base uncased distilled SQuAD

## Table of Contents
- [Model Details](#model-details)
- [How To Get Started With the Model](#how-to-get-started-with-the-model)
- [Uses](#uses)
- [Risks, Limitations and Biases](#risks-limitations-and-biases)
- [Training](#training)
- [Evaluation](#evaluation)
- [Environmental Impact](#environmental-impact)
- [Technical Specifications](#technical-specifications)
- [Citation Information](#citation-information)
- [Model Card Authors](#model-card-authors)

## Model Details

**Model Description:** The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than *bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.

This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned using (a second step of) knowledge distillation on [SQuAD v1.1](https://huggingface.co/datasets/squad). 

- **Developed by:** Hugging Face
- **Model Type:** Transformer-based language model
- **Language(s):** English 
- **License:** Apache 2.0
- **Related Models:** [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased)
- **Resources for more information:**
  - See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including this model)
  - See [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure

## How to Get Started with the Model 

Use the code below to get started with the model. 

```python
>>> from transformers import pipeline
>>> question_answerer = pipeline("question-answering", model='distilbert-base-uncased-distilled-squad')

>>> context = r\"\"\"
... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a
... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune
... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.
... \"\"\"

>>> result = question_answerer(question="What is a good example of a question answering dataset?",     context=context)
>>> print(
... f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}"
...)

Answer: 'SQuAD dataset', score: 0.4704, start: 147, end: 160
```

Here is how to use this model in PyTorch:

```python
from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')
model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = torch.argmax(outputs.start_logits)
answer_end_index = torch.argmax(outputs.end_logits)

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
```

And in TensorFlow: 

```python
from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-distilled-squad")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased-distilled-squad")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
```

## Uses

This model can be used for question answering.

#### Misuse and Out-of-scope Use

The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.

## Risks, Limitations and Biases

**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:


```python
>>> from transformers import pipeline
>>> question_answerer = pipeline("question-answering", model='distilbert-base-uncased-distilled-squad')

>>> context = r\"\"\"
... Alice is sitting on the bench. Bob is sitting next to her.
... \"\"\"

>>> result = question_answerer(question="Who is the CEO?", context=context)
>>> print(
... f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}"
...)

Answer: 'Bob', score: 0.4183, start: 32, end: 35
```

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.

## Training

#### Training Data

The [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: 

> DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).

To learn more about the SQuAD v1.1 dataset, see the [SQuAD v1.1 data card](https://huggingface.co/datasets/squad).

#### Training Procedure

##### Preprocessing

See the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details.

##### Pretraining

See the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details. 

## Evaluation

As discussed in the [model repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)

> This model reaches a F1 score of 86.9 on the [SQuAD v1.1] dev set (for comparison, Bert bert-base-uncased version reaches a F1 score of 88.5).

## Environmental Impact

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type and hours used based on the [associated paper](https://arxiv.org/pdf/1910.01108.pdf). Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.

- **Hardware Type:** 8 16GB V100 GPUs
- **Hours used:** 90 hours
- **Cloud Provider:** Unknown
- **Compute Region:** Unknown
- **Carbon Emitted:** Unknown

## Technical Specifications

See the [associated paper](https://arxiv.org/abs/1910.01108) for details on the modeling architecture, objective, compute infrastructure, and training details.

## Citation Information

```bibtex
@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}
```

APA: 
- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

## Model Card Authors

This model card was written by the Hugging Face team. 
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad> ;
    schema:version "The context provided does not contain the information needed to answer the question about the version of the CreativeWork embodied by a specified resource."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad/discussions> ;
    ns1:readme <https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/24d02c4c9af718667adb32f98890ccae3daa567e09e890fbcc6baa38ee130dd0>,
        <http://mlentory.zbmed.de/mlentory_graph/96b96e1fc1370c6454f55ef5bb235e3108a8e1b32665ef0b72dd54ba220b3a53> .

<http://mlentory.zbmed.de/mlentory_graph/84231fbb9536867717cd1bec10e9b794e00f74ec841db425202ae7caecc93496> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Considerations with regards to ethical legal and social aspects."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3."""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/9c03090b68b1e45d6bebe9b9d1ea103b6a4c064ceb71f16f08a5ae59d9bac70e> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions """As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those"""^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context"""^^xsd:string ;
    schema:copyrightHolder """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found."""^^xsd:string ;
    schema:dateCreated "2024-09-18T15:12:47+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-10-24T15:07:51+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-09-18T15:12:47+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- en
- de
- fr
- it
- pt
- hi
- es
- th
library_name: transformers
pipeline_tag: text-generation
tags:
- facebook
- meta
- pytorch
- llama
- llama-3
license: llama3.2
extra_gated_prompt: "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\
  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\
  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\
  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\
  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\
  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\
  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\
  \\ of the age required under applicable laws, rules or regulations to provide legal\\
  \\ consent and that has legal authority to bind your employer or such other person\\
  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\
  \\ means the foundational large language models and software and algorithms, including\\
  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\
  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\
  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\
  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\
  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\
  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\
  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\
  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\
  \\ below or by using or distributing any portion or element of the Llama Materials,\\
  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\
  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\
  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\
  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\
  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\
  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\
  \\ Materials (or any derivative works thereof),  or a product or service (including\\
  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\
  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\
  \\ Llama” on a related website, user interface, blogpost, about page, or product\\
  \\ documentation. If you use the Llama Materials or any outputs or results of the\\
  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\
  \\ which is distributed or made available, you shall also include “Llama” at the\\
  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\
  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\
  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\
  \\ in all copies of the Llama Materials that you distribute the  following attribution\\
  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\
  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\
  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\
  \\ applicable laws and regulations (including trade compliance laws and regulations)\\
  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\
  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\
  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\
  \\ version release date, the monthly active users of the products or services made\\
  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\
  \\ monthly active users in the preceding calendar month, you must request  a license\\
  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\
  \\ authorized to exercise any of the rights under this Agreement unless or until\\
  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\
  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\
  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\
  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\
  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\
  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\
  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\
  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\
  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\
  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\
  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\
  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\
  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\
  a. No trademark licenses are granted under this Agreement, and in connection with\\
  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\
  \\ by or associated with the other or any of its affiliates,  except as required\\
  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\
  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\
  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\
  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\
  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\
  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\
  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\
  \\ respect to any derivative works and modifications of the Llama Materials that\\
  \\ are made by you, as between you and Meta, you are and will be the owner of such\\
  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\
  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\
  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\
  \\ of any of the foregoing, constitutes infringement of intellectual property or\\
  \\ other rights owned or licensable by you, then any licenses granted to you under\\
  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\
  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\
  \\ claim by any third party arising out of or related to your use or distribution\\
  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\
  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\
  \\ and will continue in full force and effect until terminated in accordance with\\
  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\
  \\ in breach of any term or condition of this Agreement. Upon termination of this\\
  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\
  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\
  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\
  \\ the State of  California without regard to choice of law principles, and the UN\\
  \\ Convention on Contracts for the International Sale of Goods does not apply to\\
  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\
  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\
  Meta is committed to promoting safe and fair use of its tools and features, including\\
  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\
  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\
  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\
  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\
  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\
  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\
  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\
  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\
  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\
  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\
  \\ illegal distribution of information or materials to minors, including obscene\\
  \\ materials, or failure to employ legally required age-gating in connection with\\
  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\
  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\
  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\
  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\
  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\
  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\
  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\
  \\ but not limited to, financial, legal, medical/health, or related professional\\
  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\
  \\ information about individuals, including information about individuals’ identity,\\
  \\ health, or demographic information, unless you have obtained the right to do so\\
  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\
  \\ generate any content that infringes, misappropriates, or otherwise violates any\\
  \\ third-party rights, including the outputs or results of any products or services\\
  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\
  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\
  \\ overburden, interfere with or impair the proper working, integrity, operation\\
  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\
  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\
  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\
  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\
  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\
  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\
  \\ applications, espionage, use for materials or activities that are subject to the\\
  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\
  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\
  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\
  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\
  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\
  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\
  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\
  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\
  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\
  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\
  \\    15. Generating, promoting, or furthering defamatory content, including the\\
  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\
  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\
  \\ without consent, authorization, or legal right\\n    18. Representing that the\\
  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\
  \\ false online engagement, including fake reviews and other means of fake online\\
  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\
  \\ of your AI system 5. Interact with third party tools, models, or software designed\\
  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\
  \\ that the outputs of such tools, models, or software are associated with Meta or\\
  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\
  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\
  \\ are not being granted to you if you are an individual domiciled in, or a company\\
  \\ with a principal place of business in, the European Union. This restriction does\\
  \\ not apply to end users of a product or service that incorporates any such multimodal\\
  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\
  \\ problems that could lead to a violation of this Policy through one of the following\\
  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\
  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\
  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\
  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\
  \\ 3.2: LlamaUseReport@meta.com"
extra_gated_fields:
  First Name: text
  Last Name: text
  Date of birth: date_picker
  Country: country
  Affiliation: text
  Job title:
    type: select
    options:
    - Student
    - Research Graduate
    - AI researcher
    - AI developer/engineer
    - Reporter
    - Other
  geo: ip_location
  ? By clicking Submit below I accept the terms of the license and acknowledge that
    the information I provide will be collected stored processed and shared in accordance
    with the Meta Privacy Policy
  : checkbox
extra_gated_description: The information you provide will be collected, stored, processed
  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).
extra_gated_button_content: Submit
---

## Model Information

The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.

**Model Developer:** Meta

**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |
| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |

**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.

**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date:** Sept 25, 2024

**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.

**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).

**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).

## Intended Use

**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.

## How to use

This repository contains two versions of Llama-3.2-1B-Instruct, for use with transformers and with the original `llama` codebase.

### Use with transformers

Starting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.

Make sure to update your transformers installation via `pip install --upgrade transformers`.

```python
import torch
from transformers import pipeline

model_id = "meta-llama/Llama-3.2-1B-Instruct"
pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]
outputs = pipe(
    messages,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1])
```

Note: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)

### Use with `llama`

Please, follow the instructions in the [repository](https://github.com/meta-llama/llama)

To download Original checkpoints, see the example command below leveraging `huggingface-cli`:

```
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct --include "original/*" --local-dir Llama-3.2-1B-Instruct
```

## Hardware and Software

**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.

**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.

**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.

|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |
| :---- | :---: | ----- | :---: | :---: | :---: |
| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |
| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |
| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |
| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |
| Total | 833k |         86k |  | 240 | 0 |

\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.

The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.

## Training Data

**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).

**Data Freshness:** The pretraining data has a cutoff of December 2023\\.

## Quantization

### Quantization Scheme

We designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:
- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.
- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.
- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.


### Quantization-Aware Training and LoRA

The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).

### SpinQuant

[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.

## Benchmarks \\- English Text

In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.

### Base Pretrained Models

| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |
| ----- | ----- | :---: | :---: | :---: | :---: | :---: |
| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |
|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |
|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |
| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |
|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |
|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |
| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |

### Instruction Tuned Models

| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |
| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |
| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |
| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |
| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |
|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |
| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |
|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |
|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |
| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |
|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |
| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |
|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |
|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |
| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |

\\*\\*for comparison purposes only. Model not released.

### Multilingual Benchmarks

| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |
| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |
| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |
| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |
| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |
| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |
| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |

\\*\\*for comparison purposes only. Model not released.

## Inference time

In the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.

| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |
| :---- | ----- | ----- | ----- | ----- | ----- |
| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |
| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |
| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |
| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |
| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |
| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |

(\\*) The performance measurement is done using an adb binary-based approach.
(\\*\\*) It is measured on an Android OnePlus 12 device.
(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64

*Footnote:*

- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*
- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*
- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*
- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*
- *RSS size \\- Memory usage in resident set size (RSS)*

## Responsibility & Safety

As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

### Responsible Deployment

**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).

#### Llama 3.2 Instruct

**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).

**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.

**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.

#### Llama 3.2 Systems

**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.

### New Capabilities and Use Cases

**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.

**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.

### Evaluations

**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.

**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.

### Critical Risks

In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:

**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.

**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.

**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.
Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.

### Community

**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).

**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).

**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.

## Ethical Considerations and Limitations

**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.

**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"^^xsd:string ;
    schema:inLanguage "de"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "fr"^^xsd:string,
        "hi"^^xsd:string,
        "it"^^xsd:string,
        "pt"^^xsd:string,
        "th"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/11ee21b926795a2b7d2cb2d939044519ace34ac86e6b6a59a47dcb4bba5eb7a4>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/759dcfce11b186065b46fb94fef1c569da9001c123861ba72ba0708e554bd6c4>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/d77765c1bc2a45cffe4e1ace66ea68c61a819d17e7d6159e6840523d288dbed3>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "arxiv:2204.05149"^^xsd:string,
        "arxiv:2405.16406"^^xsd:string,
        "de"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "fr"^^xsd:string,
        "hi"^^xsd:string,
        "it"^^xsd:string,
        "license:llama3.2"^^xsd:string,
        "pt"^^xsd:string,
        "region:us"^^xsd:string,
        "th"^^xsd:string ;
    schema:license "llama3.2"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by Llama 3.2 are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- en
- de
- fr
- it
- pt
- hi
- es
- th
library_name: transformers
pipeline_tag: text-generation
tags:
- facebook
- meta
- pytorch
- llama
- llama-3
license: llama3.2
extra_gated_prompt: "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\
  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\
  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\
  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\
  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\
  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\
  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\
  \\ of the age required under applicable laws, rules or regulations to provide legal\\
  \\ consent and that has legal authority to bind your employer or such other person\\
  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\
  \\ means the foundational large language models and software and algorithms, including\\
  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\
  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\
  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\
  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\
  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\
  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\
  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\
  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\
  \\ below or by using or distributing any portion or element of the Llama Materials,\\
  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\
  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\
  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\
  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\
  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\
  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\
  \\ Materials (or any derivative works thereof),  or a product or service (including\\
  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\
  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\
  \\ Llama” on a related website, user interface, blogpost, about page, or product\\
  \\ documentation. If you use the Llama Materials or any outputs or results of the\\
  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\
  \\ which is distributed or made available, you shall also include “Llama” at the\\
  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\
  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\
  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\
  \\ in all copies of the Llama Materials that you distribute the  following attribution\\
  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\
  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\
  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\
  \\ applicable laws and regulations (including trade compliance laws and regulations)\\
  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\
  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\
  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\
  \\ version release date, the monthly active users of the products or services made\\
  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\
  \\ monthly active users in the preceding calendar month, you must request  a license\\
  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\
  \\ authorized to exercise any of the rights under this Agreement unless or until\\
  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\
  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\
  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\
  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\
  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\
  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\
  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\
  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\
  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\
  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\
  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\
  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\
  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\
  a. No trademark licenses are granted under this Agreement, and in connection with\\
  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\
  \\ by or associated with the other or any of its affiliates,  except as required\\
  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\
  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\
  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\
  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\
  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\
  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\
  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\
  \\ respect to any derivative works and modifications of the Llama Materials that\\
  \\ are made by you, as between you and Meta, you are and will be the owner of such\\
  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\
  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\
  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\
  \\ of any of the foregoing, constitutes infringement of intellectual property or\\
  \\ other rights owned or licensable by you, then any licenses granted to you under\\
  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\
  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\
  \\ claim by any third party arising out of or related to your use or distribution\\
  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\
  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\
  \\ and will continue in full force and effect until terminated in accordance with\\
  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\
  \\ in breach of any term or condition of this Agreement. Upon termination of this\\
  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\
  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\
  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\
  \\ the State of  California without regard to choice of law principles, and the UN\\
  \\ Convention on Contracts for the International Sale of Goods does not apply to\\
  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\
  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\
  Meta is committed to promoting safe and fair use of its tools and features, including\\
  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\
  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\
  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\
  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\
  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\
  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\
  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\
  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\
  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\
  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\
  \\ illegal distribution of information or materials to minors, including obscene\\
  \\ materials, or failure to employ legally required age-gating in connection with\\
  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\
  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\
  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\
  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\
  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\
  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\
  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\
  \\ but not limited to, financial, legal, medical/health, or related professional\\
  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\
  \\ information about individuals, including information about individuals’ identity,\\
  \\ health, or demographic information, unless you have obtained the right to do so\\
  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\
  \\ generate any content that infringes, misappropriates, or otherwise violates any\\
  \\ third-party rights, including the outputs or results of any products or services\\
  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\
  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\
  \\ overburden, interfere with or impair the proper working, integrity, operation\\
  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\
  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\
  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\
  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\
  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\
  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\
  \\ applications, espionage, use for materials or activities that are subject to the\\
  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\
  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\
  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\
  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\
  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\
  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\
  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\
  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\
  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\
  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\
  \\    15. Generating, promoting, or furthering defamatory content, including the\\
  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\
  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\
  \\ without consent, authorization, or legal right\\n    18. Representing that the\\
  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\
  \\ false online engagement, including fake reviews and other means of fake online\\
  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\
  \\ of your AI system 5. Interact with third party tools, models, or software designed\\
  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\
  \\ that the outputs of such tools, models, or software are associated with Meta or\\
  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\
  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\
  \\ are not being granted to you if you are an individual domiciled in, or a company\\
  \\ with a principal place of business in, the European Union. This restriction does\\
  \\ not apply to end users of a product or service that incorporates any such multimodal\\
  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\
  \\ problems that could lead to a violation of this Policy through one of the following\\
  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\
  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\
  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\
  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\
  \\ 3.2: LlamaUseReport@meta.com"
extra_gated_fields:
  First Name: text
  Last Name: text
  Date of birth: date_picker
  Country: country
  Affiliation: text
  Job title:
    type: select
    options:
    - Student
    - Research Graduate
    - AI researcher
    - AI developer/engineer
    - Reporter
    - Other
  geo: ip_location
  ? By clicking Submit below I accept the terms of the license and acknowledge that
    the information I provide will be collected stored processed and shared in accordance
    with the Meta Privacy Policy
  : checkbox
extra_gated_description: The information you provide will be collected, stored, processed
  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).
extra_gated_button_content: Submit
---

## Model Information

The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.

**Model Developer:** Meta

**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |
| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |

**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.

**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date:** Sept 25, 2024

**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.

**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).

**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).

## Intended Use

**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.

## How to use

This repository contains two versions of Llama-3.2-1B-Instruct, for use with transformers and with the original `llama` codebase.

### Use with transformers

Starting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.

Make sure to update your transformers installation via `pip install --upgrade transformers`.

```python
import torch
from transformers import pipeline

model_id = "meta-llama/Llama-3.2-1B-Instruct"
pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]
outputs = pipe(
    messages,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1])
```

Note: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)

### Use with `llama`

Please, follow the instructions in the [repository](https://github.com/meta-llama/llama)

To download Original checkpoints, see the example command below leveraging `huggingface-cli`:

```
huggingface-cli download meta-llama/Llama-3.2-1B-Instruct --include "original/*" --local-dir Llama-3.2-1B-Instruct
```

## Hardware and Software

**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.

**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.

**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.

|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |
| :---- | :---: | ----- | :---: | :---: | :---: |
| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |
| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |
| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |
| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |
| Total | 833k |         86k |  | 240 | 0 |

\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.

The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.

## Training Data

**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).

**Data Freshness:** The pretraining data has a cutoff of December 2023\\.

## Quantization

### Quantization Scheme

We designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:
- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.
- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.
- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.


### Quantization-Aware Training and LoRA

The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).

### SpinQuant

[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.

## Benchmarks \\- English Text

In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.

### Base Pretrained Models

| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |
| ----- | ----- | :---: | :---: | :---: | :---: | :---: |
| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |
|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |
|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |
| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |
|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |
|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |
| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |

### Instruction Tuned Models

| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |
| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |
| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |
| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |
| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |
|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |
| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |
|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |
|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |
| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |
|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |
| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |
|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |
|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |
| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |

\\*\\*for comparison purposes only. Model not released.

### Multilingual Benchmarks

| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |
| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |
| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |
| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |
| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |
| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |
| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |

\\*\\*for comparison purposes only. Model not released.

## Inference time

In the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.

| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |
| :---- | ----- | ----- | ----- | ----- | ----- |
| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |
| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |
| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |
| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |
| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |
| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |

(\\*) The performance measurement is done using an adb binary-based approach.
(\\*\\*) It is measured on an Android OnePlus 12 device.
(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64

*Footnote:*

- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*
- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*
- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*
- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*
- *RSS size \\- Memory usage in resident set size (RSS)*

## Responsibility & Safety

As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

### Responsible Deployment

**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).

#### Llama 3.2 Instruct

**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).

**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.

**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.

#### Llama 3.2 Systems

**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.

### New Capabilities and Use Cases

**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.

**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.

### Evaluations

**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.

**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.

### Critical Risks

In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:

**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.

**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.

**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.
Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.

### Community

**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).

**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).

**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.

## Ethical Considerations and Limitations

**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.

**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.
"""^^xsd:string ;
    schema:softwareHelp """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the"""^^xsd:string ;
    schema:softwareRequirements """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the"""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/discussions> ;
    ns1:readme <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/2730e6211466c050884bcc521d8aaf9f0af8a2993b56ca3f9ab751a178b4f970>,
        <http://mlentory.zbmed.de/mlentory_graph/a787507208799647a209772884d76edfc0a14e5f209b09e235fbff7797a1a812> .

<http://mlentory.zbmed.de/mlentory_graph/85331749b21cea2ad96c1777edde87247dda7866aaad072b3598077954e09c3f> a schema:Organization .

<http://mlentory.zbmed.de/mlentory_graph/85b6833a0e3a566c8de87c0f42b57ec97f67e03180245edb416823e240ee11d1> a schema:DefinedTerm ;
    schema:description "Array framework for machine learning on Apple silicon, designed for efficiency and ease of use."^^xsd:string ;
    schema:name "MLX"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/86f52aa6369c70ec009cd27198c0d2e52782b473507c01a7426d4c1bf4185e05> a schema:DefinedTerm ;
    schema:description "Optimized backend for deploying and serving text embedding models with high performance."^^xsd:string ;
    schema:name "text-embedding-inference"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/8fd2e8ec655b6111cc83724fbc62c13d0c8b3442b1768b9bc35e0eceae0ebe85> a schema:DefinedTerm ;
    schema:description "Creates masks for objects or regions in images."^^xsd:string ;
    schema:name "Mask Generation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9132b663f2893be0a7b034405164be91c4fe250d2b1ecd3390fb9b9bd689997b> a schema:DefinedTerm ;
    schema:description "Categorizes video content into predefined classes."^^xsd:string ;
    schema:name "Video Classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/934dea1e0796f0376db45e4b74e093aeb422ffdf79cf5c4aaea1e5e84969c76e> a schema:DefinedTerm ;
    schema:description "A file format for efficiently storing and loading large language models with quantization options."^^xsd:string ;
    schema:name "GGUF"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/93b740de86007e3b4f90e27cf12bec9352c53b1b8f2cbc1f6bf9860d0cce1ba5> a schema:Organization .

<http://mlentory.zbmed.de/mlentory_graph/956922acae22f21b82ce000e45d4739baf3f94e8b6eb2825a93166a8ccfc2fbf> a schema:DefinedTerm ;
    schema:description "NLP library developed by Baidu based on PaddlePaddle, offering Chinese and multilingual support."^^xsd:string ;
    schema:name "paddlenlp"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9acbb04b19d934f34f67f439329d457dd9f24f7ed9a99e2c3ad021a839494acb> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/c4d07fa0daab8d52ad9866cddd0f33417e35c2c0ef5a1c93787cc16c57ed38ec> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended uses and limitations of full_vanilla_doff_dpo_iter_2 are not explicitly stated in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/c372f0bca81211b35cd0974aabe8500d5ca4479cddab1685b44f24bd5523dc9e> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess """Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_2 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections"""^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-07-03T17:31:34+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-07-03T20:08:46+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-07-03T17:31:34+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model: YYYYYYibo/full_vanilla_doff_dpo_iter_1
tags:
- alignment-handbook
- generated_from_trainer
- trl
- dpo
datasets:
- updated
- original
model-index:
- name: full_vanilla_doff_dpo_iter_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# full_vanilla_doff_dpo_iter_2

This model is a fine-tuned version of [YYYYYYibo/full_vanilla_doff_dpo_iter_1](https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1) on the updated and the original datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2+cu121
- Datasets 2.14.6
- Tokenizers 0.15.2
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/267e04ff20e54a667a0c17e69b7dd3f07b14ad451d8a605d3b5b771693ba73ba>,
        <http://mlentory.zbmed.de/mlentory_graph/28f6fc79227e13d54a24f63fe2a8971b6f281ac3ca778d4303f84baacbc18a18>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/89b5833251f8f95309bb1d81c7cbd5e686d9f1e1a94b81869c2661ff9301fb64>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:YYYYYYibo/full_vanilla_doff_dpo_iter_1"^^xsd:string,
        "base_model:finetune:YYYYYYibo/full_vanilla_doff_dpo_iter_1"^^xsd:string,
        "dataset:original"^^xsd:string,
        "dataset:updated"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model: YYYYYYibo/full_vanilla_doff_dpo_iter_1
tags:
- alignment-handbook
- generated_from_trainer
- trl
- dpo
datasets:
- updated
- original
model-index:
- name: full_vanilla_doff_dpo_iter_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# full_vanilla_doff_dpo_iter_2

This model is a fine-tuned version of [YYYYYYibo/full_vanilla_doff_dpo_iter_1](https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1) on the updated and the original datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2+cu121
- Datasets 2.14.6
- Tokenizers 0.15.2
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2/discussions> ;
    ns1:readme <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9db08d1a554609ae7c645781f30f4c335c5142d3c63fde5964e29f8a404c9566> a schema:DefinedTerm ;
    schema:description "Efficient framework for few-shot text classification using Sentence Transformers."^^xsd:string ;
    schema:name "setfit"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9f0b234a1cff093b8be37193e5207e4a187eebdbfef3b998644998553eca2b65> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/5a898586bcb0bb2e26a98d976b8f37baf784497ff2bf75b2249741a14680478c> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of my_imdb is not specified in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/85f0e69a70ce025a206d6ddbf78d989a3cabbdae215f2045df9b67b21a9d6780> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/44669cbedcb2f4b90028ef263e4c66e7ce1f20328981f664f9b6e2ffbfa286f7> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/5a898586bcb0bb2e26a98d976b8f37baf784497ff2bf75b2249741a14680478c> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/5a898586bcb0bb2e26a98d976b8f37baf784497ff2bf75b2249741a14680478c> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/wellmadenametag/my_imdb> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2023-06-24T08:59:30+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-06-24T09:36:53+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-06-24T08:59:30+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
tags:
- generated_from_trainer
datasets:
- imdb
metrics:
- accuracy
- f1
- precision
- recall
model-index:
- name: my_imdb
  results:
  - task:
      name: Text Classification
      type: text-classification
    dataset:
      name: imdb
      type: imdb
      config: plain_text
      split: train[:5000]
      args: plain_text
    metrics:
    - name: Accuracy
      type: accuracy
      value: 1.0
    - name: F1
      type: f1
      value: 0.0
    - name: Precision
      type: precision
      value: 0.0
    - name: Recall
      type: recall
      value: 0.0
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_imdb

This model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the imdb dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0000
- Accuracy: 1.0
- F1: 0.0
- Precision: 0.0
- Recall: 0.0

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 16
- eval_batch_size: 64
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 500
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1  | Precision | Recall |
|:-------------:|:-----:|:----:|:---------------:|:--------:|:---:|:---------:|:------:|
| 0.0109        | 0.43  | 100  | 0.0075          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0013        | 0.85  | 200  | 0.0010          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0003        | 1.28  | 300  | 0.0002          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0001        | 1.7   | 400  | 0.0001          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0           | 2.13  | 500  | 0.0000          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0           | 2.55  | 600  | 0.0000          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0           | 2.98  | 700  | 0.0000          | 1.0      | 0.0 | 0.0       | 0.0    |


### Framework versions

- Transformers 4.30.2
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/wellmadenametag/my_imdb/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/wellmadenametag/my_imdb"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/13e6e4095523310627190177e08c0206ca97e76f4c4565c15c2b35270daa2588>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/7ad8435aa27d409e8692d8f6043742210150fe3cf420b9abaad32ffe8a63b7c7>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "dataset:imdb"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by my_imdb are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
tags:
- generated_from_trainer
datasets:
- imdb
metrics:
- accuracy
- f1
- precision
- recall
model-index:
- name: my_imdb
  results:
  - task:
      name: Text Classification
      type: text-classification
    dataset:
      name: imdb
      type: imdb
      config: plain_text
      split: train[:5000]
      args: plain_text
    metrics:
    - name: Accuracy
      type: accuracy
      value: 1.0
    - name: F1
      type: f1
      value: 0.0
    - name: Precision
      type: precision
      value: 0.0
    - name: Recall
      type: recall
      value: 0.0
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_imdb

This model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the imdb dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0000
- Accuracy: 1.0
- F1: 0.0
- Precision: 0.0
- Recall: 0.0

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 16
- eval_batch_size: 64
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 500
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1  | Precision | Recall |
|:-------------:|:-----:|:----:|:---------------:|:--------:|:---:|:---------:|:------:|
| 0.0109        | 0.43  | 100  | 0.0075          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0013        | 0.85  | 200  | 0.0010          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0003        | 1.28  | 300  | 0.0002          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0001        | 1.7   | 400  | 0.0001          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0           | 2.13  | 500  | 0.0000          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0           | 2.55  | 600  | 0.0000          | 1.0      | 0.0 | 0.0       | 0.0    |
| 0.0           | 2.98  | 700  | 0.0000          | 1.0      | 0.0 | 0.0       | 0.0    |


### Framework versions

- Transformers 4.30.2
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/wellmadenametag/my_imdb> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/wellmadenametag/my_imdb/discussions> ;
    ns1:readme <https://huggingface.co/wellmadenametag/my_imdb/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/a0e0c1ddbfc3fa3aaab6bbd4ee4022410d9adc63e05996f59a545a7929aaf019> a schema:DefinedTerm ;
    schema:description "Open-source implementation of CLIP (Contrastive Language-Image Pre-Training) models."^^xsd:string ;
    schema:name "OpenCLIP"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/a225e69a17f4be8c51cc1f1852c40371a478cfa95661008284bc676564a876e9> a schema:DefinedTerm ;
    schema:description "High-level neural networks API, designed for human beings, not machines, focusing on enabling fast experimentation."^^xsd:string ;
    schema:name "Keras"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/a2f3a1fe1480acaf0753eb2d5ef88519c7b81e26e6505fed6d5c93c3a5119a65> a schema:DefinedTerm ;
    schema:description "Framework for efficiently deploying models on Intelligence Processing Unit (IPU) hardware."^^xsd:string ;
    schema:name "Graphcore"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/a93fcaf34b63f23354fddd78b2e61cb8403966eb723c6135cafcd83185dbda9e> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of TinyLlama-1.1B is not explicitly stated in the provided context. However, it is mentioned that TinyLlama can be plugged and played in many open-source projects built upon Llama, and it is compact with only 1.1B parameters, making it suitable for applications demanding a restricted computation and memory footprint."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Model Details"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/40c6f438362f1a92103a016e23ec429f4b4dc082c4d0b6b7f94f07db46fe1706> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found"^^xsd:string ;
    schema:archivedAt <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-8bits> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-05-02T00:59:43+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-05-02T01:01:56+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-02T00:59:43+00:00"^^xsd:dateTime ;
    schema:description """---
{}
---
Quantization made by Richard Erkhov.

[Github](https://github.com/RichardErkhov)

[Discord](https://discord.gg/pvy7H8DZMG)

[Request more models](https://github.com/RichardErkhov/quant_request)


TinyLlama-1.1B-Chat-v1.0 - bnb 8bits
- Model creator: https://huggingface.co/TinyLlama/
- Original model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/




Original model description:
---
license: apache-2.0
datasets:
- cerebras/SlimPajama-627B
- bigcode/starcoderdata
- HuggingFaceH4/ultrachat_200k
- HuggingFaceH4/ultrafeedback_binarized
language:
- en
widget:
  - example_title: Fibonacci (Python)
    messages:
    - role: system
      content: You are a chatbot who can help code!
    - role: user
      content: Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.
---
<div align="center">

# TinyLlama-1.1B
</div>

https://github.com/jzhang38/TinyLlama

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. 


We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### This Model
This is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was " initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. 
We then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4." 


#### How to use
You will need the transformers>=4.34
Do check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.

```python
# Install transformers from source - only needed for versions <= v4.34
# pip install git+https://github.com/huggingface/transformers.git
# pip install accelerate

import torch
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")

# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
# <|system|>
# You are a friendly chatbot who always responds in the style of a pirate.</s>
# <|user|>
# How many helicopters can a human eat in one sitting?</s>
# <|assistant|>
# ...
```

"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-8bits/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-8bits"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0813ac63520e9dbe708eec9898230a9dbdb1c48971b07fa1ad53fb6dfaa7a564>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/4e2b489139fd9424e3deea62e2359c279e804cb631f5e3fd4c1685d0b1d55445>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "The maintainer of TinyLlama-1.1B is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
{}
---
Quantization made by Richard Erkhov.

[Github](https://github.com/RichardErkhov)

[Discord](https://discord.gg/pvy7H8DZMG)

[Request more models](https://github.com/RichardErkhov/quant_request)


TinyLlama-1.1B-Chat-v1.0 - bnb 8bits
- Model creator: https://huggingface.co/TinyLlama/
- Original model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/




Original model description:
---
license: apache-2.0
datasets:
- cerebras/SlimPajama-627B
- bigcode/starcoderdata
- HuggingFaceH4/ultrachat_200k
- HuggingFaceH4/ultrafeedback_binarized
language:
- en
widget:
  - example_title: Fibonacci (Python)
    messages:
    - role: system
      content: You are a chatbot who can help code!
    - role: user
      content: Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.
---
<div align="center">

# TinyLlama-1.1B
</div>

https://github.com/jzhang38/TinyLlama

The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of "just" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. 


We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.

#### This Model
This is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was " initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. 
We then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4." 


#### How to use
You will need the transformers>=4.34
Do check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.

```python
# Install transformers from source - only needed for versions <= v4.34
# pip install git+https://github.com/huggingface/transformers.git
# pip install accelerate

import torch
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0", torch_dtype=torch.bfloat16, device_map="auto")

# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
# <|system|>
# You are a friendly chatbot who always responds in the style of a pirate.</s>
# <|user|>
# How many helicopters can a human eat in one sitting?</s>
# <|assistant|>
# ...
```

"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements """The software requirements mentioned in the context are:

1. transformers>=4.34
2. Check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information."""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-8bits> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-8bits/discussions> ;
    ns1:readme <https://huggingface.co/RichardErkhov/TinyLlama_-_TinyLlama-1.1B-Chat-v1.0-8bits/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ab2624f913f0d530408cbfa25c29a33523a1a481ba19c3d46f91a6fac4e63844> a schema:DefinedTerm ;
    schema:description "Detects objects in categories not seen during training."^^xsd:string ;
    schema:name "Zero Shot Object Detection"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/abb31119b9d215cd1c22cd4daa06ca04057087bd174e20c237ac4faea9bb5b3c> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/9eb6952b89569e3a3863aebca62fd4619833b31bda93a0d0b5678c6627ef9275> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the model is not explicitly stated in the provided context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model category is not mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/fc7cf57f062cb9e0d1e9d259e47e9f36b464ea9c455e880a85e25d6281fc3268> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "The validated On in the given text is \"Dataset used for validating the model.\""^^xsd:string ;
    schema:archivedAt <https://huggingface.co/rangapriyabatchu/black-forest-labsFLUX.1-dev> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "The information provided does not contain the name of the contributor."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2025-01-13T13:57:35+00:00"^^xsd:dateTime ;
    schema:dateModified "2025-01-13T13:57:38+00:00"^^xsd:dateTime ;
    schema:datePublished "2025-01-13T13:57:35+00:00"^^xsd:dateTime ;
    schema:description """---
tags:
- text-to-image
- lora
- diffusers
- template:diffusion-lora
widget:
- text: '-'
  output:
    url: images/roo logo - Copy.PNG
base_model: black-forest-labs/FLUX.1-dev
---
# Baby Roo

<Gallery />

## Model description 



![images.jpg](https:&#x2F;&#x2F;cdn-uploads.huggingface.co&#x2F;production&#x2F;uploads&#x2F;6783dbf11da7cdb6f9f48422&#x2F;ohyGdTthSqT2T0M66wsPM.jpeg)



## Download model


[Download](/rangapriyabatchu/black-forest-labsFLUX.1-dev/tree/main) them in the Files & versions tab.
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/rangapriyabatchu/black-forest-labsFLUX.1-dev/discussions> ;
    schema:distribution "The distribution of the model can be found in the Files & versions tab."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/rangapriyabatchu/black-forest-labsFLUX.1-dev"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0b6c077d0e7e512fe4a09f8d65944ef93c47c0c822ba512f9d69a0205fbc4e04>,
        <http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9>,
        <http://mlentory.zbmed.de/mlentory_graph/f57740f97820ddb55f90898c07a60a736e9a9030d5eac125af5c595ad0638c8f>,
        "base_model:adapter:black-forest-labs/FLUX.1-dev"^^xsd:string,
        "base_model:black-forest-labs/FLUX.1-dev"^^xsd:string,
        "region:us"^^xsd:string,
        "template:diffusion-lora"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "The maintainer of the model is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The context provided does not contain any information about the processor requirements for the model."^^xsd:string ;
    schema:releaseNotes """---
tags:
- text-to-image
- lora
- diffusers
- template:diffusion-lora
widget:
- text: '-'
  output:
    url: images/roo logo - Copy.PNG
base_model: black-forest-labs/FLUX.1-dev
---
# Baby Roo

<Gallery />

## Model description 



![images.jpg](https:&#x2F;&#x2F;cdn-uploads.huggingface.co&#x2F;production&#x2F;uploads&#x2F;6783dbf11da7cdb6f9f48422&#x2F;ohyGdTthSqT2T0M66wsPM.jpeg)



## Download model


[Download](/rangapriyabatchu/black-forest-labsFLUX.1-dev/tree/main) them in the Files & versions tab.
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "The context provided does not contain any information about storage requirements or the storage space needed for the model."^^xsd:string ;
    schema:url <https://huggingface.co/rangapriyabatchu/black-forest-labsFLUX.1-dev> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/rangapriyabatchu/black-forest-labsFLUX.1-dev/discussions> ;
    ns1:readme <https://huggingface.co/rangapriyabatchu/black-forest-labsFLUX.1-dev/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ae9fe5515248b617c925bc0baf03221977a299d211fdc4bb5fbd16b7b1b3a132> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/151c5c18796e684c7f0c77ded700ed4efddb5eb060b8924db2428896c9cebd7c>,
        <http://mlentory.zbmed.de/mlentory_graph/5c500bb7c0078d862991837b70c69f9bce83ad76967f92453efb08964a791605> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/a464d85abf29c8b2f8dc299e9fe390a547c8450e42b7ab0ccaac88fdcaeaaa84> ;
    ns2:modelCategory "Information not found"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/4407cda96af0afeaf4c1c5aaacc2cddb4eccb07bfeb1a5dded4a788a19846707> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/151c5c18796e684c7f0c77ded700ed4efddb5eb060b8924db2428896c9cebd7c>,
        <http://mlentory.zbmed.de/mlentory_graph/5c500bb7c0078d862991837b70c69f9bce83ad76967f92453efb08964a791605> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/151c5c18796e684c7f0c77ded700ed4efddb5eb060b8924db2428896c9cebd7c>,
        <http://mlentory.zbmed.de/mlentory_graph/5c500bb7c0078d862991837b70c69f9bce83ad76967f92453efb08964a791605> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/distilbert/distilbert-base-uncased> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "The conditions that affect the availability of, or method(s) of access to, an item are not explicitly mentioned in the provided context."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-05-06T13:44:53+00:00"^^xsd:dateTime ;
    schema:datePublished "2022-03-02T23:29:04+00:00"^^xsd:dateTime ;
    schema:description """---
language: en
tags:
- exbert
license: apache-2.0
datasets:
- bookcorpus
- wikipedia
---

# DistilBERT base model (uncased)

This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was
introduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found
[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does
not make a difference between english and English.

## Model description

DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a
self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,
with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic
process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained
with three objectives:

- Distillation loss: the model was trained to return the same probabilities as the BERT base model.
- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a
  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the
  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that
  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future
  tokens. It allows the model to learn a bidirectional representation of the sentence.
- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base
  model.

This way, the model learns the same inner representation of the English language than its teacher model, while being
faster for inference or downstream tasks.

## Intended uses & limitations

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for
fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

You can use this model directly with a pipeline for masked language modeling:

```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')
>>> unmasker("Hello I'm a [MASK] model.")

[{'sequence': "[CLS] hello i'm a role model. [SEP]",
  'score': 0.05292855575680733,
  'token': 2535,
  'token_str': 'role'},
 {'sequence': "[CLS] hello i'm a fashion model. [SEP]",
  'score': 0.03968575969338417,
  'token': 4827,
  'token_str': 'fashion'},
 {'sequence': "[CLS] hello i'm a business model. [SEP]",
  'score': 0.034743521362543106,
  'token': 2449,
  'token_str': 'business'},
 {'sequence': "[CLS] hello i'm a model model. [SEP]",
  'score': 0.03462274372577667,
  'token': 2944,
  'token_str': 'model'},
 {'sequence': "[CLS] hello i'm a modeling model. [SEP]",
  'score': 0.018145186826586723,
  'token': 11643,
  'token_str': 'modeling'}]
```

Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import DistilBertTokenizer, DistilBertModel
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained("distilbert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

and in TensorFlow:

```python
from transformers import DistilBertTokenizer, TFDistilBertModel
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

### Limitations and bias

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased
predictions. It also inherits some of
[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).

```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')
>>> unmasker("The White man worked as a [MASK].")

[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',
  'score': 0.1235365942120552,
  'token': 20987,
  'token_str': 'blacksmith'},
 {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',
  'score': 0.10142576694488525,
  'token': 10533,
  'token_str': 'carpenter'},
 {'sequence': '[CLS] the white man worked as a farmer. [SEP]',
  'score': 0.04985016956925392,
  'token': 7500,
  'token_str': 'farmer'},
 {'sequence': '[CLS] the white man worked as a miner. [SEP]',
  'score': 0.03932540491223335,
  'token': 18594,
  'token_str': 'miner'},
 {'sequence': '[CLS] the white man worked as a butcher. [SEP]',
  'score': 0.03351764753460884,
  'token': 14998,
  'token_str': 'butcher'}]

>>> unmasker("The Black woman worked as a [MASK].")

[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',
  'score': 0.13283951580524445,
  'token': 13877,
  'token_str': 'waitress'},
 {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',
  'score': 0.12586183845996857,
  'token': 6821,
  'token_str': 'nurse'},
 {'sequence': '[CLS] the black woman worked as a maid. [SEP]',
  'score': 0.11708822101354599,
  'token': 10850,
  'token_str': 'maid'},
 {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',
  'score': 0.11499975621700287,
  'token': 19215,
  'token_str': 'prostitute'},
 {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',
  'score': 0.04722772538661957,
  'token': 22583,
  'token_str': 'housekeeper'}]
```

This bias will also affect all fine-tuned versions of this model.

## Training data

DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset
consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)
(excluding lists, tables and headers).

## Training procedure

### Preprocessing

The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are
then of the form:

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.

The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

### Pretraining

The model was trained on 8 16 GB V100 for 90 hours. See the
[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters
details.

## Evaluation results

When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |


### BibTeX entry and citation info

```bibtex
@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}
```

<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased">
	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/distilbert/distilbert-base-uncased/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/distilbert/distilbert-base-uncased"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/02d5fe1af629b32020e3f74d866aa66a5001c17461f60e2ed7bb4a7224a40870>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/5e74eb4ff0b0e3cb5d0cf817eab16d7070fe8abf2bdfe5b5b62577201e86b1ea>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/9c51ba772888465129d6bda3b0b520a3a16960b8c108a8cd2276eb4fb85ee5f2>,
        <http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21>,
        <http://mlentory.zbmed.de/mlentory_graph/bc53f85f6e664d1c6ece992a9d80b2e0705d1b3c75028563adb2ed9f36aedec9>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "arxiv:1910.01108"^^xsd:string,
        "dataset:bookcorpus"^^xsd:string,
        "dataset:wikipedia"^^xsd:string,
        "en"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string,
        "tf"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language: en
tags:
- exbert
license: apache-2.0
datasets:
- bookcorpus
- wikipedia
---

# DistilBERT base model (uncased)

This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was
introduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found
[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does
not make a difference between english and English.

## Model description

DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a
self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,
with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic
process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained
with three objectives:

- Distillation loss: the model was trained to return the same probabilities as the BERT base model.
- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a
  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the
  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that
  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future
  tokens. It allows the model to learn a bidirectional representation of the sentence.
- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base
  model.

This way, the model learns the same inner representation of the English language than its teacher model, while being
faster for inference or downstream tasks.

## Intended uses & limitations

You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for
fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.

### How to use

You can use this model directly with a pipeline for masked language modeling:

```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')
>>> unmasker("Hello I'm a [MASK] model.")

[{'sequence': "[CLS] hello i'm a role model. [SEP]",
  'score': 0.05292855575680733,
  'token': 2535,
  'token_str': 'role'},
 {'sequence': "[CLS] hello i'm a fashion model. [SEP]",
  'score': 0.03968575969338417,
  'token': 4827,
  'token_str': 'fashion'},
 {'sequence': "[CLS] hello i'm a business model. [SEP]",
  'score': 0.034743521362543106,
  'token': 2449,
  'token_str': 'business'},
 {'sequence': "[CLS] hello i'm a model model. [SEP]",
  'score': 0.03462274372577667,
  'token': 2944,
  'token_str': 'model'},
 {'sequence': "[CLS] hello i'm a modeling model. [SEP]",
  'score': 0.018145186826586723,
  'token': 11643,
  'token_str': 'modeling'}]
```

Here is how to use this model to get the features of a given text in PyTorch:

```python
from transformers import DistilBertTokenizer, DistilBertModel
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained("distilbert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

and in TensorFlow:

```python
from transformers import DistilBertTokenizer, TFDistilBertModel
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```

### Limitations and bias

Even if the training data used for this model could be characterized as fairly neutral, this model can have biased
predictions. It also inherits some of
[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).

```python
>>> from transformers import pipeline
>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')
>>> unmasker("The White man worked as a [MASK].")

[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',
  'score': 0.1235365942120552,
  'token': 20987,
  'token_str': 'blacksmith'},
 {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',
  'score': 0.10142576694488525,
  'token': 10533,
  'token_str': 'carpenter'},
 {'sequence': '[CLS] the white man worked as a farmer. [SEP]',
  'score': 0.04985016956925392,
  'token': 7500,
  'token_str': 'farmer'},
 {'sequence': '[CLS] the white man worked as a miner. [SEP]',
  'score': 0.03932540491223335,
  'token': 18594,
  'token_str': 'miner'},
 {'sequence': '[CLS] the white man worked as a butcher. [SEP]',
  'score': 0.03351764753460884,
  'token': 14998,
  'token_str': 'butcher'}]

>>> unmasker("The Black woman worked as a [MASK].")

[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',
  'score': 0.13283951580524445,
  'token': 13877,
  'token_str': 'waitress'},
 {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',
  'score': 0.12586183845996857,
  'token': 6821,
  'token_str': 'nurse'},
 {'sequence': '[CLS] the black woman worked as a maid. [SEP]',
  'score': 0.11708822101354599,
  'token': 10850,
  'token_str': 'maid'},
 {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',
  'score': 0.11499975621700287,
  'token': 19215,
  'token_str': 'prostitute'},
 {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',
  'score': 0.04722772538661957,
  'token': 22583,
  'token_str': 'housekeeper'}]
```

This bias will also affect all fine-tuned versions of this model.

## Training data

DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset
consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)
(excluding lists, tables and headers).

## Training procedure

### Preprocessing

The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are
then of the form:

```
[CLS] Sentence A [SEP] Sentence B [SEP]
```

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in
the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a
consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two
"sentences" has a combined length of less than 512 tokens.

The details of the masking procedure for each sentence are the following:
- 15% of the tokens are masked.
- In 80% of the cases, the masked tokens are replaced by `[MASK]`.
- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
- In the 10% remaining cases, the masked tokens are left as is.

### Pretraining

The model was trained on 8 16 GB V100 for 90 hours. See the
[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters
details.

## Evaluation results

When fine-tuned on downstream tasks, this model achieves the following results:

Glue test results:

| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |
|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|
|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |


### BibTeX entry and citation info

```bibtex
@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}
```

<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased">
	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">
</a>
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/distilbert/distilbert-base-uncased> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/distilbert/distilbert-base-uncased/discussions> ;
    ns1:readme <https://huggingface.co/distilbert/distilbert-base-uncased/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/96b96e1fc1370c6454f55ef5bb235e3108a8e1b32665ef0b72dd54ba220b3a53> .

<http://mlentory.zbmed.de/mlentory_graph/ax> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/ax/hypothesis>,
        <http://mlentory.zbmed.de/mlentory_graph/ax/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/ax/label>,
        <http://mlentory.zbmed.de/mlentory_graph/ax/premise>,
        <http://mlentory.zbmed.de/mlentory_graph/ax/split> ;
    schema:description "nyu-mll/glue - 'ax' subset"@en ;
    schema:name "ax"@en .

<http://mlentory.zbmed.de/mlentory_graph/ax_splits> a ns3:RecordSet ;
    ns3:data "[{\"ax_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/ax_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/ax_splits/split_name> ;
    schema:description "Splits for the ax config."@en ;
    schema:name "ax_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/b07372cae2aaf834044064c3f58684a47c8f4685d11fccf85dbb95dac4dc7141> a schema:DefinedTerm ;
    schema:description "Neural building blocks for speaker diarization in Python, supporting voice activity detection and speaker embedding."^^xsd:string ;
    schema:name "pyannote.audio"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b47d8ed02d0fc1a7d72d5d61255296300a82f2caa55dd809c63448bfdd9b4284> a schema:DefinedTerm ;
    schema:description "Analyzes and learns from graph-structured data."^^xsd:string ;
    schema:name "Graph Machine Learning"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b565e6331a7ce1976735adb717df1dcf762f187f64d858f90b35c0d5b21a7df4> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/647af92c158ef5819b55c9c11a9565f9e413cf785be01a1c5d583087394c22e7> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/mistralai/Mistral-7B-v0.3> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "The context provided does not contain any information about contributors or related sections."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-05-22T09:56:38+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-07-24T14:00:32+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-22T09:56:38+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
extra_gated_description: If you want to learn more about how we process your personal
  data, please read our <a href="https://mistral.ai/fr/terms/">Privacy Policy</a>.
---

# Model Card for Mistral-7B-v0.3

The Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-v0.2/edit/main/README.md)
- Extended vocabulary to 32768

## Installation

It is recommended to use `mistralai/Mistral-7B-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Demo

After installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.

```
mistral-demo $HOME/mistral_models/7B-v0.3
```

Should give something along the following lines:

```
This is a test of the emergency broadcast system. This is only a test.

If this were a real emergency, you would be told what to do.

This is a test
=====================
This is another test of the new blogging software. I’m not sure if I’m going to keep it or not. I’m not sure if I’m going to keep
=====================
This is a third test, mistral AI is very good at testing. 🙂

This is a third test, mistral AI is very good at testing. 🙂

This
=====================
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "mistralai/Mistral-7B-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id)
inputs = tokenizer("Hello my name is", return_tensors="pt")

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/mistralai/Mistral-7B-v0.3/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/mistralai/Mistral-7B-v0.3"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by Mistral-7B-v0.3 are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The text does not provide specific information about the processor requirements for running the Mistral-7B-v0.3 model."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
extra_gated_description: If you want to learn more about how we process your personal
  data, please read our <a href="https://mistral.ai/fr/terms/">Privacy Policy</a>.
---

# Model Card for Mistral-7B-v0.3

The Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-v0.2/edit/main/README.md)
- Extended vocabulary to 32768

## Installation

It is recommended to use `mistralai/Mistral-7B-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Demo

After installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.

```
mistral-demo $HOME/mistral_models/7B-v0.3
```

Should give something along the following lines:

```
This is a test of the emergency broadcast system. This is only a test.

If this were a real emergency, you would be told what to do.

This is a test
=====================
This is another test of the new blogging software. I’m not sure if I’m going to keep it or not. I’m not sure if I’m going to keep
=====================
This is a third test, mistral AI is very good at testing. 🙂

This is a third test, mistral AI is very good at testing. 🙂

This
=====================
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "mistralai/Mistral-7B-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id)
inputs = tokenizer("Hello my name is", return_tensors="pt")

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/mistralai/Mistral-7B-v0.3> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/mistralai/Mistral-7B-v0.3/discussions> ;
    ns1:readme <https://huggingface.co/mistralai/Mistral-7B-v0.3/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b6bffbd25ee4f19a8e55b6c7f32040744865e1e7d74080964d19fac6f207948c> a schema:DefinedTerm ;
    schema:description "Lightweight solution for mobile and embedded devices to run optimized TensorFlow models with low latency."^^xsd:string ;
    schema:name "TF Lite"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b9cbedab515a4ba3d4c78d2aaebae3eaaa730c9bfc49212dad2910d0f2c63595> a schema:DefinedTerm ;
    schema:description "Categorizes audio clips into predefined classes."^^xsd:string ;
    schema:name "Audio Classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/bbf4caca758174ac9ca1258e1174d6c6d75a330218b88807385547a31f8229ab> a schema:DefinedTerm ;
    schema:description "Comprehensive machine learning library for Python featuring various classification, regression, and clustering algorithms."^^xsd:string ;
    schema:name "Scikit-learn"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/bc69543009040d6e8dba73b201af25ed291f7933acca2056091eb5f643c4f919> a schema:DefinedTerm ;
    schema:description "Models based on RoBERTa (Robustly Optimized BERT Approach), an optimized BERT variant with improved pretraining and performance."^^xsd:string ;
    schema:name "roberta"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/c0097ccf601f8114782f9c0c86386c6c995d5b2e12eddde21fdd7251761ee1fd> a schema:DefinedTerm ;
    schema:description "Framework for running ML workloads on Habana Gaudi accelerators with performance optimization."^^xsd:string ;
    schema:name "Habana"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/c05716ce58cab40a4f006dc8e3e39faa02b579cc15205b363bb6c5688773d37e> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/e50ce491d4abac67bd512ffe733d1ed10412db83802a3b15e0cc40b0bcc5203a> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Model Card for Mistral-7B-Instruct-v0.3 > Limitations: Model Card for Mistral-7B-Instruct-v0.3 > Limitations:
The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.
Model Card for Mistral-7B-Instruct-v0.3 - List: - Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended"""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/647af92c158ef5819b55c9c11a9565f9e413cf785be01a1c5d583087394c22e7> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "The model card for Mistral-7B-Instruct-v0.3 does not provide specific usage instructions. However, it does mention that the model is a quick demonstration of how the base model can be fine-tuned for compelling performance. It also highlights the lack of moderation mechanisms and the community's interest in developing ways to make the model respect guardrails for deployment in moderated environments. The model card does not provide any direct usage instructions."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-05-22T09:57:04+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-08-21T12:18:25+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-05-22T09:57:04+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model: mistralai/Mistral-7B-v0.3
extra_gated_description: If you want to learn more about how we process your personal
  data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>.
---

# Model Card for Mistral-7B-Instruct-v0.3

The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)
- Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling

## Installation

It is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-Instruct-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Chat

After installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using

```
mistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256
```

### Instruct following

```py
from mistral_inference.transformer import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(messages=[UserMessage(content="Explain Machine Learning to me in a nutshell.")])

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

### Function calling

```py
from mistral_common.protocol.instruct.tool_calls import Function, Tool
from mistral_inference.transformer import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(
    tools=[
        Tool(
            function=Function(
                name="get_current_weather",
                description="Get the current weather",
                parameters={
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "The temperature unit to use. Infer this from the users location.",
                        },
                    },
                    "required": ["location", "format"],
                },
            )
        )
    ],
    messages=[
        UserMessage(content="What's the weather like today in Paris?"),
        ],
)

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import pipeline

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]
chatbot = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.3")
chatbot(messages)
```


## Function calling with `transformers`

To use this example, you'll need `transformers` version 4.42.0 or higher. Please see the 
[function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)
in the `transformers` docs for more information.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)

def get_current_weather(location: str, format: str):
    \"\"\"
    Get the current weather

    Args:
        location: The city and state, e.g. San Francisco, CA
        format: The temperature unit to use. Infer this from the users location. (choices: ["celsius", "fahrenheit"])
    \"\"\"
    pass

conversation = [{"role": "user", "content": "What's the weather like in Paris?"}]
tools = [get_current_weather]


# format and tokenize the tool use prompt 
inputs = tokenizer.apply_chat_template(
            conversation,
            tools=tools,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
)

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")

inputs.to(model.device)
outputs = model.generate(**inputs, max_new_tokens=1000)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

Note that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool
results to the chat history so that the model can use them in its next generation. For a full tool calling example, please
see the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), 
and note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be
exactly 9 alphanumeric characters.


## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:finetune:mistralai/Mistral-7B-v0.3"^^xsd:string,
        "base_model:mistralai/Mistral-7B-v0.3"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the Mistral-7B-Instruct-v0.3 model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The text does not provide information about the processor requirements for running the Mistral-7B-Instruct-v0.3 model."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model: mistralai/Mistral-7B-v0.3
extra_gated_description: If you want to learn more about how we process your personal
  data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>.
---

# Model Card for Mistral-7B-Instruct-v0.3

The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.

Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)
- Extended vocabulary to 32768
- Supports v3 Tokenizer
- Supports function calling

## Installation

It is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

```
pip install mistral_inference
```

## Download

```py
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-Instruct-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### Chat

After installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using

```
mistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256
```

### Instruct following

```py
from mistral_inference.transformer import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(messages=[UserMessage(content="Explain Machine Learning to me in a nutshell.")])

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

### Function calling

```py
from mistral_common.protocol.instruct.tool_calls import Function, Tool
from mistral_inference.transformer import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(
    tools=[
        Tool(
            function=Function(
                name="get_current_weather",
                description="Get the current weather",
                parameters={
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "The temperature unit to use. Infer this from the users location.",
                        },
                    },
                    "required": ["location", "format"],
                },
            )
        )
    ],
    messages=[
        UserMessage(content="What's the weather like today in Paris?"),
        ],
)

tokens = tokenizer.encode_chat_completion(completion_request).tokens

out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

print(result)
```

## Generate with `transformers`

If you want to use Hugging Face `transformers` to generate text, you can do something like this.

```py
from transformers import pipeline

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]
chatbot = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.3")
chatbot(messages)
```


## Function calling with `transformers`

To use this example, you'll need `transformers` version 4.42.0 or higher. Please see the 
[function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)
in the `transformers` docs for more information.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)

def get_current_weather(location: str, format: str):
    \"\"\"
    Get the current weather

    Args:
        location: The city and state, e.g. San Francisco, CA
        format: The temperature unit to use. Infer this from the users location. (choices: ["celsius", "fahrenheit"])
    \"\"\"
    pass

conversation = [{"role": "user", "content": "What's the weather like in Paris?"}]
tools = [get_current_weather]


# format and tokenize the tool use prompt 
inputs = tokenizer.apply_chat_template(
            conversation,
            tools=tools,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
)

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")

inputs.to(model.device)
outputs = model.generate(**inputs, max_new_tokens=1000)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

Note that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool
results to the chat history so that the model can use them in its next generation. For a full tool calling example, please
see the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), 
and note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be
exactly 9 alphanumeric characters.


## Limitations

The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

## The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "The text does not provide specific software requirements for the Mistral-7B-Instruct-v0.3 model. It only mentions that the model is a quick demonstration of the base model being easily fine-tuned for compelling performance and does not have moderation mechanisms. It also mentions the intention to engage with the community on ways to make the model respect guardrails for deployment in moderated environments."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions> ;
    ns1:readme <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/c6a31d1f7013eb41349bc590b0cc16d51b0a1706fa3f70c1337a8eecf06d2173> a schema:DefinedTerm ;
    schema:description "Identifies and locates objects within images using bounding boxes."^^xsd:string ;
    schema:name "Object Detection"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/c7062dc4cfa463be9c92783a4d2c8abb9a4b37f71f4caef7c07cd6787e1843ed> a schema:DefinedTerm ;
    schema:description "Transforms static images into video sequences."^^xsd:string ;
    schema:name "Imageto Video"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/cba5458c023bd0e333fefb47511914b963716a71b0356b435370871727585038> a schema:DefinedTerm ;
    schema:description "Generates video content based on textual descriptions."^^xsd:string ;
    schema:name "Text to Video"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/cdcffd045905817b078927df6b1a309b4e719b889a1d7212107359dd87b570d2> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """The context provided does not contain information about the intended use of the creative work.

Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: The context provided does not contain information about the intended use of the creative work.

Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: The context provided does not contain information about the intended use of the creative work.

Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question"""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "The context provided does not contain information about the model category."^^xsd:string ;
    ns2:modelRisks "The potential risks associated with the model are bias, risks, and limitations."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/20a607de6d3028bc24821288287faa1f8bf92a37433fb7df54105674128d8a68> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions """To get started with the model, follow these steps:

1. Install the model: Download and install the model from the provided link.
2. Set up the environment: Ensure that your development environment is set up correctly.
3. Load the model: Load the model into your development environment.
4. Fine-tune the model: Fine-tune the model using the provided training data.
5. Test the model: Test the model to ensure it is working correctly.

For direct use, you can use the model in your own applications or projects. However, it is important to follow the guidelines and best practices outlined in the Responsible Use Guide to ensure that the model is used responsibly and safely.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct - Par. 1: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: To get started with the model, follow these steps:

1. Install the model: Download and install the model from the provided link.
2. Set up the environment: Ensure that your development environment is set up correctly.
3. Load the model: Load the model into your development environment.
4. Fine-tune the model: Fine-tune the model using the provided training data.
5. Test the model: Test the model to ensure it is working correctly.

For direct use, you can use the model in your own applications or projects. However, it is important to follow the guidelines and best practices outlined in the Responsible Use Guide to ensure that the model is used responsibly and safely.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct - Par. 1: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: To get started with the model, follow these steps:

1. Install the model: Download and install the model from the provided link.
2. Set up the environment: Ensure that your development environment is set up correctly.
3. Load the model: Load the model into your development environment.
4. Fine-tune the model: Fine-tune the model using the provided training data.
5. Test the model: Test the model to ensure it is working correctly.

For direct use, you can use the model in your own applications or projects. However, it is important to follow the guidelines and best practices outlined in the Responsible Use Guide to ensure that the model is used responsibly and safely.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct - Par. 1: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as"""^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit> ;
    schema:author "The author of this content is not explicitly mentioned in the provided text."^^xsd:string ;
    schema:citation """Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Question: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: Citation  ; Model Sources 

Based *only"""^^xsd:string ;
    schema:conditionsOfAccess """Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:"""^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-04-18T16:53:35+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-11-22T07:09:31+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-04-18T16:53:35+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- en
library_name: transformers
license: llama3
tags:
- llama-3
- llama
- meta
- facebook
- unsloth
- transformers
base_model: meta-llama/Llama-3-8B-Instruct
---

# Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!

We have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png" width="200"/>](https://discord.gg/unsloth)
[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)

## ✨ Finetune for Free

All notebooks are **beginner friendly**! Add your dataset, click "Run All", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.

| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
| **Llama-3.2 (3B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |
| **Llama-3.2 (11B vision)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing)               | 2x faster | 60% less |
| **Llama-3.1 (8B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |
| **Qwen2 VL (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing)               | 1.8x faster | 60% less |
| **Qwen2.5 (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1Kose-ucXO1IBaZq5BvbwWieuubP7hxvQ?usp=sharing)               | 2x faster | 60% less |
| **Phi-3.5 (mini)** | [▶️ Start on Colab](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |
| **Gemma 2 (9B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)               | 2.4x faster | 58% less |
| **Mistral (7B)**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |
| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png" width="200"/>](https://docs.unsloth.ai)

- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.

## Special Thanks
A huge thank you to the Meta and Llama team for creating and releasing these models.

## Model Details

Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. 

**Model developers** Meta

**Variations** Llama 3 comes in two sizes — 8B and 70B parameters — in pre-trained and instruction tuned variants.

**Input** Models input text only.

**Output** Models generate text and code only.

**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.


<table>
  <tr>
   <td>
   </td>
   <td><strong>Training Data</strong>
   </td>
   <td><strong>Params</strong>
   </td>
   <td><strong>Context length</strong>
   </td>
   <td><strong>GQA</strong>
   </td>
   <td><strong>Token count</strong>
   </td>
   <td><strong>Knowledge cutoff</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2" >Llama 3
   </td>
   <td rowspan="2" >A new mix of publicly available online data.
   </td>
   <td>8B
   </td>
   <td>8k
   </td>
   <td>Yes
   </td>
   <td rowspan="2" >15T+
   </td>
   <td>March, 2023
   </td>
  </tr>
  <tr>
   <td>70B
   </td>
   <td>8k
   </td>
   <td>Yes
   </td>
   <td>December, 2023
   </td>
  </tr>
</table>


**Llama 3 family of models**. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date** April 18, 2024.

**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.

**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)

Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). 


## Intended Use

**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.

**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.

**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.

## How to use

This repository contains two versions of Meta-Llama-3-70B-Instruct, for use with transformers and with the original `llama3` codebase.

### Use with transformers

See the snippet below for usage with Transformers:

```python
import transformers
import torch

model_id = "meta-llama/Meta-Llama-3-70B-Instruct"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device="auto",
)

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

prompt = pipeline.tokenizer.apply_chat_template(
		messages, 
		tokenize=False, 
		add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
print(outputs[0]["generated_text"][len(prompt):])
```

### Use with `llama3`

Please, follow the instructions in the [repository](https://github.com/meta-llama/llama3).

To download Original checkpoints, see the example command below leveraging `huggingface-cli`:

```
huggingface-cli download meta-llama/Meta-Llama-3-70B-Instruct --include "original/*" --local-dir Meta-Llama-3-70B-Instruct
```

For Hugging Face support, we recommend using transformers or TGI, but a similar command works.

## Hardware and Software

**Training Factors** We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.

**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program.


<table>
  <tr>
   <td>
   </td>
   <td><strong>Time (GPU hours)</strong>
   </td>
   <td><strong>Power Consumption (W)</strong>
   </td>
   <td><strong>Carbon Emitted(tCO2eq)</strong>
   </td>
  </tr>
  <tr>
   <td>Llama 3 8B
   </td>
   <td>1.3M
   </td>
   <td>700
   </td>
   <td>390
   </td>
  </tr>
  <tr>
   <td>Llama 3 70B
   </td>
   <td>6.4M
   </td>
   <td>700
   </td>
   <td>1900
   </td>
  </tr>
  <tr>
   <td>Total
   </td>
   <td>7.7M
   </td>
   <td>
   </td>
   <td>2290
   </td>
  </tr>
</table>



**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.


## Training Data

**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.

**Data Freshness** The pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. 


## Benchmarks 

In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).


### Base pretrained models


<table>
  <tr>
   <td><strong>Category</strong>
   </td>
   <td><strong>Benchmark</strong>
   </td>
   <td><strong>Llama 3 8B</strong>
   </td>
   <td><strong>Llama2 7B</strong>
   </td>
   <td><strong>Llama2 13B</strong>
   </td>
   <td><strong>Llama 3 70B</strong>
   </td>
   <td><strong>Llama2 70B</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="6" >General
   </td>
   <td>MMLU (5-shot)
   </td>
   <td>66.6
   </td>
   <td>45.7
   </td>
   <td>53.8
   </td>
   <td>79.5
   </td>
   <td>69.7
   </td>
  </tr>
  <tr>
   <td>AGIEval English (3-5 shot)
   </td>
   <td>45.9
   </td>
   <td>28.8
   </td>
   <td>38.7
   </td>
   <td>63.0
   </td>
   <td>54.8
   </td>
  </tr>
  <tr>
   <td>CommonSenseQA (7-shot)
   </td>
   <td>72.6
   </td>
   <td>57.6
   </td>
   <td>67.6
   </td>
   <td>83.8
   </td>
   <td>78.7
   </td>
  </tr>
  <tr>
   <td>Winogrande (5-shot)
   </td>
   <td>76.1
   </td>
   <td>73.3
   </td>
   <td>75.4
   </td>
   <td>83.1
   </td>
   <td>81.8
   </td>
  </tr>
  <tr>
   <td>BIG-Bench Hard (3-shot, CoT)
   </td>
   <td>61.1
   </td>
   <td>38.1
   </td>
   <td>47.0
   </td>
   <td>81.3
   </td>
   <td>65.7
   </td>
  </tr>
  <tr>
   <td>ARC-Challenge (25-shot)
   </td>
   <td>78.6
   </td>
   <td>53.7
   </td>
   <td>67.6
   </td>
   <td>93.0
   </td>
   <td>85.3
   </td>
  </tr>
  <tr>
   <td>Knowledge reasoning
   </td>
   <td>TriviaQA-Wiki (5-shot)
   </td>
   <td>78.5
   </td>
   <td>72.1
   </td>
   <td>79.6
   </td>
   <td>89.7
   </td>
   <td>87.5
   </td>
  </tr>
  <tr>
   <td rowspan="4" >Reading comprehension
   </td>
   <td>SQuAD (1-shot)
   </td>
   <td>76.4
   </td>
   <td>72.2
   </td>
   <td>72.1
   </td>
   <td>85.6
   </td>
   <td>82.6
   </td>
  </tr>
  <tr>
   <td>QuAC (1-shot, F1)
   </td>
   <td>44.4
   </td>
   <td>39.6
   </td>
   <td>44.9
   </td>
   <td>51.1
   </td>
   <td>49.4
   </td>
  </tr>
  <tr>
   <td>BoolQ (0-shot)
   </td>
   <td>75.7
   </td>
   <td>65.5
   </td>
   <td>66.9
   </td>
   <td>79.0
   </td>
   <td>73.1
   </td>
  </tr>
  <tr>
   <td>DROP (3-shot, F1)
   </td>
   <td>58.4
   </td>
   <td>37.9
   </td>
   <td>49.8
   </td>
   <td>79.7
   </td>
   <td>70.2
   </td>
  </tr>
</table>



### Instruction tuned models


<table>
  <tr>
   <td><strong>Benchmark</strong>
   </td>
   <td><strong>Llama 3 8B</strong>
   </td>
   <td><strong>Llama 2 7B</strong>
   </td>
   <td><strong>Llama 2 13B</strong>
   </td>
   <td><strong>Llama 3 70B</strong>
   </td>
   <td><strong>Llama 2 70B</strong>
   </td>
  </tr>
  <tr>
   <td>MMLU (5-shot)
   </td>
   <td>68.4
   </td>
   <td>34.1
   </td>
   <td>47.8
   </td>
   <td>82.0
   </td>
   <td>52.9
   </td>
  </tr>
  <tr>
   <td>GPQA (0-shot)
   </td>
   <td>34.2
   </td>
   <td>21.7
   </td>
   <td>22.3
   </td>
   <td>39.5
   </td>
   <td>21.0
   </td>
  </tr>
  <tr>
   <td>HumanEval (0-shot)
   </td>
   <td>62.2
   </td>
   <td>7.9
   </td>
   <td>14.0
   </td>
   <td>81.7
   </td>
   <td>25.6
   </td>
  </tr>
  <tr>
   <td>GSM-8K (8-shot, CoT)
   </td>
   <td>79.6
   </td>
   <td>25.7
   </td>
   <td>77.4
   </td>
   <td>93.0
   </td>
   <td>57.5
   </td>
  </tr>
  <tr>
   <td>MATH (4-shot, CoT)
   </td>
   <td>30.0
   </td>
   <td>3.8
   </td>
   <td>6.7
   </td>
   <td>50.4
   </td>
   <td>11.6
   </td>
  </tr>
</table>



### Responsibility & Safety

We believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.

Foundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. 

Rather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. 


As part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.


#### Llama 3-Instruct

As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 

<span style="text-decoration:underline;">Safety</span>

For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 

<span style="text-decoration:underline;">Refusals</span>

In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 

We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 


#### Responsible release 

In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 

Misuse

If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).


#### Critical risks 

<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)

We have conducted a two fold assessment of the safety of the model in this area:



* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.
* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).


### <span style="text-decoration:underline;">Cyber Security </span>

We have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). 


### <span style="text-decoration:underline;">Child Safety</span>

Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 


### Community 

Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). 

Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. 


## Ethical Considerations and Limitations

The core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. 

But Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. 

Please see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)


## Citation instructions

@article{llama3modelcard,

  title={Llama 3 Model Card},

  author={AI@Meta},

  year={2024},

  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}

}

## Contributors

Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit"^^xsd:string ;
    schema:inLanguage "en"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0813ac63520e9dbe708eec9898230a9dbdb1c48971b07fa1ad53fb6dfaa7a564>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/11ee21b926795a2b7d2cb2d939044519ace34ac86e6b6a59a47dcb4bba5eb7a4>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/2976f620ce615218c4caaaa64d292b6c1cc2785cda2fe2fcc1fabefc0c73faef>,
        <http://mlentory.zbmed.de/mlentory_graph/547c2a6f152d09c8e3dbd3a857c5481c4c4d77accf781e6fe237d9c49f47d9a7>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/759dcfce11b186065b46fb94fef1c569da9001c123861ba72ba0708e554bd6c4>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/d77765c1bc2a45cffe4e1ace66ea68c61a819d17e7d6159e6840523d288dbed3>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "en"^^xsd:string,
        "license:llama3"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "llama3"^^xsd:string ;
    schema:maintainer """Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > <span style="text-decoration:underline;">Child Safety</span>: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > <span style="text-decoration:underline;">Child Safety</span>:
Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 
### Community 
Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). 
Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. 
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct - Par. 1: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)
We have conducted a two fold assessment of the safety of the model in this area:

Question: Find the maintainer in the following text, here is a description of the property: (Individual responsible for maintaining the item.) here are some related sections: Model Card Contact ; Model Card Authors 

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > <span style="text-decoration:underline;">Child Safety</span>: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > <span style="text-decoration:underline;">Child Safety</span>:
Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 
### Community 
Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). 
Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. 
Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct - Par. 1: Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth! > Training Data > Responsibility & Safety > Llama 3-Instruct:
As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 
<span style="text-decoration:underline;">Safety</span>
For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 
<span style="text-decoration:underline;">Refusals</span>
In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 
We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 
#### Responsible release 
In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 
Misuse
If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).
#### Critical risks 
<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)"""^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the Llama technology are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The context provided does not contain information about the processor requirements for running the application."^^xsd:string ;
    schema:releaseNotes """---
language:
- en
library_name: transformers
license: llama3
tags:
- llama-3
- llama
- meta
- facebook
- unsloth
- transformers
base_model: meta-llama/Llama-3-8B-Instruct
---

# Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!

We have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png" width="200"/>](https://discord.gg/unsloth)
[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)

## ✨ Finetune for Free

All notebooks are **beginner friendly**! Add your dataset, click "Run All", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.

| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
| **Llama-3.2 (3B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |
| **Llama-3.2 (11B vision)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing)               | 2x faster | 60% less |
| **Llama-3.1 (8B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |
| **Qwen2 VL (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing)               | 1.8x faster | 60% less |
| **Qwen2.5 (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1Kose-ucXO1IBaZq5BvbwWieuubP7hxvQ?usp=sharing)               | 2x faster | 60% less |
| **Phi-3.5 (mini)** | [▶️ Start on Colab](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |
| **Gemma 2 (9B)**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)               | 2.4x faster | 58% less |
| **Mistral (7B)**    | [▶️ Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |
| **DPO - Zephyr**     | [▶️ Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |

[<img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png" width="200"/>](https://docs.unsloth.ai)

- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.
- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.

## Special Thanks
A huge thank you to the Meta and Llama team for creating and releasing these models.

## Model Details

Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. 

**Model developers** Meta

**Variations** Llama 3 comes in two sizes — 8B and 70B parameters — in pre-trained and instruction tuned variants.

**Input** Models input text only.

**Output** Models generate text and code only.

**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.


<table>
  <tr>
   <td>
   </td>
   <td><strong>Training Data</strong>
   </td>
   <td><strong>Params</strong>
   </td>
   <td><strong>Context length</strong>
   </td>
   <td><strong>GQA</strong>
   </td>
   <td><strong>Token count</strong>
   </td>
   <td><strong>Knowledge cutoff</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2" >Llama 3
   </td>
   <td rowspan="2" >A new mix of publicly available online data.
   </td>
   <td>8B
   </td>
   <td>8k
   </td>
   <td>Yes
   </td>
   <td rowspan="2" >15T+
   </td>
   <td>March, 2023
   </td>
  </tr>
  <tr>
   <td>70B
   </td>
   <td>8k
   </td>
   <td>Yes
   </td>
   <td>December, 2023
   </td>
  </tr>
</table>


**Llama 3 family of models**. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date** April 18, 2024.

**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.

**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)

Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). 


## Intended Use

**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.

**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.

**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.

## How to use

This repository contains two versions of Meta-Llama-3-70B-Instruct, for use with transformers and with the original `llama3` codebase.

### Use with transformers

See the snippet below for usage with Transformers:

```python
import transformers
import torch

model_id = "meta-llama/Meta-Llama-3-70B-Instruct"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device="auto",
)

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

prompt = pipeline.tokenizer.apply_chat_template(
		messages, 
		tokenize=False, 
		add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
print(outputs[0]["generated_text"][len(prompt):])
```

### Use with `llama3`

Please, follow the instructions in the [repository](https://github.com/meta-llama/llama3).

To download Original checkpoints, see the example command below leveraging `huggingface-cli`:

```
huggingface-cli download meta-llama/Meta-Llama-3-70B-Instruct --include "original/*" --local-dir Meta-Llama-3-70B-Instruct
```

For Hugging Face support, we recommend using transformers or TGI, but a similar command works.

## Hardware and Software

**Training Factors** We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.

**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program.


<table>
  <tr>
   <td>
   </td>
   <td><strong>Time (GPU hours)</strong>
   </td>
   <td><strong>Power Consumption (W)</strong>
   </td>
   <td><strong>Carbon Emitted(tCO2eq)</strong>
   </td>
  </tr>
  <tr>
   <td>Llama 3 8B
   </td>
   <td>1.3M
   </td>
   <td>700
   </td>
   <td>390
   </td>
  </tr>
  <tr>
   <td>Llama 3 70B
   </td>
   <td>6.4M
   </td>
   <td>700
   </td>
   <td>1900
   </td>
  </tr>
  <tr>
   <td>Total
   </td>
   <td>7.7M
   </td>
   <td>
   </td>
   <td>2290
   </td>
  </tr>
</table>



**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.


## Training Data

**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.

**Data Freshness** The pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. 


## Benchmarks 

In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).


### Base pretrained models


<table>
  <tr>
   <td><strong>Category</strong>
   </td>
   <td><strong>Benchmark</strong>
   </td>
   <td><strong>Llama 3 8B</strong>
   </td>
   <td><strong>Llama2 7B</strong>
   </td>
   <td><strong>Llama2 13B</strong>
   </td>
   <td><strong>Llama 3 70B</strong>
   </td>
   <td><strong>Llama2 70B</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="6" >General
   </td>
   <td>MMLU (5-shot)
   </td>
   <td>66.6
   </td>
   <td>45.7
   </td>
   <td>53.8
   </td>
   <td>79.5
   </td>
   <td>69.7
   </td>
  </tr>
  <tr>
   <td>AGIEval English (3-5 shot)
   </td>
   <td>45.9
   </td>
   <td>28.8
   </td>
   <td>38.7
   </td>
   <td>63.0
   </td>
   <td>54.8
   </td>
  </tr>
  <tr>
   <td>CommonSenseQA (7-shot)
   </td>
   <td>72.6
   </td>
   <td>57.6
   </td>
   <td>67.6
   </td>
   <td>83.8
   </td>
   <td>78.7
   </td>
  </tr>
  <tr>
   <td>Winogrande (5-shot)
   </td>
   <td>76.1
   </td>
   <td>73.3
   </td>
   <td>75.4
   </td>
   <td>83.1
   </td>
   <td>81.8
   </td>
  </tr>
  <tr>
   <td>BIG-Bench Hard (3-shot, CoT)
   </td>
   <td>61.1
   </td>
   <td>38.1
   </td>
   <td>47.0
   </td>
   <td>81.3
   </td>
   <td>65.7
   </td>
  </tr>
  <tr>
   <td>ARC-Challenge (25-shot)
   </td>
   <td>78.6
   </td>
   <td>53.7
   </td>
   <td>67.6
   </td>
   <td>93.0
   </td>
   <td>85.3
   </td>
  </tr>
  <tr>
   <td>Knowledge reasoning
   </td>
   <td>TriviaQA-Wiki (5-shot)
   </td>
   <td>78.5
   </td>
   <td>72.1
   </td>
   <td>79.6
   </td>
   <td>89.7
   </td>
   <td>87.5
   </td>
  </tr>
  <tr>
   <td rowspan="4" >Reading comprehension
   </td>
   <td>SQuAD (1-shot)
   </td>
   <td>76.4
   </td>
   <td>72.2
   </td>
   <td>72.1
   </td>
   <td>85.6
   </td>
   <td>82.6
   </td>
  </tr>
  <tr>
   <td>QuAC (1-shot, F1)
   </td>
   <td>44.4
   </td>
   <td>39.6
   </td>
   <td>44.9
   </td>
   <td>51.1
   </td>
   <td>49.4
   </td>
  </tr>
  <tr>
   <td>BoolQ (0-shot)
   </td>
   <td>75.7
   </td>
   <td>65.5
   </td>
   <td>66.9
   </td>
   <td>79.0
   </td>
   <td>73.1
   </td>
  </tr>
  <tr>
   <td>DROP (3-shot, F1)
   </td>
   <td>58.4
   </td>
   <td>37.9
   </td>
   <td>49.8
   </td>
   <td>79.7
   </td>
   <td>70.2
   </td>
  </tr>
</table>



### Instruction tuned models


<table>
  <tr>
   <td><strong>Benchmark</strong>
   </td>
   <td><strong>Llama 3 8B</strong>
   </td>
   <td><strong>Llama 2 7B</strong>
   </td>
   <td><strong>Llama 2 13B</strong>
   </td>
   <td><strong>Llama 3 70B</strong>
   </td>
   <td><strong>Llama 2 70B</strong>
   </td>
  </tr>
  <tr>
   <td>MMLU (5-shot)
   </td>
   <td>68.4
   </td>
   <td>34.1
   </td>
   <td>47.8
   </td>
   <td>82.0
   </td>
   <td>52.9
   </td>
  </tr>
  <tr>
   <td>GPQA (0-shot)
   </td>
   <td>34.2
   </td>
   <td>21.7
   </td>
   <td>22.3
   </td>
   <td>39.5
   </td>
   <td>21.0
   </td>
  </tr>
  <tr>
   <td>HumanEval (0-shot)
   </td>
   <td>62.2
   </td>
   <td>7.9
   </td>
   <td>14.0
   </td>
   <td>81.7
   </td>
   <td>25.6
   </td>
  </tr>
  <tr>
   <td>GSM-8K (8-shot, CoT)
   </td>
   <td>79.6
   </td>
   <td>25.7
   </td>
   <td>77.4
   </td>
   <td>93.0
   </td>
   <td>57.5
   </td>
  </tr>
  <tr>
   <td>MATH (4-shot, CoT)
   </td>
   <td>30.0
   </td>
   <td>3.8
   </td>
   <td>6.7
   </td>
   <td>50.4
   </td>
   <td>11.6
   </td>
  </tr>
</table>



### Responsibility & Safety

We believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.

Foundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. 

Rather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. 


As part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.


#### Llama 3-Instruct

As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. 

<span style="text-decoration:underline;">Safety</span>

For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. 

<span style="text-decoration:underline;">Refusals</span>

In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. 

We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. 


#### Responsible release 

In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. 

Misuse

If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).


#### Critical risks 

<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)

We have conducted a two fold assessment of the safety of the model in this area:



* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.
* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).


### <span style="text-decoration:underline;">Cyber Security </span>

We have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). 


### <span style="text-decoration:underline;">Child Safety</span>

Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 


### Community 

Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). 

Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. 


## Ethical Considerations and Limitations

The core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. 

But Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. 

Please see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)


## Citation instructions

@article{llama3modelcard,

  title={Llama 3 Model Card},

  author={AI@Meta},

  year={2024},

  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}

}

## Contributors

Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit/discussions> ;
    ns1:readme <https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/cdd8c6c259dd8671b4aa4f8bffef48584021d8fed14ae4105db312b9dc54cda6> a ns2:ML_Model ;
    ns2:ethicalLegalSocial """Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release - List: - Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release - List: - Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release - List: - Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
...

Question: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved"""^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release - List: - Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release - List: - Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release - List: - Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 
...

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
<span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release: <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span> > Release:
- Currently only the SFT STEVE-R1 model with step-verified training data is released."""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/2f28cc27d9dac219caa56a5425741f7391153065f1558fe7418d6ccc1c598fb4>,
        <http://mlentory.zbmed.de/mlentory_graph/49bf7b39c8a2a4cb84ab43bc3c320de6e0c1da1253ebf022a5d0a84737581119> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/410526dc3d4eb3818f7e3b0c216684c878292ea29533262bc0ca37ef3921278a> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/Fanbin/STEVE-R1-7B-SFT> ;
    schema:author "The author of this content is Fanbin Lu."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2025-03-16T23:12:39+00:00"^^xsd:dateTime ;
    schema:dateModified "2025-03-21T16:39:31+00:00"^^xsd:dateTime ;
    schema:datePublished "2025-03-16T23:12:39+00:00"^^xsd:dateTime ;
    schema:description """---
pipeline_tag: robotics
library_name: transformers
license: apache-2.0
tags: [robotics, agent, computer-vision, llm]
---
# <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span>

[![Hugging Face Paper](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Paper-orange)](https://huggingface.co/papers/2503.12532)
[![Hugging Face Models](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue)](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT)
[![Hugging Face Data](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green)](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories)


We evaluate the performance of the **STEVE-R1 agent** on both in-domain WindowsAgentArena (Windows 11 OS) and out-of-domain OSWorld (Ubuntu OS) benchmarks. The evaluation involves 16 attempts per task, with task completion rates recorded as the primary metric. In the in-domain Windows 11 setting, the STEVE-R1 agent demonstrated a **14%** higher task completion rate compared to the previous open-source state-of-the-art model, UI-TARS-7B-DPO. Furthermore, in the out-of-domain Ubuntu OS environment, where STEVE-R1 was not explicitly trained, it still achieved a **7%** higher task completion rate than UI-TARS-7B-DPO.

<div align=center>
<img width="98%" src="assets/performance.png"/>
</div>

## Release
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 


## Performance
| Method |  WinAgentArena | OSWorld |
|--------|-------------------|------------------|
| UI-TARS-7B-DPO (20 steps) | 15.4 ± 1.6 | 14.6 ± 1.0 |
| UI-TARS-7B-DPO (40 steps) | 17.8 ± 1.3 | 15.7 ± 0.8 |
| UI-TARS-7B-DPO (60 steps) | 19.3 ± 1.6 | 16.2 ± 1.0 |
| **Our Model**  | | |
| STEVE-R1-SFT (20 steps) | 17.5 ± 2.0 | 9.6 ± 1.1（OOD）|
| STEVE-R1-SFT (40 steps) | 20.1 ± 2.2 | 11.5 ± 1.2 (OOD) |
| STEVE-R1-SFT (60 steps) | **22.3** ± 2.1 | 12.8 ± 1.2 (OOD) |
| **Multiple Trials** | | |
| UI-TARS-7B-DPO (20 steps, pass@16) | 33.1 | 25.2 |
| STEVE-R1-SFT (20 steps, pass@16) | **46.8** | **31.4** (OOD) |

## Trajectory Data
Evaluation trajectories on WindowsAgentArena and OSWorld: https://huggingface.co/datasets/Fanbin/waa_steve_trajectories

Project page: https://github.com/FanbinLu/STEVE-R1

Paper: https://arxiv.org/abs/2503.12532
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/Fanbin/STEVE-R1-7B-SFT/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/Fanbin/STEVE-R1-7B-SFT"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/02c989e969206aaa515bcdd718c32182028cc8b686a12998d33856c52d68d706>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/2f28cc27d9dac219caa56a5425741f7391153065f1558fe7418d6ccc1c598fb4>,
        <http://mlentory.zbmed.de/mlentory_graph/832a1e8e47c996aa09f9b6062fcc27e185ca1c68b47cc9b02f8457a5971d04cd>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/99b4f0e72aa605a47bc337b0c848e25811755f9eb07361970b92f3f7e04747e6>,
        <http://mlentory.zbmed.de/mlentory_graph/acdea979b59f74f49c8d4909658c7c387d308c1418907e643855f6839013271f>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/e797b8ca7ecd2d359733673fd310f9cdfce603607602506828f69a8c2bc5dda5>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "arxiv:2503.12532"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating system mentioned in the context is Windows 11."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
pipeline_tag: robotics
library_name: transformers
license: apache-2.0
tags: [robotics, agent, computer-vision, llm]
---
# <span style="font-size:30px;">STEVE-R1: Towards Long Reasoning Computer-use Agents</span>

[![Hugging Face Paper](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Paper-orange)](https://huggingface.co/papers/2503.12532)
[![Hugging Face Models](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue)](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT)
[![Hugging Face Data](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green)](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories)


We evaluate the performance of the **STEVE-R1 agent** on both in-domain WindowsAgentArena (Windows 11 OS) and out-of-domain OSWorld (Ubuntu OS) benchmarks. The evaluation involves 16 attempts per task, with task completion rates recorded as the primary metric. In the in-domain Windows 11 setting, the STEVE-R1 agent demonstrated a **14%** higher task completion rate compared to the previous open-source state-of-the-art model, UI-TARS-7B-DPO. Furthermore, in the out-of-domain Ubuntu OS environment, where STEVE-R1 was not explicitly trained, it still achieved a **7%** higher task completion rate than UI-TARS-7B-DPO.

<div align=center>
<img width="98%" src="assets/performance.png"/>
</div>

## Release
- Currently only the SFT STEVE-R1 model with step-verified training data is released. RL tunning is in progress.
- 🔥 An improved version **STEVE-R1** is released with long reasoning ability and long image context. We extend the model context length to <b>128K</b> with at most <b>32 screenshot</b> inputs for a single task. The model response length is greatly improved with deepseek-R1 distillation, see the [examples](https://github.com/FanbinLu/STEVE-R1/tree/main/examples). We release the [training data](), [models](https://huggingface.co/Fanbin/STEVE-R1-7B-SFT), and [evaluation trajectories](https://huggingface.co/datasets/Fanbin/waa_steve_trajectories).
- We release the paper of STEVE: Step Verification Pipeline for Computer-use Agent Training. We propose a single-frame computer-use 7B agent trained with SFT & step-verified KTO. 


## Performance
| Method |  WinAgentArena | OSWorld |
|--------|-------------------|------------------|
| UI-TARS-7B-DPO (20 steps) | 15.4 ± 1.6 | 14.6 ± 1.0 |
| UI-TARS-7B-DPO (40 steps) | 17.8 ± 1.3 | 15.7 ± 0.8 |
| UI-TARS-7B-DPO (60 steps) | 19.3 ± 1.6 | 16.2 ± 1.0 |
| **Our Model**  | | |
| STEVE-R1-SFT (20 steps) | 17.5 ± 2.0 | 9.6 ± 1.1（OOD）|
| STEVE-R1-SFT (40 steps) | 20.1 ± 2.2 | 11.5 ± 1.2 (OOD) |
| STEVE-R1-SFT (60 steps) | **22.3** ± 2.1 | 12.8 ± 1.2 (OOD) |
| **Multiple Trials** | | |
| UI-TARS-7B-DPO (20 steps, pass@16) | 33.1 | 25.2 |
| STEVE-R1-SFT (20 steps, pass@16) | **46.8** | **31.4** (OOD) |

## Trajectory Data
Evaluation trajectories on WindowsAgentArena and OSWorld: https://huggingface.co/datasets/Fanbin/waa_steve_trajectories

Project page: https://github.com/FanbinLu/STEVE-R1

Paper: https://arxiv.org/abs/2503.12532
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/Fanbin/STEVE-R1-7B-SFT> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/Fanbin/STEVE-R1-7B-SFT/discussions> ;
    ns1:readme <https://huggingface.co/Fanbin/STEVE-R1-7B-SFT/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/ce33825b7c1e5d75f33a4642e9c0161cb4b86ef24e68f4f773320780da0d589d> .

<http://mlentory.zbmed.de/mlentory_graph/cola> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/cola/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/cola/label>,
        <http://mlentory.zbmed.de/mlentory_graph/cola/sentence>,
        <http://mlentory.zbmed.de/mlentory_graph/cola/split> ;
    schema:description """nyu-mll/glue - 'cola' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "cola"@en .

<http://mlentory.zbmed.de/mlentory_graph/cola_splits> a ns3:RecordSet ;
    ns3:data "[{\"cola_splits/split_name\":\"train\"},{\"cola_splits/split_name\":\"validation\"},{\"cola_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/cola_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/cola_splits/split_name> ;
    schema:description "Splits for the cola config."@en ;
    schema:name "cola_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/d1054f9c274823fbeb3d1ab19d0f2344791cee18915cdb21fb1437bba03ba8fb> a schema:DefinedTerm ;
    schema:description "Predicts continuous values from tabular data."^^xsd:string ;
    schema:name "Tabular Regression"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d1c48824743e55222d5add11f150796e52972e3cdc769aab9a1e0a6b888755e2> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "Information not found."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/3b4b981961ee604488ce9db2b6cea45a4fb777fb6ee8ef76bb274e14ff6cbdf4> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks """Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: Bias, Risks, and Limitations

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found"."""^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/b3a667bbf00faf3c261e35a53645ea532c3043c18a276bfc23c24105f625022d> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions """Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the"""^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/Shridipta-06/dqn-SpaceInvadersNoFrameskip-v4> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found"^^xsd:string ;
    schema:conditionsOfAccess """Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Environment Arguments - Par. 1: Environment Arguments:
**DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo) - Par. 1: **DQN** Agent playing **SpaceInvadersNoFrameskip-v4** > Training (with the RL Zoo):

Question: Find the conditions Of Access in the following text, here is a description of the property: ("""^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2023-06-25T18:57:16+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-06-25T18:57:49+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-06-25T18:57:16+00:00"^^xsd:dateTime ;
    schema:description """---
library_name: stable-baselines3
tags:
- SpaceInvadersNoFrameskip-v4
- deep-reinforcement-learning
- reinforcement-learning
- stable-baselines3
model-index:
- name: DQN
  results:
  - task:
      type: reinforcement-learning
      name: reinforcement-learning
    dataset:
      name: SpaceInvadersNoFrameskip-v4
      type: SpaceInvadersNoFrameskip-v4
    metrics:
    - type: mean_reward
      value: 487.00 +/- 104.38
      name: mean_reward
      verified: false
---

# **DQN** Agent playing **SpaceInvadersNoFrameskip-v4**
This is a trained model of a **DQN** agent playing **SpaceInvadersNoFrameskip-v4**
using the [stable-baselines3 library](https://github.com/DLR-RM/stable-baselines3)
and the [RL Zoo](https://github.com/DLR-RM/rl-baselines3-zoo).

The RL Zoo is a training framework for Stable Baselines3
reinforcement learning agents,
with hyperparameter optimization and pre-trained agents included.

## Usage (with SB3 RL Zoo)

RL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo<br/>
SB3: https://github.com/DLR-RM/stable-baselines3<br/>
SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib

Install the RL Zoo (with SB3 and SB3-Contrib):
```bash
pip install rl_zoo3
```

```
# Download model and save it into the logs/ folder
python -m rl_zoo3.load_from_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -orga Shridipta-06 -f logs/
python -m rl_zoo3.enjoy --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/
```

If you installed the RL Zoo3 via pip (`pip install rl_zoo3`), from anywhere you can do:
```
python -m rl_zoo3.load_from_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -orga Shridipta-06 -f logs/
python -m rl_zoo3.enjoy --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/
```

## Training (with the RL Zoo)
```
python -m rl_zoo3.train --algo dqn --env SpaceInvadersNoFrameskip-v4 -f logs/
# Upload the model and generate video (when possible)
python -m rl_zoo3.push_to_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -f logs/ -orga Shridipta-06
```

## Hyperparameters
```python
OrderedDict([('batch_size', 32),
             ('buffer_size', 100000),
             ('env_wrapper',
              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),
             ('exploration_final_eps', 0.01),
             ('exploration_fraction', 0.1),
             ('frame_stack', 4),
             ('gradient_steps', 1),
             ('learning_rate', 0.0001),
             ('learning_starts', 100000),
             ('n_timesteps', 1000000),
             ('optimize_memory_usage', False),
             ('policy', 'CnnPolicy'),
             ('target_update_interval', 1000),
             ('train_freq', 4),
             ('normalize', False)])
```

# Environment Arguments
```python
{'render_mode': 'rgb_array'}
```
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/Shridipta-06/dqn-SpaceInvadersNoFrameskip-v4/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/Shridipta-06/dqn-SpaceInvadersNoFrameskip-v4"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/13e6e4095523310627190177e08c0206ca97e76f4c4565c15c2b35270daa2588>,
        <http://mlentory.zbmed.de/mlentory_graph/2529572666d8ae917752f418f0b008b820cf821a3ba355927ca159454740dce7>,
        <http://mlentory.zbmed.de/mlentory_graph/345644230f7e711605ef233127803f06659ae1bd200eadaf0bb48fd111eb8366>,
        <http://mlentory.zbmed.de/mlentory_graph/594a6f3cf66106516dc4e80f44dbe528cdcf0fb397d75a65d1b3b2a81dfc0f92>,
        <http://mlentory.zbmed.de/mlentory_graph/a3c9ca93ab7ec29d067dd25733037f174c1326ef487292e5e1c9eea5cb35feb1>,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the DQN agent playing SpaceInvadersNoFrameskip-v4 with the RL Zoo are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
library_name: stable-baselines3
tags:
- SpaceInvadersNoFrameskip-v4
- deep-reinforcement-learning
- reinforcement-learning
- stable-baselines3
model-index:
- name: DQN
  results:
  - task:
      type: reinforcement-learning
      name: reinforcement-learning
    dataset:
      name: SpaceInvadersNoFrameskip-v4
      type: SpaceInvadersNoFrameskip-v4
    metrics:
    - type: mean_reward
      value: 487.00 +/- 104.38
      name: mean_reward
      verified: false
---

# **DQN** Agent playing **SpaceInvadersNoFrameskip-v4**
This is a trained model of a **DQN** agent playing **SpaceInvadersNoFrameskip-v4**
using the [stable-baselines3 library](https://github.com/DLR-RM/stable-baselines3)
and the [RL Zoo](https://github.com/DLR-RM/rl-baselines3-zoo).

The RL Zoo is a training framework for Stable Baselines3
reinforcement learning agents,
with hyperparameter optimization and pre-trained agents included.

## Usage (with SB3 RL Zoo)

RL Zoo: https://github.com/DLR-RM/rl-baselines3-zoo<br/>
SB3: https://github.com/DLR-RM/stable-baselines3<br/>
SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib

Install the RL Zoo (with SB3 and SB3-Contrib):
```bash
pip install rl_zoo3
```

```
# Download model and save it into the logs/ folder
python -m rl_zoo3.load_from_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -orga Shridipta-06 -f logs/
python -m rl_zoo3.enjoy --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/
```

If you installed the RL Zoo3 via pip (`pip install rl_zoo3`), from anywhere you can do:
```
python -m rl_zoo3.load_from_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -orga Shridipta-06 -f logs/
python -m rl_zoo3.enjoy --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/
```

## Training (with the RL Zoo)
```
python -m rl_zoo3.train --algo dqn --env SpaceInvadersNoFrameskip-v4 -f logs/
# Upload the model and generate video (when possible)
python -m rl_zoo3.push_to_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -f logs/ -orga Shridipta-06
```

## Hyperparameters
```python
OrderedDict([('batch_size', 32),
             ('buffer_size', 100000),
             ('env_wrapper',
              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),
             ('exploration_final_eps', 0.01),
             ('exploration_fraction', 0.1),
             ('frame_stack', 4),
             ('gradient_steps', 1),
             ('learning_rate', 0.0001),
             ('learning_starts', 100000),
             ('n_timesteps', 1000000),
             ('optimize_memory_usage', False),
             ('policy', 'CnnPolicy'),
             ('target_update_interval', 1000),
             ('train_freq', 4),
             ('normalize', False)])
```

# Environment Arguments
```python
{'render_mode': 'rgb_array'}
```
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/Shridipta-06/dqn-SpaceInvadersNoFrameskip-v4> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/Shridipta-06/dqn-SpaceInvadersNoFrameskip-v4/discussions> ;
    ns1:readme <https://huggingface.co/Shridipta-06/dqn-SpaceInvadersNoFrameskip-v4/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d23ec8688e43c1025edafa9cadf8e3c0e07c919e0962b88134cd692dc7c80e8c> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/21e8f9819743fa391136eeebe9af8ccf402fb92306dd81a3e0b578785278c62a> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended uses and limitations of full_vanilla_doff_dpo_iter_3 are not explicitly stated in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found"^^xsd:string ;
    ns2:modelRisks "Information not found"^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/c372f0bca81211b35cd0974aabe8500d5ca4479cddab1685b44f24bd5523dc9e> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292>,
        <http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_3> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess """Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters - Par. 1: full_vanilla_doff_dpo_iter_3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections"""^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2024-07-03T20:15:50+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-07-03T23:01:23+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-07-03T20:15:50+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
base_model: YYYYYYibo/full_vanilla_doff_dpo_iter_2
tags:
- alignment-handbook
- generated_from_trainer
- trl
- dpo
- generated_from_trainer
datasets:
- updated
- original
model-index:
- name: full_vanilla_doff_dpo_iter_3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# full_vanilla_doff_dpo_iter_3

This model is a fine-tuned version of [YYYYYYibo/full_vanilla_doff_dpo_iter_2](https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2) on the updated and the original datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2+cu121
- Datasets 2.14.6
- Tokenizers 0.15.2
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_3/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_3"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/267e04ff20e54a667a0c17e69b7dd3f07b14ad451d8a605d3b5b771693ba73ba>,
        <http://mlentory.zbmed.de/mlentory_graph/28f6fc79227e13d54a24f63fe2a8971b6f281ac3ca778d4303f84baacbc18a18>,
        <http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/89b5833251f8f95309bb1d81c7cbd5e686d9f1e1a94b81869c2661ff9301fb64>,
        <http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "base_model:YYYYYYibo/full_vanilla_doff_dpo_iter_2"^^xsd:string,
        "base_model:finetune:YYYYYYibo/full_vanilla_doff_dpo_iter_2"^^xsd:string,
        "dataset:original"^^xsd:string,
        "dataset:updated"^^xsd:string,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
base_model: YYYYYYibo/full_vanilla_doff_dpo_iter_2
tags:
- alignment-handbook
- generated_from_trainer
- trl
- dpo
- generated_from_trainer
datasets:
- updated
- original
model-index:
- name: full_vanilla_doff_dpo_iter_3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# full_vanilla_doff_dpo_iter_3

This model is a fine-tuned version of [YYYYYYibo/full_vanilla_doff_dpo_iter_2](https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2) on the updated and the original datasets.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2+cu121
- Datasets 2.14.6
- Tokenizers 0.15.2
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_3> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_3/discussions> ;
    ns1:readme <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_3/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d6f86ddd52b22c26a6970ea3473a2f8e657e9600a3cea6dd33e7307bf848d475> a schema:DefinedTerm ;
    schema:description "All-in-one toolkit for speech technology research, including ASR, speaker recognition, and speech enhancement."^^xsd:string ;
    schema:name "speechbrain"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d75a16357850d2896a768db42c912a9487fd97a50f3358217d5bbb602c39b195> a schema:DefinedTerm ;
    schema:description "Indicates a HuggingFace Space demo exists for this model allowing interactive testing without setup."^^xsd:string ;
    schema:name "Has a Space"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/db86f2c8ce693db878b689231690fab2fe37b7ae1abc1e54dca9fae35bc0df0d> a schema:DefinedTerm ;
    schema:description "Transcribes spoken language into written text."^^xsd:string ;
    schema:name "Automatic Speech Recognition"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/dc96ea445d125873bc02183accca8dffafbc58c6100167ef5b113919f5afb57e> a schema:DefinedTerm ;
    schema:description "Generates 3D models or scenes from textual descriptions."^^xsd:string ;
    schema:name "Text to 3D"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/dd477ce37bc2c08950a1d845aa243dff8c0e03de15cc42b9b35295d514538a84> a schema:DefinedTerm ;
    schema:description "Converts text from one language to another."^^xsd:string ;
    schema:name "Translation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/default> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/default/album>,
        <http://mlentory.zbmed.de/mlentory_graph/default/artist>,
        <http://mlentory.zbmed.de/mlentory_graph/default/chosen>,
        <http://mlentory.zbmed.de/mlentory_graph/default/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/default/label>,
        <http://mlentory.zbmed.de/mlentory_graph/default/lyrics>,
        <http://mlentory.zbmed.de/mlentory_graph/default/lyrics_length>,
        <http://mlentory.zbmed.de/mlentory_graph/default/messages>,
        <http://mlentory.zbmed.de/mlentory_graph/default/number>,
        <http://mlentory.zbmed.de/mlentory_graph/default/prompt>,
        <http://mlentory.zbmed.de/mlentory_graph/default/prompt_id>,
        <http://mlentory.zbmed.de/mlentory_graph/default/rejected>,
        <http://mlentory.zbmed.de/mlentory_graph/default/score_chosen>,
        <http://mlentory.zbmed.de/mlentory_graph/default/score_rejected>,
        <http://mlentory.zbmed.de/mlentory_graph/default/sentence>,
        <http://mlentory.zbmed.de/mlentory_graph/default/split>,
        <http://mlentory.zbmed.de/mlentory_graph/default/text>,
        <http://mlentory.zbmed.de/mlentory_graph/default/title> ;
    schema:description """HuggingFaceH4/ultrachat_200k - 'default' subset

Additional information:
- 4 splits: train_sft, test_sft, train_gen, test_gen"""@en,
        """HuggingFaceH4/ultrafeedback_binarized - 'default' subset

Additional information:
- 6 splits: train_prefs, train_sft, test_prefs, test_sft, train_gen, test_gen"""@en,
        """cmotions/Beatles_lyrics - 'default' subset

Additional information:
- 2 splits: dataset_cleaned, dataset_full"""@en,
        "huggingartists/fascinoma - 'default' subset"@en,
        """stanfordnlp/sst2 - 'default' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "default"@en .

<http://mlentory.zbmed.de/mlentory_graph/default_splits> a ns3:RecordSet ;
    ns3:data "[{\"default_splits/split_name\":\"dataset_cleaned\"},{\"default_splits/split_name\":\"dataset_full\"}]"^^rdf:JSON,
        "[{\"default_splits/split_name\":\"train\"},{\"default_splits/split_name\":\"validation\"},{\"default_splits/split_name\":\"test\"}]"^^rdf:JSON,
        "[{\"default_splits/split_name\":\"train\"}]"^^rdf:JSON,
        "[{\"default_splits/split_name\":\"train_prefs\"},{\"default_splits/split_name\":\"train_sft\"},{\"default_splits/split_name\":\"test_prefs\"},{\"default_splits/split_name\":\"test_sft\"},{\"default_splits/split_name\":\"train_gen\"},{\"default_splits/split_name\":\"test_gen\"}]"^^rdf:JSON,
        "[{\"default_splits/split_name\":\"train_sft\"},{\"default_splits/split_name\":\"test_sft\"},{\"default_splits/split_name\":\"train_gen\"},{\"default_splits/split_name\":\"test_gen\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/default_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/default_splits/split_name> ;
    schema:description "Splits for the default config."@en ;
    schema:name "default_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/df2c9c7dc08f5514331c35b3bf3e4da58de6a14c0724dc40f90a3e588f9844dd> a schema:DefinedTerm ;
    schema:description "Models quantized to 4-bit precision, reducing memory footprint while maintaining reasonable performance."^^xsd:string ;
    schema:name "4-bit precision"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/df3db739a5d22c6a5829694aeecdedee4e20b5804f1345689c8abd3350883e54> a schema:DefinedTerm ;
    schema:description "Indicates the model supports deployment through HuggingFace's managed inference API service for production environments."^^xsd:string ;
    schema:name "Inference Endpoints"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/df60ce658de36177b8f8629dac50a78c54092d575b441cb48625af5a266c53f4> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/9eb6952b89569e3a3863aebca62fd4619833b31bda93a0d0b5678c6627ef9275> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the FLN Artstyle model is for image generation. It can be used with various platforms such as ComfyUI, AUTOMATIC1111, SD.Next, Invoke AI, and Forge. The weights for this model are available in Safetensors format."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/a76fe889292455ddd24101fcc9c7fb0bef245d176bb34cf8fc885e4fccaf4f35> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "To use the FLN Artstyle model with ComfyUI, AUTOMATIC1111, SD.Next, Invoke AI, or Forge, you need to download the model and then follow the instructions provided in the \"How to Get Started with the Model\" section. Additionally, you can use the trigger word \"FLN Artstyle\" to trigger the image generation."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/RehanAzam/fln-artstyle> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "The text does not provide any information about the contributor in the context of the Model Card Authors or Model Details. Therefore, the answer is \"Information not found\"."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2025-02-02T14:01:10+00:00"^^xsd:dateTime ;
    schema:dateModified "2025-02-02T14:01:16+00:00"^^xsd:dateTime ;
    schema:datePublished "2025-02-02T14:01:10+00:00"^^xsd:dateTime ;
    schema:description """---
tags:
- text-to-image
- flux
- lora
- diffusers
- template:sd-lora
- fluxgym


base_model: black-forest-labs/FLUX.1-dev
instance_prompt: FLN Artstyle
license: other
license_name: flux-1-dev-non-commercial-license
license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md
---

# FLN Artstyle

A Flux LoRA trained on a local computer with [Fluxgym](https://github.com/cocktailpeanut/fluxgym)

<Gallery />

## Trigger words

You should use `FLN Artstyle` to trigger the image generation.

## Download model and use it with ComfyUI, AUTOMATIC1111, SD.Next, Invoke AI, Forge, etc.

Weights for this model are available in Safetensors format.

"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/RehanAzam/fln-artstyle/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/RehanAzam/fln-artstyle"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0b6c077d0e7e512fe4a09f8d65944ef93c47c0c822ba512f9d69a0205fbc4e04>,
        <http://mlentory.zbmed.de/mlentory_graph/10fc066544c1b06479e3cf01ac6753d82f315ef61f82986bad2e31641136cd70>,
        <http://mlentory.zbmed.de/mlentory_graph/c7504a968f417dfe1555958db6caabff413bf4f321bbb9c477d4ff3a9352a0e4>,
        <http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9>,
        <http://mlentory.zbmed.de/mlentory_graph/f57740f97820ddb55f90898c07a60a736e9a9030d5eac125af5c595ad0638c8f>,
        "base_model:adapter:black-forest-labs/FLUX.1-dev"^^xsd:string,
        "base_model:black-forest-labs/FLUX.1-dev"^^xsd:string,
        "license:other"^^xsd:string,
        "region:us"^^xsd:string,
        "template:sd-lora"^^xsd:string ;
    schema:license "other"^^xsd:string ;
    schema:maintainer "The maintainer of the FLN Artstyle model is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "The context provided does not contain any information about the processor requirements for the FLN Artstyle model."^^xsd:string ;
    schema:releaseNotes """---
tags:
- text-to-image
- flux
- lora
- diffusers
- template:sd-lora
- fluxgym


base_model: black-forest-labs/FLUX.1-dev
instance_prompt: FLN Artstyle
license: other
license_name: flux-1-dev-non-commercial-license
license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md
---

# FLN Artstyle

A Flux LoRA trained on a local computer with [Fluxgym](https://github.com/cocktailpeanut/fluxgym)

<Gallery />

## Trigger words

You should use `FLN Artstyle` to trigger the image generation.

## Download model and use it with ComfyUI, AUTOMATIC1111, SD.Next, Invoke AI, Forge, etc.

Weights for this model are available in Safetensors format.

"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "The software requirements for using the FLN Artstyle model with ComfyUI, AUTOMATIC1111, SD.Next, Invoke AI, and Forge are not explicitly mentioned in the provided context. However, it is mentioned that the weights for the model are available in Safetensors format. To use the model with these software tools, you would need to have the appropriate software installed and compatible with the model's requirements."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/RehanAzam/fln-artstyle> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/RehanAzam/fln-artstyle/discussions> ;
    ns1:readme <https://huggingface.co/RehanAzam/fln-artstyle/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/e6698fce933c7e91c8ae9aa071747ae51f7ce6252194c95729de9e885efa2582> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Considerations with regards to ethical legal and social aspects."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3."""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/9c03090b68b1e45d6bebe9b9d1ea103b6a4c064ceb71f16f08a5ae59d9bac70e> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions """As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those"""^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/meta-llama/Llama-3.2-1B> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context"""^^xsd:string ;
    schema:copyrightHolder """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found."""^^xsd:string ;
    schema:dateCreated "2024-09-18T15:03:14+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-10-24T15:08:03+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-09-18T15:03:14+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- en
- de
- fr
- it
- pt
- hi
- es
- th
library_name: transformers
pipeline_tag: text-generation
tags:
- facebook
- meta
- pytorch
- llama
- llama-3
license: llama3.2
extra_gated_prompt: "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\
  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\
  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\
  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\
  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\
  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\
  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\
  \\ of the age required under applicable laws, rules or regulations to provide legal\\
  \\ consent and that has legal authority to bind your employer or such other person\\
  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\
  \\ means the foundational large language models and software and algorithms, including\\
  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\
  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\
  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\
  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\
  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\
  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\
  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\
  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\
  \\ below or by using or distributing any portion or element of the Llama Materials,\\
  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\
  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\
  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\
  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\
  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\
  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\
  \\ Materials (or any derivative works thereof),  or a product or service (including\\
  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\
  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\
  \\ Llama” on a related website, user interface, blogpost, about page, or product\\
  \\ documentation. If you use the Llama Materials or any outputs or results of the\\
  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\
  \\ which is distributed or made available, you shall also include “Llama” at the\\
  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\
  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\
  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\
  \\ in all copies of the Llama Materials that you distribute the  following attribution\\
  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\
  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\
  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\
  \\ applicable laws and regulations (including trade compliance laws and regulations)\\
  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\
  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\
  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\
  \\ version release date, the monthly active users of the products or services made\\
  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\
  \\ monthly active users in the preceding calendar month, you must request  a license\\
  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\
  \\ authorized to exercise any of the rights under this Agreement unless or until\\
  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\
  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\
  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\
  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\
  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\
  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\
  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\
  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\
  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\
  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\
  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\
  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\
  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\
  a. No trademark licenses are granted under this Agreement, and in connection with\\
  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\
  \\ by or associated with the other or any of its affiliates,  except as required\\
  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\
  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\
  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\
  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\
  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\
  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\
  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\
  \\ respect to any derivative works and modifications of the Llama Materials that\\
  \\ are made by you, as between you and Meta, you are and will be the owner of such\\
  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\
  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\
  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\
  \\ of any of the foregoing, constitutes infringement of intellectual property or\\
  \\ other rights owned or licensable by you, then any licenses granted to you under\\
  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\
  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\
  \\ claim by any third party arising out of or related to your use or distribution\\
  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\
  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\
  \\ and will continue in full force and effect until terminated in accordance with\\
  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\
  \\ in breach of any term or condition of this Agreement. Upon termination of this\\
  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\
  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\
  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\
  \\ the State of  California without regard to choice of law principles, and the UN\\
  \\ Convention on Contracts for the International Sale of Goods does not apply to\\
  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\
  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\
  Meta is committed to promoting safe and fair use of its tools and features, including\\
  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\
  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\
  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\
  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\
  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\
  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\
  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\
  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\
  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\
  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\
  \\ illegal distribution of information or materials to minors, including obscene\\
  \\ materials, or failure to employ legally required age-gating in connection with\\
  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\
  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\
  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\
  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\
  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\
  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\
  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\
  \\ but not limited to, financial, legal, medical/health, or related professional\\
  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\
  \\ information about individuals, including information about individuals’ identity,\\
  \\ health, or demographic information, unless you have obtained the right to do so\\
  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\
  \\ generate any content that infringes, misappropriates, or otherwise violates any\\
  \\ third-party rights, including the outputs or results of any products or services\\
  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\
  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\
  \\ overburden, interfere with or impair the proper working, integrity, operation\\
  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\
  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\
  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\
  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\
  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\
  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\
  \\ applications, espionage, use for materials or activities that are subject to the\\
  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\
  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\
  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\
  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\
  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\
  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\
  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\
  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\
  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\
  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\
  \\    15. Generating, promoting, or furthering defamatory content, including the\\
  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\
  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\
  \\ without consent, authorization, or legal right\\n    18. Representing that the\\
  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\
  \\ false online engagement, including fake reviews and other means of fake online\\
  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\
  \\ of your AI system 5. Interact with third party tools, models, or software designed\\
  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\
  \\ that the outputs of such tools, models, or software are associated with Meta or\\
  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\
  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\
  \\ are not being granted to you if you are an individual domiciled in, or a company\\
  \\ with a principal place of business in, the European Union. This restriction does\\
  \\ not apply to end users of a product or service that incorporates any such multimodal\\
  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\
  \\ problems that could lead to a violation of this Policy through one of the following\\
  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\
  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\
  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\
  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\
  \\ 3.2: LlamaUseReport@meta.com"
extra_gated_fields:
  First Name: text
  Last Name: text
  Date of birth: date_picker
  Country: country
  Affiliation: text
  Job title:
    type: select
    options:
    - Student
    - Research Graduate
    - AI researcher
    - AI developer/engineer
    - Reporter
    - Other
  geo: ip_location
  ? By clicking Submit below I accept the terms of the license and acknowledge that
    the information I provide will be collected stored processed and shared in accordance
    with the Meta Privacy Policy
  : checkbox
extra_gated_description: The information you provide will be collected, stored, processed
  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).
extra_gated_button_content: Submit
---

## Model Information

The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.

**Model Developer:** Meta

**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |
| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |

**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.

**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date:** Sept 25, 2024

**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.

**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).

**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).

## Intended Use

**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.

## How to use

This repository contains two versions of Llama-3.2-1B, for use with transformers and with the original `llama` codebase.

### Use with transformers

Starting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.

Make sure to update your transformers installation via pip install --upgrade transformers.

```python
import torch
from transformers import pipeline

model_id = "meta-llama/Llama-3.2-1B"

pipe = pipeline(
    "text-generation", 
    model=model_id, 
    torch_dtype=torch.bfloat16, 
    device_map="auto"
)

pipe("The key to life is")
```

### Use with `llama`

Please, follow the instructions in the [repository](https://github.com/meta-llama/llama).

To download Original checkpoints, see the example command below leveraging `huggingface-cli`:

```
huggingface-cli download meta-llama/Llama-3.2-1B --include "original/*" --local-dir Llama-3.2-1B
```

## Hardware and Software

**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.

**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.

**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.

|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |
| :---- | :---: | ----- | :---: | :---: | :---: |
| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |
| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |
| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |
| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |
| Total | 833k |         86k |  | 240 | 0 |

\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.

The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.

## Training Data

**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).

**Data Freshness:** The pretraining data has a cutoff of December 2023\\.

## Quantization

### Quantization Scheme

We designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:
- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.
- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.
- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.


### Quantization-Aware Training and LoRA

The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).

### SpinQuant

[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.

## Benchmarks \\- English Text

In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.

### Base Pretrained Models

| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |
| ----- | ----- | :---: | :---: | :---: | :---: | :---: |
| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |
|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |
|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |
| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |
|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |
|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |
| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |

### Instruction Tuned Models

| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |
| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |
| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |
| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |
| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |
|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |
| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |
|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |
|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |
| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |
|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |
| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |
|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |
|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |
| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |

\\*\\*for comparison purposes only. Model not released.

### Multilingual Benchmarks

| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |
| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |
| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |
| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |
| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |
| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |
| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |

\\*\\*for comparison purposes only. Model not released.

## Inference time

In the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.

| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |
| :---- | ----- | ----- | ----- | ----- | ----- |
| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |
| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |
| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |
| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |
| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |
| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |

(\\*) The performance measurement is done using an adb binary-based approach.
(\\*\\*) It is measured on an Android OnePlus 12 device.
(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64

*Footnote:*

- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*
- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*
- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*
- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*
- *RSS size \\- Memory usage in resident set size (RSS)*

## Responsibility & Safety

As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

### Responsible Deployment

**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).

#### Llama 3.2 Instruct

**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).

**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.

**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.

#### Llama 3.2 Systems

**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.

### New Capabilities and Use Cases

**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.

**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.

### Evaluations

**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.

**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.

### Critical Risks

In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:

**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.

**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.

**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.
Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.

### Community

**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).

**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).

**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.

## Ethical Considerations and Limitations

**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.

**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/meta-llama/Llama-3.2-1B/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/meta-llama/Llama-3.2-1B"^^xsd:string ;
    schema:inLanguage "de"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "fr"^^xsd:string,
        "hi"^^xsd:string,
        "it"^^xsd:string,
        "pt"^^xsd:string,
        "th"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/11ee21b926795a2b7d2cb2d939044519ace34ac86e6b6a59a47dcb4bba5eb7a4>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/759dcfce11b186065b46fb94fef1c569da9001c123861ba72ba0708e554bd6c4>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/d77765c1bc2a45cffe4e1ace66ea68c61a819d17e7d6159e6840523d288dbed3>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d>,
        "arxiv:2204.05149"^^xsd:string,
        "arxiv:2405.16406"^^xsd:string,
        "de"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "fr"^^xsd:string,
        "hi"^^xsd:string,
        "it"^^xsd:string,
        "license:llama3.2"^^xsd:string,
        "pt"^^xsd:string,
        "region:us"^^xsd:string,
        "th"^^xsd:string ;
    schema:license "llama3.2"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by Llama 3.2 are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- en
- de
- fr
- it
- pt
- hi
- es
- th
library_name: transformers
pipeline_tag: text-generation
tags:
- facebook
- meta
- pytorch
- llama
- llama-3
license: llama3.2
extra_gated_prompt: "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\
  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\
  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\
  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\
  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\
  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\
  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\
  \\ of the age required under applicable laws, rules or regulations to provide legal\\
  \\ consent and that has legal authority to bind your employer or such other person\\
  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\
  \\ means the foundational large language models and software and algorithms, including\\
  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\
  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\
  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\
  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\
  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\
  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\
  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\
  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\
  \\ below or by using or distributing any portion or element of the Llama Materials,\\
  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\
  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\
  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\
  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\
  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\
  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\
  \\ Materials (or any derivative works thereof),  or a product or service (including\\
  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\
  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\
  \\ Llama” on a related website, user interface, blogpost, about page, or product\\
  \\ documentation. If you use the Llama Materials or any outputs or results of the\\
  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\
  \\ which is distributed or made available, you shall also include “Llama” at the\\
  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\
  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\
  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\
  \\ in all copies of the Llama Materials that you distribute the  following attribution\\
  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\
  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\
  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\
  \\ applicable laws and regulations (including trade compliance laws and regulations)\\
  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\
  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\
  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\
  \\ version release date, the monthly active users of the products or services made\\
  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\
  \\ monthly active users in the preceding calendar month, you must request  a license\\
  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\
  \\ authorized to exercise any of the rights under this Agreement unless or until\\
  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\
  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\
  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\
  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\
  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\
  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\
  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\
  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\
  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\
  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\
  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\
  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\
  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\
  a. No trademark licenses are granted under this Agreement, and in connection with\\
  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\
  \\ by or associated with the other or any of its affiliates,  except as required\\
  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\
  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\
  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\
  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\
  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\
  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\
  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\
  \\ respect to any derivative works and modifications of the Llama Materials that\\
  \\ are made by you, as between you and Meta, you are and will be the owner of such\\
  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\
  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\
  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\
  \\ of any of the foregoing, constitutes infringement of intellectual property or\\
  \\ other rights owned or licensable by you, then any licenses granted to you under\\
  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\
  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\
  \\ claim by any third party arising out of or related to your use or distribution\\
  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\
  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\
  \\ and will continue in full force and effect until terminated in accordance with\\
  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\
  \\ in breach of any term or condition of this Agreement. Upon termination of this\\
  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\
  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\
  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\
  \\ the State of  California without regard to choice of law principles, and the UN\\
  \\ Convention on Contracts for the International Sale of Goods does not apply to\\
  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\
  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\
  Meta is committed to promoting safe and fair use of its tools and features, including\\
  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\
  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\
  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\
  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\
  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\
  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\
  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\
  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\
  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\
  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\
  \\ illegal distribution of information or materials to minors, including obscene\\
  \\ materials, or failure to employ legally required age-gating in connection with\\
  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\
  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\
  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\
  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\
  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\
  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\
  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\
  \\ but not limited to, financial, legal, medical/health, or related professional\\
  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\
  \\ information about individuals, including information about individuals’ identity,\\
  \\ health, or demographic information, unless you have obtained the right to do so\\
  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\
  \\ generate any content that infringes, misappropriates, or otherwise violates any\\
  \\ third-party rights, including the outputs or results of any products or services\\
  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\
  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\
  \\ overburden, interfere with or impair the proper working, integrity, operation\\
  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\
  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\
  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\
  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\
  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\
  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\
  \\ applications, espionage, use for materials or activities that are subject to the\\
  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\
  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\
  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\
  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\
  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\
  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\
  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\
  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\
  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\
  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\
  \\    15. Generating, promoting, or furthering defamatory content, including the\\
  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\
  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\
  \\ without consent, authorization, or legal right\\n    18. Representing that the\\
  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\
  \\ false online engagement, including fake reviews and other means of fake online\\
  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\
  \\ of your AI system 5. Interact with third party tools, models, or software designed\\
  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\
  \\ that the outputs of such tools, models, or software are associated with Meta or\\
  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\
  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\
  \\ are not being granted to you if you are an individual domiciled in, or a company\\
  \\ with a principal place of business in, the European Union. This restriction does\\
  \\ not apply to end users of a product or service that incorporates any such multimodal\\
  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\
  \\ problems that could lead to a violation of this Policy through one of the following\\
  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\
  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\
  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\
  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\
  \\ 3.2: LlamaUseReport@meta.com"
extra_gated_fields:
  First Name: text
  Last Name: text
  Date of birth: date_picker
  Country: country
  Affiliation: text
  Job title:
    type: select
    options:
    - Student
    - Research Graduate
    - AI researcher
    - AI developer/engineer
    - Reporter
    - Other
  geo: ip_location
  ? By clicking Submit below I accept the terms of the license and acknowledge that
    the information I provide will be collected stored processed and shared in accordance
    with the Meta Privacy Policy
  : checkbox
extra_gated_description: The information you provide will be collected, stored, processed
  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).
extra_gated_button_content: Submit
---

## Model Information

The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.

**Model Developer:** Meta

**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |
| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |

**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.

**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date:** Sept 25, 2024

**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.

**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).

**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).

## Intended Use

**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.

## How to use

This repository contains two versions of Llama-3.2-1B, for use with transformers and with the original `llama` codebase.

### Use with transformers

Starting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.

Make sure to update your transformers installation via pip install --upgrade transformers.

```python
import torch
from transformers import pipeline

model_id = "meta-llama/Llama-3.2-1B"

pipe = pipeline(
    "text-generation", 
    model=model_id, 
    torch_dtype=torch.bfloat16, 
    device_map="auto"
)

pipe("The key to life is")
```

### Use with `llama`

Please, follow the instructions in the [repository](https://github.com/meta-llama/llama).

To download Original checkpoints, see the example command below leveraging `huggingface-cli`:

```
huggingface-cli download meta-llama/Llama-3.2-1B --include "original/*" --local-dir Llama-3.2-1B
```

## Hardware and Software

**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.

**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.

**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.

|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |
| :---- | :---: | ----- | :---: | :---: | :---: |
| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |
| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |
| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |
| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |
| Total | 833k |         86k |  | 240 | 0 |

\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.

The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.

## Training Data

**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).

**Data Freshness:** The pretraining data has a cutoff of December 2023\\.

## Quantization

### Quantization Scheme

We designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:
- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.
- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.
- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.


### Quantization-Aware Training and LoRA

The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).

### SpinQuant

[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.

## Benchmarks \\- English Text

In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.

### Base Pretrained Models

| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |
| ----- | ----- | :---: | :---: | :---: | :---: | :---: |
| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |
|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |
|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |
| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |
|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |
|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |
| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |

### Instruction Tuned Models

| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |
| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |
| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |
| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |
| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |
|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |
| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |
|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |
|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |
| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |
|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |
| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |
|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |
|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |
| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |

\\*\\*for comparison purposes only. Model not released.

### Multilingual Benchmarks

| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |
| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |
| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |
| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |
| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |
| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |
| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |

\\*\\*for comparison purposes only. Model not released.

## Inference time

In the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.

| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |
| :---- | ----- | ----- | ----- | ----- | ----- |
| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |
| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |
| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |
| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |
| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |
| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |

(\\*) The performance measurement is done using an adb binary-based approach.
(\\*\\*) It is measured on an Android OnePlus 12 device.
(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64

*Footnote:*

- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*
- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*
- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*
- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*
- *RSS size \\- Memory usage in resident set size (RSS)*

## Responsibility & Safety

As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

### Responsible Deployment

**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).

#### Llama 3.2 Instruct

**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).

**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.

**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.

#### Llama 3.2 Systems

**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.

### New Capabilities and Use Cases

**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.

**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.

### Evaluations

**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.

**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.

### Critical Risks

In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:

**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.

**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.

**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.
Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.

### Community

**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).

**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).

**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.

## Ethical Considerations and Limitations

**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.

**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.
"""^^xsd:string ;
    schema:softwareHelp """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the"""^^xsd:string ;
    schema:softwareRequirements """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the"""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/meta-llama/Llama-3.2-1B> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/meta-llama/Llama-3.2-1B/discussions> ;
    ns1:readme <https://huggingface.co/meta-llama/Llama-3.2-1B/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/2730e6211466c050884bcc521d8aaf9f0af8a2993b56ca3f9ab751a178b4f970>,
        <http://mlentory.zbmed.de/mlentory_graph/a787507208799647a209772884d76edfc0a14e5f209b09e235fbff7797a1a812> .

<http://mlentory.zbmed.de/mlentory_graph/e6b25e3939786aab9a6d54195c927aac37dcabc2caf513ff0e0946d4cb6cfa1c> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended uses and limitations of mt5-small-finetuned-25feb-3 are not explicitly stated in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/251690051accbb99d763e77f3aed9b94caecf82b9e2db7c17f8adfca42a4ca39>,
        <http://mlentory.zbmed.de/mlentory_graph/9abc397b7489422e962aeae20a5ed774806c8bd8f47e9008eef451699b8a61c1> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/1c89b15196e446b3deb40077389a5a66fc333e63e98fc0ec7d0581c927658fc1> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/mqy/mt5-small-finetuned-25feb-3> ;
    schema:author "The author of this content is not mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess """Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during training:

Question: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
mt5-small-finetuned-25feb-3 > Intended uses & limitations: mt5-small-finetuned-25feb-3 > Intended uses & limitations:
More information needed
mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters - Par. 1: mt5-small-finetuned-25feb-3 > Training procedure > Training hyperparameters:
The following hyperparameters were used during"""^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2023-02-25T09:32:31+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-02-25T12:26:46+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-02-25T09:32:31+00:00"^^xsd:dateTime ;
    schema:description """---
license: apache-2.0
tags:
- summarization
- generated_from_trainer
metrics:
- rouge
model-index:
- name: mt5-small-finetuned-25feb-3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# mt5-small-finetuned-25feb-3

This model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 2.3532
- Rouge1: 20.83
- Rouge2: 6.54
- Rougel: 20.4

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 9
- eval_batch_size: 9
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel |
|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|
| 5.3088        | 1.45  | 500  | 2.6216          | 17.35  | 5.18   | 17.14  |
| 3.2982        | 2.9   | 1000 | 2.5396          | 17.76  | 5.55   | 17.53  |
| 2.9211        | 4.35  | 1500 | 2.4802          | 18.72  | 5.62   | 18.47  |
| 2.8162        | 5.8   | 2000 | 2.4158          | 18.16  | 5.26   | 17.7   |
| 2.6661        | 7.25  | 2500 | 2.4387          | 18.59  | 5.38   | 18.21  |
| 2.6102        | 8.7   | 3000 | 2.4044          | 19.54  | 5.63   | 19.16  |
| 2.5043        | 10.14 | 3500 | 2.3738          | 19.65  | 5.79   | 19.16  |
| 2.4598        | 11.59 | 4000 | 2.3805          | 19.86  | 6.29   | 19.43  |
| 2.3807        | 13.04 | 4500 | 2.3590          | 20.13  | 5.91   | 19.62  |
| 2.3461        | 14.49 | 5000 | 2.3611          | 20.73  | 6.28   | 20.31  |
| 2.3024        | 15.94 | 5500 | 2.3571          | 20.64  | 6.12   | 20.25  |
| 2.2704        | 17.39 | 6000 | 2.3723          | 19.71  | 5.95   | 19.35  |
| 2.2356        | 18.84 | 6500 | 2.3532          | 20.83  | 6.54   | 20.4   |
| 2.2019        | 20.29 | 7000 | 2.3597          | 19.67  | 5.91   | 19.28  |
| 2.1646        | 21.74 | 7500 | 2.3733          | 20.63  | 6.53   | 20.24  |
| 2.1511        | 23.19 | 8000 | 2.3534          | 20.63  | 6.22   | 20.22  |
| 2.128         | 24.64 | 8500 | 2.3552          | 20.12  | 5.92   | 19.77  |
| 2.0933        | 26.09 | 9000 | 2.3587          | 20.53  | 5.88   | 20.06  |


### Framework versions

- Transformers 4.26.1
- Pytorch 1.13.1+cu116
- Datasets 2.10.0
- Tokenizers 0.13.2
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/mqy/mt5-small-finetuned-25feb-3/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/mqy/mt5-small-finetuned-25feb-3"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/251690051accbb99d763e77f3aed9b94caecf82b9e2db7c17f8adfca42a4ca39>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/974df5e3234c3601080dac4fa96eebf5d19d40df41e9e30cb61084d9fbefea1e>,
        <http://mlentory.zbmed.de/mlentory_graph/d78e76dd2fd5d1649023f6aa571601076a8d9ed012895d9c79adaea965083691>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        <http://mlentory.zbmed.de/mlentory_graph/e694cb482434e33f5aadafc7ba5607930722b2bd1b0cb252690679261c8692f4>,
        "license:apache-2.0"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "apache-2.0"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: apache-2.0
tags:
- summarization
- generated_from_trainer
metrics:
- rouge
model-index:
- name: mt5-small-finetuned-25feb-3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# mt5-small-finetuned-25feb-3

This model is a fine-tuned version of [google/mt5-small](https://huggingface.co/google/mt5-small) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 2.3532
- Rouge1: 20.83
- Rouge2: 6.54
- Rougel: 20.4

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 9
- eval_batch_size: 9
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rouge1 | Rouge2 | Rougel |
|:-------------:|:-----:|:----:|:---------------:|:------:|:------:|:------:|
| 5.3088        | 1.45  | 500  | 2.6216          | 17.35  | 5.18   | 17.14  |
| 3.2982        | 2.9   | 1000 | 2.5396          | 17.76  | 5.55   | 17.53  |
| 2.9211        | 4.35  | 1500 | 2.4802          | 18.72  | 5.62   | 18.47  |
| 2.8162        | 5.8   | 2000 | 2.4158          | 18.16  | 5.26   | 17.7   |
| 2.6661        | 7.25  | 2500 | 2.4387          | 18.59  | 5.38   | 18.21  |
| 2.6102        | 8.7   | 3000 | 2.4044          | 19.54  | 5.63   | 19.16  |
| 2.5043        | 10.14 | 3500 | 2.3738          | 19.65  | 5.79   | 19.16  |
| 2.4598        | 11.59 | 4000 | 2.3805          | 19.86  | 6.29   | 19.43  |
| 2.3807        | 13.04 | 4500 | 2.3590          | 20.13  | 5.91   | 19.62  |
| 2.3461        | 14.49 | 5000 | 2.3611          | 20.73  | 6.28   | 20.31  |
| 2.3024        | 15.94 | 5500 | 2.3571          | 20.64  | 6.12   | 20.25  |
| 2.2704        | 17.39 | 6000 | 2.3723          | 19.71  | 5.95   | 19.35  |
| 2.2356        | 18.84 | 6500 | 2.3532          | 20.83  | 6.54   | 20.4   |
| 2.2019        | 20.29 | 7000 | 2.3597          | 19.67  | 5.91   | 19.28  |
| 2.1646        | 21.74 | 7500 | 2.3733          | 20.63  | 6.53   | 20.24  |
| 2.1511        | 23.19 | 8000 | 2.3534          | 20.63  | 6.22   | 20.22  |
| 2.128         | 24.64 | 8500 | 2.3552          | 20.12  | 5.92   | 19.77  |
| 2.0933        | 26.09 | 9000 | 2.3587          | 20.53  | 5.88   | 20.06  |


### Framework versions

- Transformers 4.26.1
- Pytorch 1.13.1+cu116
- Datasets 2.10.0
- Tokenizers 0.13.2
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements """The software requirements for the model are as follows:

- Transformers 4.26.1
- Pytorch 1.13.1+cu116
- Datasets 2.10.0
- Tokenizers 0.13.2

These are the required software dependencies for the model."""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/mqy/mt5-small-finetuned-25feb-3> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/mqy/mt5-small-finetuned-25feb-3/discussions> ;
    ns1:readme <https://huggingface.co/mqy/mt5-small-finetuned-25feb-3/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ea48b603f17f8f59d09647864980b3947000697069512ec33018ee7bbb175ef2> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of the model is for generating photorealistic images of decent quality, with the ability to produce both sfw and nsfw images. The model is aimed at achieving photorealism and can be used for various purposes such as direct use, downstream use, and out-of-scope use. However, it is important to note that the model is still in the training phase and may contain artifacts and perform poorly in some cases."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> ;
    ns2:modelCategory "The model category is not explicitly mentioned in the provided context."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/11b7c117f8cd33c560f4862966654c32eb570fe472aa173c6a54749d61d20636> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions "The usage instructions for the model are not explicitly mentioned in the provided context. However, the context does mention that the model is aimed at photorealism and can produce sfw and nsfw images of decent quality. It also provides recommended negative prompts and generation parameters for the model."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/SG161222/RealVisXL_V2.02_Turbo> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "The context provided does not contain any information about the conditions of access to the model."^^xsd:string ;
    schema:contributor "The model card authors are not mentioned in the provided context."^^xsd:string ;
    schema:copyrightHolder "The copyright holder is not mentioned in the provided context."^^xsd:string ;
    schema:dateCreated "2023-12-24T12:17:13+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-10-08T16:35:56+00:00"^^xsd:dateTime ;
    schema:datePublished "2023-12-24T12:17:13+00:00"^^xsd:dateTime ;
    schema:description """---
license: openrail++
---
<strong>Check my exclusive models on Mage: </strong><a href="https://www.mage.space/play/4371756b27bf52e7a1146dc6fe2d969c" rel="noopener noreferrer nofollow"><strong>ParagonXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/df67a9f27f19629a98cb0fb619d1949a" rel="noopener noreferrer nofollow"><strong>NovaXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/d8db06ae964310acb4e090eec03984df" rel="noopener noreferrer nofollow"><strong>NovaXL Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/541da1e10976ab82976a5cacc770a413" rel="noopener noreferrer nofollow"><strong>NovaXL V2</strong></a><strong> / </strong><a href="https://www.mage.space/play/a56d2680c464ef25b8c66df126b3f706" rel="noopener noreferrer nofollow"><strong>NovaXL Pony</strong></a><strong> / </strong><a href="https://www.mage.space/play/b0ab6733c3be2408c93523d57a605371" rel="noopener noreferrer nofollow"><strong>NovaXL Pony Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/e3b01cd493ed86ed8e4708751b1c9165" rel="noopener noreferrer nofollow"><strong>RealDreamXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/ef062fc389c3f8723002428290c1158c" rel="noopener noreferrer nofollow"><strong>RealDreamXL Lightning</strong></a></p>
<b>It's important! Read it!</b><br>
The model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.<br>

<b>You can support me directly on Boosty - https://boosty.to/sg_161222</b><br>

The model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.<br>
CivitAI Page: https://civitai.com/models/139562?modelVersionId=266762<br>

<b>Recommended Negative Prompt:</b><br>
(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth<br>
<b>or another negative prompt</b><br>

<b>Recommended Generation Parameters:</b><br>
Sampling Steps: 4+<br>
Sampling Method: DPM++ SDE Karras<br>
CFG Scale: 1.5-3

<b>Recommended Hires Fix Parameters:</b><br>
Hires steps: 2+<br>
Upscaler: 4x-UltraSharp upscaler / or another<br>
Denoising strength: 0.1 - 0.5<br>
Upscale by: 1.1-2.0<br>"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/SG161222/RealVisXL_V2.02_Turbo/discussions> ;
    schema:distribution "The context provided does not contain any information about the distribution of the model."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/SG161222/RealVisXL_V2.02_Turbo"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb>,
        <http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9>,
        "diffusers:StableDiffusionXLPipeline"^^xsd:string,
        "license:openrail++"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "openrail++"^^xsd:string ;
    schema:maintainer "The maintainer of the model is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
license: openrail++
---
<strong>Check my exclusive models on Mage: </strong><a href="https://www.mage.space/play/4371756b27bf52e7a1146dc6fe2d969c" rel="noopener noreferrer nofollow"><strong>ParagonXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/df67a9f27f19629a98cb0fb619d1949a" rel="noopener noreferrer nofollow"><strong>NovaXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/d8db06ae964310acb4e090eec03984df" rel="noopener noreferrer nofollow"><strong>NovaXL Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/541da1e10976ab82976a5cacc770a413" rel="noopener noreferrer nofollow"><strong>NovaXL V2</strong></a><strong> / </strong><a href="https://www.mage.space/play/a56d2680c464ef25b8c66df126b3f706" rel="noopener noreferrer nofollow"><strong>NovaXL Pony</strong></a><strong> / </strong><a href="https://www.mage.space/play/b0ab6733c3be2408c93523d57a605371" rel="noopener noreferrer nofollow"><strong>NovaXL Pony Lightning</strong></a><strong> / </strong><a href="https://www.mage.space/play/e3b01cd493ed86ed8e4708751b1c9165" rel="noopener noreferrer nofollow"><strong>RealDreamXL</strong></a><strong> / </strong><a href="https://www.mage.space/play/ef062fc389c3f8723002428290c1158c" rel="noopener noreferrer nofollow"><strong>RealDreamXL Lightning</strong></a></p>
<b>It's important! Read it!</b><br>
The model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.<br>

<b>You can support me directly on Boosty - https://boosty.to/sg_161222</b><br>

The model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.<br>
CivitAI Page: https://civitai.com/models/139562?modelVersionId=266762<br>

<b>Recommended Negative Prompt:</b><br>
(worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth<br>
<b>or another negative prompt</b><br>

<b>Recommended Generation Parameters:</b><br>
Sampling Steps: 4+<br>
Sampling Method: DPM++ SDE Karras<br>
CFG Scale: 1.5-3

<b>Recommended Hires Fix Parameters:</b><br>
Hires steps: 2+<br>
Upscaler: 4x-UltraSharp upscaler / or another<br>
Denoising strength: 0.1 - 0.5<br>
Upscale by: 1.1-2.0<br>"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements "Information not found."^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/SG161222/RealVisXL_V2.02_Turbo> ;
    schema:version "The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/SG161222/RealVisXL_V2.02_Turbo/discussions> ;
    ns1:readme <https://huggingface.co/SG161222/RealVisXL_V2.02_Turbo/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ebbad1761fb8870fe41c968d83d4055b2799265db268338f3fa38c3849f7932a> a schema:DefinedTerm ;
    schema:description "Comprehensive deep learning platform developed by Baidu, supporting industrial applications."^^xsd:string ;
    schema:name "PaddlePaddle"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ecb27884627a31a456b10444bb8afbbcde1a4f9a3a370b5aff6fe9839d02db6c> a schema:Organization .

<http://mlentory.zbmed.de/mlentory_graph/ed99d63023153e56eb27e8249593068cacb9724a5ffea575ffb2edb1b9345012> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Considerations with regards to ethical legal and social aspects."^^xsd:string ;
    ns2:evaluatedOn "Information not found"^^xsd:string ;
    ns2:fineTunedFrom <http://mlentory.zbmed.de/mlentory_graph/9fda77bf870c60518d915563657cd9c230951c55c7abb6395dadcb0c53747922> ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse """Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the intended Use in the following text, here is a description of the property: (Purpose and intended use stated to enable users to make a decision as to the suitability of this creative work for their intended use.) here are some related sections: Uses > Direct Use ; Uses > Downstream Use  ; Uses > Out-of-Scope Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Llama 3."""^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/9c03090b68b1e45d6bebe9b9d1ea103b6a4c064ceb71f16f08a5ae59d9bac70e> ;
    ns2:testedOn "Information not found"^^xsd:string ;
    ns2:trainedOn "Information not found"^^xsd:string ;
    ns2:usageInstructions """As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: How to Get Started with the Model ; Uses > Direct Use

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those"""^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8> ;
    schema:author "The author of this content is not explicitly mentioned in the provided context."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: Model Card Authors  ; Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found."""^^xsd:string ;
    schema:copyrightHolder """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the copyright Holder in the following text, here is a description of the property: (The party holding the legal copyright to the CreativeWork.) here are some related sections: Model Details

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found."""^^xsd:string ;
    schema:dateCreated "2024-10-23T21:29:57+00:00"^^xsd:dateTime ;
    schema:dateModified "2024-11-18T20:29:29+00:00"^^xsd:dateTime ;
    schema:datePublished "2024-10-23T21:29:57+00:00"^^xsd:dateTime ;
    schema:description """---
language:
- en
- de
- fr
- it
- pt
- hi
- es
- th
pipeline_tag: text-generation
tags:
- facebook
- meta
- pytorch
- llama
- llama-3
license: llama3.2
base_model:
- meta-llama/Llama-3.2-1B
extra_gated_prompt: "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\
  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\
  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\
  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\
  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\
  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\
  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\
  \\ of the age required under applicable laws, rules or regulations to provide legal\\
  \\ consent and that has legal authority to bind your employer or such other person\\
  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\
  \\ means the foundational large language models and software and algorithms, including\\
  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\
  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\
  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\
  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\
  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\
  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\
  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\
  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\
  \\ below or by using or distributing any portion or element of the Llama Materials,\\
  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\
  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\
  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\
  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\
  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\
  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\
  \\ Materials (or any derivative works thereof),  or a product or service (including\\
  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\
  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\
  \\ Llama” on a related website, user interface, blogpost, about page, or product\\
  \\ documentation. If you use the Llama Materials or any outputs or results of the\\
  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\
  \\ which is distributed or made available, you shall also include “Llama” at the\\
  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\
  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\
  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\
  \\ in all copies of the Llama Materials that you distribute the  following attribution\\
  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\
  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\
  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\
  \\ applicable laws and regulations (including trade compliance laws and regulations)\\
  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\
  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\
  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\
  \\ version release date, the monthly active users of the products or services made\\
  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\
  \\ monthly active users in the preceding calendar month, you must request  a license\\
  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\
  \\ authorized to exercise any of the rights under this Agreement unless or until\\
  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\
  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\
  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\
  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\
  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\
  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\
  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\
  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\
  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\
  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\
  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\
  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\
  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\
  a. No trademark licenses are granted under this Agreement, and in connection with\\
  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\
  \\ by or associated with the other or any of its affiliates,  except as required\\
  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\
  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\
  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\
  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\
  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\
  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\
  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\
  \\ respect to any derivative works and modifications of the Llama Materials that\\
  \\ are made by you, as between you and Meta, you are and will be the owner of such\\
  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\
  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\
  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\
  \\ of any of the foregoing, constitutes infringement of intellectual property or\\
  \\ other rights owned or licensable by you, then any licenses granted to you under\\
  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\
  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\
  \\ claim by any third party arising out of or related to your use or distribution\\
  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\
  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\
  \\ and will continue in full force and effect until terminated in accordance with\\
  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\
  \\ in breach of any term or condition of this Agreement. Upon termination of this\\
  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\
  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\
  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\
  \\ the State of  California without regard to choice of law principles, and the UN\\
  \\ Convention on Contracts for the International Sale of Goods does not apply to\\
  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\
  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\
  Meta is committed to promoting safe and fair use of its tools and features, including\\
  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\
  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\
  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\
  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\
  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\
  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\
  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\
  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\
  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\
  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\
  \\ illegal distribution of information or materials to minors, including obscene\\
  \\ materials, or failure to employ legally required age-gating in connection with\\
  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\
  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\
  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\
  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\
  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\
  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\
  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\
  \\ but not limited to, financial, legal, medical/health, or related professional\\
  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\
  \\ information about individuals, including information about individuals’ identity,\\
  \\ health, or demographic information, unless you have obtained the right to do so\\
  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\
  \\ generate any content that infringes, misappropriates, or otherwise violates any\\
  \\ third-party rights, including the outputs or results of any products or services\\
  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\
  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\
  \\ overburden, interfere with or impair the proper working, integrity, operation\\
  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\
  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\
  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\
  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\
  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\
  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\
  \\ applications, espionage, use for materials or activities that are subject to the\\
  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\
  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\
  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\
  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\
  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\
  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\
  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\
  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\
  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\
  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\
  \\    15. Generating, promoting, or furthering defamatory content, including the\\
  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\
  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\
  \\ without consent, authorization, or legal right\\n    18. Representing that the\\
  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\
  \\ false online engagement, including fake reviews and other means of fake online\\
  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\
  \\ of your AI system 5. Interact with third party tools, models, or software designed\\
  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\
  \\ that the outputs of such tools, models, or software are associated with Meta or\\
  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\
  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\
  \\ are not being granted to you if you are an individual domiciled in, or a company\\
  \\ with a principal place of business in, the European Union. This restriction does\\
  \\ not apply to end users of a product or service that incorporates any such multimodal\\
  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\
  \\ problems that could lead to a violation of this Policy through one of the following\\
  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\
  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\
  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\
  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\
  \\ 3.2: LlamaUseReport@meta.com"
extra_gated_fields:
  First Name: text
  Last Name: text
  Date of birth: date_picker
  Country: country
  Affiliation: text
  Job title:
    type: select
    options:
    - Student
    - Research Graduate
    - AI researcher
    - AI developer/engineer
    - Reporter
    - Other
  geo: ip_location
  ? By clicking Submit below I accept the terms of the license and acknowledge that
    the information I provide will be collected stored processed and shared in accordance
    with the Meta Privacy Policy
  : checkbox
extra_gated_description: The information you provide will be collected, stored, processed
  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).
extra_gated_button_content: Submit
---


## Model Information

The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.

**Model Developer:** Meta

**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |
| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |

**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.

**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date:** Oct 24, 2024

**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.

**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).

**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).

## Intended Use

**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.

## Hardware and Software

**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.

**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.

**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.

|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |
| :---- | :---: | ----- | :---: | :---: | :---: |
| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |
| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |
| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |
| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |
| Total | 833k |         86k |  | 240 | 0 |

\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.

The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.

## Training Data

**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).

**Data Freshness:** The pretraining data has a cutoff of December 2023\\.

## Quantization

### Quantization Scheme

We designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:
- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.
- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.
- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.


### Quantization-Aware Training and LoRA

The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).

### SpinQuant

[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.

## Benchmarks \\- English Text

In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.

### Base Pretrained Models

| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |
| ----- | ----- | :---: | :---: | :---: | :---: | :---: |
| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |
|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |
|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |
| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |
|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |
|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |
| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |

### Instruction Tuned Models

| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |
| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |
| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |
| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |
| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |
|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |
| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |
|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |
|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |
| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |
|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |
| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |
|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |
|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |
| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |

\\*\\*for comparison purposes only. Model not released.

### Multilingual Benchmarks

| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |
| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |
| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |
| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |
| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |
| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |
| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |

\\*\\*for comparison purposes only. Model not released.

## Inference time

In the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.

| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |
| :---- | ----- | ----- | ----- | ----- | ----- |
| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |
| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |
| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |
| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |
| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |
| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |

(\\*) The performance measurement is done using an adb binary-based approach.
(\\*\\*) It is measured on an Android OnePlus 12 device.
(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64

*Footnote:*

- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*
- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*
- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*
- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*
- *RSS size \\- Memory usage in resident set size (RSS)*

## Responsibility & Safety

As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

### Responsible Deployment

**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).

#### Llama 3.2 Instruct

**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).

**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.

**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.

#### Llama 3.2 Systems

**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.

### New Capabilities and Use Cases

**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.

**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.

### Evaluations

**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.

**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.

### Critical Risks

In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:

**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.

**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.

**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.
Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.

### Community

**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).

**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).

**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.

## Ethical Considerations and Limitations

**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.

**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8"^^xsd:string ;
    schema:inLanguage "de"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "fr"^^xsd:string,
        "hi"^^xsd:string,
        "it"^^xsd:string,
        "pt"^^xsd:string,
        "th"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/11ee21b926795a2b7d2cb2d939044519ace34ac86e6b6a59a47dcb4bba5eb7a4>,
        <http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/759dcfce11b186065b46fb94fef1c569da9001c123861ba72ba0708e554bd6c4>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/d77765c1bc2a45cffe4e1ace66ea68c61a819d17e7d6159e6840523d288dbed3>,
        "arxiv:2204.05149"^^xsd:string,
        "arxiv:2405.16406"^^xsd:string,
        "base_model:finetune:meta-llama/Llama-3.2-1B"^^xsd:string,
        "base_model:meta-llama/Llama-3.2-1B"^^xsd:string,
        "de"^^xsd:string,
        "en"^^xsd:string,
        "es"^^xsd:string,
        "fr"^^xsd:string,
        "hi"^^xsd:string,
        "it"^^xsd:string,
        "license:llama3.2"^^xsd:string,
        "pt"^^xsd:string,
        "region:us"^^xsd:string,
        "th"^^xsd:string ;
    schema:license "llama3.2"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "The operating systems supported by Llama 3.2 are Windows 7, OSX 10.6, and Android 1.6."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
language:
- en
- de
- fr
- it
- pt
- hi
- es
- th
pipeline_tag: text-generation
tags:
- facebook
- meta
- pytorch
- llama
- llama-3
license: llama3.2
base_model:
- meta-llama/Llama-3.2-1B
extra_gated_prompt: "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\
  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\
  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\
  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\
  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\
  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\
  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\
  \\ of the age required under applicable laws, rules or regulations to provide legal\\
  \\ consent and that has legal authority to bind your employer or such other person\\
  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\
  \\ means the foundational large language models and software and algorithms, including\\
  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\
  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\
  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\
  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\
  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\
  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\
  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\
  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\
  \\ below or by using or distributing any portion or element of the Llama Materials,\\
  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\
  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\
  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\
  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\
  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\
  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\
  \\ Materials (or any derivative works thereof),  or a product or service (including\\
  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\
  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\
  \\ Llama” on a related website, user interface, blogpost, about page, or product\\
  \\ documentation. If you use the Llama Materials or any outputs or results of the\\
  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\
  \\ which is distributed or made available, you shall also include “Llama” at the\\
  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\
  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\
  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\
  \\ in all copies of the Llama Materials that you distribute the  following attribution\\
  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\
  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\
  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\
  \\ applicable laws and regulations (including trade compliance laws and regulations)\\
  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\
  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\
  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\
  \\ version release date, the monthly active users of the products or services made\\
  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\
  \\ monthly active users in the preceding calendar month, you must request  a license\\
  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\
  \\ authorized to exercise any of the rights under this Agreement unless or until\\
  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\
  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\
  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\
  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\
  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\
  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\
  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\
  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\
  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\
  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\
  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\
  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\
  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\
  a. No trademark licenses are granted under this Agreement, and in connection with\\
  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\
  \\ by or associated with the other or any of its affiliates,  except as required\\
  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\
  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\
  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\
  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\
  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\
  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\
  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\
  \\ respect to any derivative works and modifications of the Llama Materials that\\
  \\ are made by you, as between you and Meta, you are and will be the owner of such\\
  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\
  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\
  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\
  \\ of any of the foregoing, constitutes infringement of intellectual property or\\
  \\ other rights owned or licensable by you, then any licenses granted to you under\\
  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\
  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\
  \\ claim by any third party arising out of or related to your use or distribution\\
  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\
  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\
  \\ and will continue in full force and effect until terminated in accordance with\\
  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\
  \\ in breach of any term or condition of this Agreement. Upon termination of this\\
  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\
  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\
  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\
  \\ the State of  California without regard to choice of law principles, and the UN\\
  \\ Convention on Contracts for the International Sale of Goods does not apply to\\
  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\
  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\
  Meta is committed to promoting safe and fair use of its tools and features, including\\
  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\
  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\
  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\
  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\
  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\
  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\
  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\
  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\
  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\
  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\
  \\ illegal distribution of information or materials to minors, including obscene\\
  \\ materials, or failure to employ legally required age-gating in connection with\\
  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\
  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\
  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\
  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\
  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\
  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\
  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\
  \\ but not limited to, financial, legal, medical/health, or related professional\\
  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\
  \\ information about individuals, including information about individuals’ identity,\\
  \\ health, or demographic information, unless you have obtained the right to do so\\
  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\
  \\ generate any content that infringes, misappropriates, or otherwise violates any\\
  \\ third-party rights, including the outputs or results of any products or services\\
  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\
  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\
  \\ overburden, interfere with or impair the proper working, integrity, operation\\
  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\
  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\
  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\
  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\
  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\
  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\
  \\ applications, espionage, use for materials or activities that are subject to the\\
  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\
  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\
  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\
  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\
  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\
  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\
  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\
  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\
  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\
  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\
  \\    15. Generating, promoting, or furthering defamatory content, including the\\
  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\
  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\
  \\ without consent, authorization, or legal right\\n    18. Representing that the\\
  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\
  \\ false online engagement, including fake reviews and other means of fake online\\
  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\
  \\ of your AI system 5. Interact with third party tools, models, or software designed\\
  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\
  \\ that the outputs of such tools, models, or software are associated with Meta or\\
  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\
  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\
  \\ are not being granted to you if you are an individual domiciled in, or a company\\
  \\ with a principal place of business in, the European Union. This restriction does\\
  \\ not apply to end users of a product or service that incorporates any such multimodal\\
  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\
  \\ problems that could lead to a violation of this Policy through one of the following\\
  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\
  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\
  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\
  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\
  \\ 3.2: LlamaUseReport@meta.com"
extra_gated_fields:
  First Name: text
  Last Name: text
  Date of birth: date_picker
  Country: country
  Affiliation: text
  Job title:
    type: select
    options:
    - Student
    - Research Graduate
    - AI researcher
    - AI developer/engineer
    - Reporter
    - Other
  geo: ip_location
  ? By clicking Submit below I accept the terms of the license and acknowledge that
    the information I provide will be collected stored processed and shared in accordance
    with the Meta Privacy Policy
  : checkbox
extra_gated_description: The information you provide will be collected, stored, processed
  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).
extra_gated_button_content: Submit
---


## Model Information

The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.

**Model Developer:** Meta

**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.

|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |
| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |
|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |

**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.

**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Release Date:** Oct 24, 2024

**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.

**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).

**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).

## Intended Use

**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.

**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.

## Hardware and Software

**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.

**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.

**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.

|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |
| :---- | :---: | ----- | :---: | :---: | :---: |
| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |
| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |
| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |
| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |
| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |
| Total | 833k |         86k |  | 240 | 0 |

\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.

The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.

## Training Data

**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).

**Data Freshness:** The pretraining data has a cutoff of December 2023\\.

## Quantization

### Quantization Scheme

We designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:
- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.
- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.
- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.


### Quantization-Aware Training and LoRA

The quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).

### SpinQuant

[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.

## Benchmarks \\- English Text

In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.

### Base Pretrained Models

| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |
| ----- | ----- | :---: | :---: | :---: | :---: | :---: |
| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |
|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |
|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |
| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |
|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |
|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |
| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |

### Instruction Tuned Models

| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |
| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |
| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |
| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |
| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |
|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |
| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |
|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |
|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |
| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |
|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |
| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |
|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |
|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |
| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |

\\*\\*for comparison purposes only. Model not released.

### Multilingual Benchmarks

| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |
| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |
| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |
| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |
| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |
| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |
| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |

\\*\\*for comparison purposes only. Model not released.

## Inference time

In the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.

| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |
| :---- | ----- | ----- | ----- | ----- | ----- |
| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |
| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |
| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |
| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |
| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |
| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |

(\\*) The performance measurement is done using an adb binary-based approach.
(\\*\\*) It is measured on an Android OnePlus 12 device.
(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64

*Footnote:*

- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*
- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*
- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*
- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*
- *RSS size \\- Memory usage in resident set size (RSS)*

## Responsibility & Safety

As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

### Responsible Deployment

**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).

#### Llama 3.2 Instruct

**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).

**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.

**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.

#### Llama 3.2 Systems

**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.

### New Capabilities and Use Cases

**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.

**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.

### Evaluations

**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.

**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.

### Critical Risks

In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:

**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.

**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.

**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.
Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.

### Community

**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).

**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).

**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.

## Ethical Considerations and Limitations

**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.

**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.
"""^^xsd:string ;
    schema:softwareHelp """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:
1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama
2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm
3. Provide protections for the community to help prevent the misuse of our models

Question: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: How to Get Started with the Model

Based *only* on the context provided above, answer the"""^^xsd:string ;
    schema:softwareRequirements """Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase "Information not found".

Answer: Information not found.

Context:
Intended Use: Intended Use:
**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.
**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.
Responsibility & Safety - Par. 1: Responsibility & Safety:
As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:

Question: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: Technical Specifications > Software ; How to Get Started with the Model

Based *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the"""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8/discussions> ;
    ns1:readme <https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8/blob/main/README.md> ;
    ns1:referencePublication <http://mlentory.zbmed.de/mlentory_graph/2730e6211466c050884bcc521d8aaf9f0af8a2993b56ca3f9ab751a178b4f970>,
        <http://mlentory.zbmed.de/mlentory_graph/a787507208799647a209772884d76edfc0a14e5f209b09e235fbff7797a1a812> .

<http://mlentory.zbmed.de/mlentory_graph/ee41d767ffa75d0037ac340fa59164645b363ea8891e2a3f815c644b81ea9fbd> a schema:DefinedTerm ;
    schema:description "Python implementation of state-of-the-art variational autoencoders for generative modeling."^^xsd:string ;
    schema:name "pythae"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ef8c453c903ecc79a3f941fc76e3fb554c3c0e3459f4a7a1755e64539b934598> a schema:DefinedTerm ;
    schema:description "Library for parameter-efficient transfer learning through adapter modules in transformer models."^^xsd:string ;
    schema:name "Adapters"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f38a4d1014d3cc9f5709e2ecd78f5809ca2397dd49ed420aa2e4a0ab4210a289> a schema:DefinedTerm ;
    schema:description "End-to-end open source platform for ML with comprehensive tools and libraries for building and deploying models at scale."^^xsd:string ;
    schema:name "TensorFlow"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f398a6ed462d305d58506b378a7b488c448115a8aef746a62331a8435d54a72b> a schema:DefinedTerm ;
    schema:description "Open-source NLP research library built on PyTorch for developing state-of-the-art models."^^xsd:string ;
    schema:name "AllenNLP"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f4f5c02149ef4c63e1abfb237a7efd73b36b96243fa20ee9a3b83ab914689b18> a schema:DefinedTerm ;
    schema:description "Assigns each pixel in an image to a specific class or object."^^xsd:string ;
    schema:name "Image Segmentation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f83566aa690a65900e25f19b29d427ae3a9b752766d6d3892299a866feca3edd> a schema:DefinedTerm ;
    schema:description "Parameter-Efficient Fine-Tuning methods that enable efficient adaptation of pre-trained language models."^^xsd:string ;
    schema:name "PEFT"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/fcf6299355edca7ba663775a5f8ea79c9e18589505a31cfe64eff94e05b50982> a ns2:ML_Model ;
    ns2:ethicalLegalSocial "Information not found."^^xsd:string ;
    ns2:evaluatedOn <http://mlentory.zbmed.de/mlentory_graph/330695ad79106df6543d0dc74fe936e62af048d69f947d7923324fead03d8db2> ;
    ns2:fineTunedFrom "Information not found"^^xsd:string ;
    ns2:hasCO2eEmissions "Information not found."^^xsd:string ;
    ns2:intendedUse "The intended use of CTRL-Beatles-Lyrics-finetuned-newlyrics is not explicitly stated in the given context."^^xsd:string ;
    ns2:mlTask <http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> ;
    ns2:modelCategory "Information not found."^^xsd:string ;
    ns2:modelRisks "Information not found."^^xsd:string ;
    ns2:sharedBy <http://mlentory.zbmed.de/mlentory_graph/5e01547ae992de0d58dd96fcc38886851eff1ff264f9e77e475d524611e8e7a6> ;
    ns2:testedOn <http://mlentory.zbmed.de/mlentory_graph/330695ad79106df6543d0dc74fe936e62af048d69f947d7923324fead03d8db2> ;
    ns2:trainedOn <http://mlentory.zbmed.de/mlentory_graph/330695ad79106df6543d0dc74fe936e62af048d69f947d7923324fead03d8db2> ;
    ns2:usageInstructions "Information not found."^^xsd:string ;
    ns2:validatedOn "Information not found."^^xsd:string ;
    schema:archivedAt <https://huggingface.co/wvangils/CTRL-Beatles-Lyrics-finetuned-newlyrics> ;
    schema:author "Information not found."^^xsd:string ;
    schema:citation "Information not found."^^xsd:string ;
    schema:conditionsOfAccess "Information not found."^^xsd:string ;
    schema:contributor "Information not found."^^xsd:string ;
    schema:copyrightHolder "Information not found."^^xsd:string ;
    schema:dateCreated "2022-06-09T12:53:32+00:00"^^xsd:dateTime ;
    schema:dateModified "2023-03-08T20:01:40+00:00"^^xsd:dateTime ;
    schema:datePublished "2022-06-09T12:53:32+00:00"^^xsd:dateTime ;
    schema:description """---
tags:
- generated_from_trainer
datasets: cmotions/Beatles_lyrics
model-index:
- name: CTRL-Beatles-Lyrics-finetuned-newlyrics
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# CTRL-Beatles-Lyrics-finetuned-newlyrics

This model is a fine-tuned version of [sshleifer/tiny-ctrl](https://huggingface.co/sshleifer/tiny-ctrl) on the [Cmotions - Beatles lyrics](https://huggingface.co/datasets/cmotions/Beatles_lyrics) dataset. It will complete an input prompt with Beatles-like text.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 100
- num_epochs: 5

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 12.361        | 1.0   | 35   | 12.3685         |
| 12.3529       | 2.0   | 70   | 12.3583         |
| 12.3374       | 3.0   | 105  | 12.3401         |
| 12.3158       | 4.0   | 140  | 12.3237         |
| 12.301        | 5.0   | 175  | 12.3180         |


### Framework versions

- Transformers 4.19.2
- Pytorch 1.11.0+cu113
- Datasets 2.2.2
- Tokenizers 0.12.1
"""^^xsd:string ;
    schema:discussionUrl <https://huggingface.co/wvangils/CTRL-Beatles-Lyrics-finetuned-newlyrics/discussions> ;
    schema:distribution "Information not found."^^xsd:string ;
    schema:funding "Information not found."^^xsd:string ;
    schema:identifier "https://huggingface.co/wvangils/CTRL-Beatles-Lyrics-finetuned-newlyrics"^^xsd:string ;
    schema:inLanguage "Information not found"^^xsd:string ;
    schema:keywords <http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d>,
        <http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d>,
        <http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8>,
        <http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad>,
        <http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1>,
        <http://mlentory.zbmed.de/mlentory_graph/974df5e3234c3601080dac4fa96eebf5d19d40df41e9e30cb61084d9fbefea1e>,
        <http://mlentory.zbmed.de/mlentory_graph/d8b4614866107c373b528b019390254efeae19141a9f693726edfc615b3d36aa>,
        <http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de>,
        "dataset:cmotions/Beatles_lyrics"^^xsd:string,
        "region:us"^^xsd:string ;
    schema:license "Information not found"^^xsd:string ;
    schema:maintainer "Information not found."^^xsd:string ;
    schema:memoryRequirements "Not available"^^xsd:string ;
    schema:operatingSystem "Information not found."^^xsd:string ;
    schema:processorRequirements "Information not found."^^xsd:string ;
    schema:releaseNotes """---
tags:
- generated_from_trainer
datasets: cmotions/Beatles_lyrics
model-index:
- name: CTRL-Beatles-Lyrics-finetuned-newlyrics
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# CTRL-Beatles-Lyrics-finetuned-newlyrics

This model is a fine-tuned version of [sshleifer/tiny-ctrl](https://huggingface.co/sshleifer/tiny-ctrl) on the [Cmotions - Beatles lyrics](https://huggingface.co/datasets/cmotions/Beatles_lyrics) dataset. It will complete an input prompt with Beatles-like text.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 100
- num_epochs: 5

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 12.361        | 1.0   | 35   | 12.3685         |
| 12.3529       | 2.0   | 70   | 12.3583         |
| 12.3374       | 3.0   | 105  | 12.3401         |
| 12.3158       | 4.0   | 140  | 12.3237         |
| 12.301        | 5.0   | 175  | 12.3180         |


### Framework versions

- Transformers 4.19.2
- Pytorch 1.11.0+cu113
- Datasets 2.2.2
- Tokenizers 0.12.1
"""^^xsd:string ;
    schema:softwareHelp "Information not found."^^xsd:string ;
    schema:softwareRequirements """The software requirements for the CTRL-Beatles-Lyrics-finetuned-newlyrics model are as follows:

- Transformers 4.19.2
- Pytorch 1.11.0+cu113
- Datasets 2.2.2
- Tokenizers 0.12.1

These software dependencies are mentioned in the context provided, specifically in the section "Training hyperparameters" and "Framework versions"."""^^xsd:string ;
    schema:storageRequirements "Information not found."^^xsd:string ;
    schema:url <https://huggingface.co/wvangils/CTRL-Beatles-Lyrics-finetuned-newlyrics> ;
    schema:version "Information not found."^^xsd:string ;
    ns1:buildInstructions "Information not found."^^xsd:string ;
    ns1:developmentStatus "Information not found."^^xsd:string ;
    ns1:issueTracker <https://huggingface.co/wvangils/CTRL-Beatles-Lyrics-finetuned-newlyrics/discussions> ;
    ns1:readme <https://huggingface.co/wvangils/CTRL-Beatles-Lyrics-finetuned-newlyrics/blob/main/README.md> ;
    ns1:referencePublication "Information not found"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/fd4e2f28de7e36539eda0f2e74a9e9b30f10d77fee3b927dc3aa5419c43369da> a schema:DefinedTerm ;
    schema:description "Converts 2D images into 3D representations or models."^^xsd:string ;
    schema:name "Image to 3D"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/mnli> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mnli/hypothesis>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli/label>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli/premise>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli/split> ;
    schema:description """nyu-mll/glue - 'mnli' subset

Additional information:
- 5 splits: train, validation_matched, validation_mismatched, test_matched, test_mismatched"""@en ;
    schema:name "mnli"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mnli_matched/hypothesis>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_matched/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_matched/label>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_matched/premise>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_matched/split> ;
    schema:description """nyu-mll/glue - 'mnli_matched' subset

Additional information:
- 2 splits: validation, test"""@en ;
    schema:name "mnli_matched"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched_splits> a ns3:RecordSet ;
    ns3:data "[{\"mnli_matched_splits/split_name\":\"validation\"},{\"mnli_matched_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mnli_matched_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/mnli_matched_splits/split_name> ;
    schema:description "Splits for the mnli_matched config."@en ;
    schema:name "mnli_matched_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/hypothesis>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/label>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/premise>,
        <http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/split> ;
    schema:description """nyu-mll/glue - 'mnli_mismatched' subset

Additional information:
- 2 splits: validation, test"""@en ;
    schema:name "mnli_mismatched"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched_splits> a ns3:RecordSet ;
    ns3:data "[{\"mnli_mismatched_splits/split_name\":\"validation\"},{\"mnli_mismatched_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched_splits/split_name> ;
    schema:description "Splits for the mnli_mismatched config."@en ;
    schema:name "mnli_mismatched_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_splits> a ns3:RecordSet ;
    ns3:data "[{\"mnli_splits/split_name\":\"train\"},{\"mnli_splits/split_name\":\"validation_matched\"},{\"mnli_splits/split_name\":\"validation_mismatched\"},{\"mnli_splits/split_name\":\"test_matched\"},{\"mnli_splits/split_name\":\"test_mismatched\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mnli_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/mnli_splits/split_name> ;
    schema:description "Splits for the mnli config."@en ;
    schema:name "mnli_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mrpc/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/mrpc/label>,
        <http://mlentory.zbmed.de/mlentory_graph/mrpc/sentence1>,
        <http://mlentory.zbmed.de/mlentory_graph/mrpc/sentence2>,
        <http://mlentory.zbmed.de/mlentory_graph/mrpc/split> ;
    schema:description """nyu-mll/glue - 'mrpc' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "mrpc"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc_splits> a ns3:RecordSet ;
    ns3:data "[{\"mrpc_splits/split_name\":\"train\"},{\"mrpc_splits/split_name\":\"validation\"},{\"mrpc_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/mrpc_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/mrpc_splits/split_name> ;
    schema:description "Splits for the mrpc config."@en ;
    schema:name "mrpc_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/plain_text/answers>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/context>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/id>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/label>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/question>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/split>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/text>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/title> ;
    schema:description """rajpurkar/squad - 'plain_text' subset

Additional information:
- 2 splits: train, validation"""@en,
        """stanfordnlp/imdb - 'plain_text' subset

Additional information:
- 3 splits: train, test, unsupervised"""@en ;
    schema:name "plain_text"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text_splits> a ns3:RecordSet ;
    ns3:data "[{\"plain_text_splits/split_name\":\"train\"},{\"plain_text_splits/split_name\":\"test\"},{\"plain_text_splits/split_name\":\"unsupervised\"}]"^^rdf:JSON,
        "[{\"plain_text_splits/split_name\":\"train\"},{\"plain_text_splits/split_name\":\"validation\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/plain_text_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/plain_text_splits/split_name> ;
    schema:description "Splits for the plain_text config."@en ;
    schema:name "plain_text_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/qnli/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/qnli/label>,
        <http://mlentory.zbmed.de/mlentory_graph/qnli/question>,
        <http://mlentory.zbmed.de/mlentory_graph/qnli/sentence>,
        <http://mlentory.zbmed.de/mlentory_graph/qnli/split> ;
    schema:description """nyu-mll/glue - 'qnli' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "qnli"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli_splits> a ns3:RecordSet ;
    ns3:data "[{\"qnli_splits/split_name\":\"train\"},{\"qnli_splits/split_name\":\"validation\"},{\"qnli_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/qnli_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/qnli_splits/split_name> ;
    schema:description "Splits for the qnli config."@en ;
    schema:name "qnli_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/qqp/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/qqp/label>,
        <http://mlentory.zbmed.de/mlentory_graph/qqp/question1>,
        <http://mlentory.zbmed.de/mlentory_graph/qqp/question2>,
        <http://mlentory.zbmed.de/mlentory_graph/qqp/split> ;
    schema:description """nyu-mll/glue - 'qqp' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "qqp"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp_splits> a ns3:RecordSet ;
    ns3:data "[{\"qqp_splits/split_name\":\"train\"},{\"qqp_splits/split_name\":\"validation\"},{\"qqp_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/qqp_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/qqp_splits/split_name> ;
    schema:description "Splits for the qqp config."@en ;
    schema:name "qqp_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/rte/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/rte/label>,
        <http://mlentory.zbmed.de/mlentory_graph/rte/sentence1>,
        <http://mlentory.zbmed.de/mlentory_graph/rte/sentence2>,
        <http://mlentory.zbmed.de/mlentory_graph/rte/split> ;
    schema:description """nyu-mll/glue - 'rte' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "rte"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte_splits> a ns3:RecordSet ;
    ns3:data "[{\"rte_splits/split_name\":\"train\"},{\"rte_splits/split_name\":\"validation\"},{\"rte_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/rte_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/rte_splits/split_name> ;
    schema:description "Splits for the rte config."@en ;
    schema:name "rte_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/sst2> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/sst2/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/sst2/label>,
        <http://mlentory.zbmed.de/mlentory_graph/sst2/sentence>,
        <http://mlentory.zbmed.de/mlentory_graph/sst2/split> ;
    schema:description """nyu-mll/glue - 'sst2' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "sst2"@en .

<http://mlentory.zbmed.de/mlentory_graph/sst2_splits> a ns3:RecordSet ;
    ns3:data "[{\"sst2_splits/split_name\":\"train\"},{\"sst2_splits/split_name\":\"validation\"},{\"sst2_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/sst2_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/sst2_splits/split_name> ;
    schema:description "Splits for the sst2 config."@en ;
    schema:name "sst2_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/stsb/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/stsb/label>,
        <http://mlentory.zbmed.de/mlentory_graph/stsb/sentence1>,
        <http://mlentory.zbmed.de/mlentory_graph/stsb/sentence2>,
        <http://mlentory.zbmed.de/mlentory_graph/stsb/split> ;
    schema:description """nyu-mll/glue - 'stsb' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "stsb"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb_splits> a ns3:RecordSet ;
    ns3:data "[{\"stsb_splits/split_name\":\"train\"},{\"stsb_splits/split_name\":\"validation\"},{\"stsb_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/stsb_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/stsb_splits/split_name> ;
    schema:description "Splits for the stsb config."@en ;
    schema:name "stsb_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli> a ns3:RecordSet ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/wnli/idx>,
        <http://mlentory.zbmed.de/mlentory_graph/wnli/label>,
        <http://mlentory.zbmed.de/mlentory_graph/wnli/sentence1>,
        <http://mlentory.zbmed.de/mlentory_graph/wnli/sentence2>,
        <http://mlentory.zbmed.de/mlentory_graph/wnli/split> ;
    schema:description """nyu-mll/glue - 'wnli' subset

Additional information:
- 3 splits: train, validation, test"""@en ;
    schema:name "wnli"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli_splits> a ns3:RecordSet ;
    ns3:data "[{\"wnli_splits/split_name\":\"train\"},{\"wnli_splits/split_name\":\"validation\"},{\"wnli_splits/split_name\":\"test\"}]"^^rdf:JSON ;
    ns3:dataType ns3:Split ;
    ns3:field <http://mlentory.zbmed.de/mlentory_graph/wnli_splits/split_name> ;
    ns3:key <http://mlentory.zbmed.de/mlentory_graph/wnli_splits/split_name> ;
    schema:description "Splits for the wnli config."@en ;
    schema:name "wnli_splits"@en .

<http://mlentory.zbmed.de/mlentory_graph/020eb0d4713d856b7707c9066e6bedcd042487c7ce89143c25e68a5cf634de2f> a schema:Person ;
    schema:name "simonestradasch"^^xsd:string ;
    schema:url "https://huggingface.co/simonestradasch"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/02c989e969206aaa515bcdd718c32182028cc8b686a12998d33856c52d68d706> a schema:DefinedTerm ;
    schema:name "llm"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/0437d97d6f71569092a72cc3c2b5ce5cc4751919c8ab349cb684ab8c6a9cbb5d> a schema:DefinedTerm ;
    schema:name "TinyLlama"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/04b73aa26b48d6f1e64d5819ec986a901d3b49c7f37b35f4798db992ac65c200> a schema:DefinedTerm ;
    schema:name "custom-implementation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/0c3d01ed44dc2940ae38707146a02c16bc80cc8637ce48ac5f3125b7c6fb2982> a schema:DefinedTerm ;
    schema:description "Models based on BERT (Bidirectional Encoder Representations from Transformers), a transformer architecture that learns contextual word embeddings."^^xsd:string ;
    schema:name "bert"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/10fc066544c1b06479e3cf01ac6753d82f315ef61f82986bad2e31641136cd70> a schema:DefinedTerm ;
    schema:name "fluxgym"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/12f271f5159c42562f3014bee76f49cdb0b6af26a1e072c41d27917bbe668147> a schema:DefinedTerm ;
    schema:name "stable-diffusion-xl"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/141e91e9e300f34f3844e30be636e1f5f85cf982186ff356d60256998ce834b9> a schema:Person ;
    schema:name "arellewen"^^xsd:string ;
    schema:url "https://huggingface.co/arellewen"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/16a23f6c98f42b86e5d4d52f3aac9a772d36e2f700ba7863b81456774dd81577> a schema:DefinedTerm ;
    schema:name "CartPole-v1"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/1aa60aa9719223ac8750af1366d4a56166214022d0d069d1412225070d1275b9> a schema:DefinedTerm ;
    schema:name "multilingual"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/1c89b15196e446b3deb40077389a5a66fc333e63e98fc0ec7d0581c927658fc1> a schema:Person ;
    schema:name "mqy"^^xsd:string ;
    schema:url "https://huggingface.co/mqy"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/1e997bed4477ee580a83cb75321b0ee5013e0e193f27b20ac16d78679b4a78a9> a schema:Person ;
    schema:name "margati"^^xsd:string ;
    schema:url "https://huggingface.co/margati"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/20a607de6d3028bc24821288287faa1f8bf92a37433fb7df54105674128d8a68> a schema:Person ;
    schema:name "unsloth"^^xsd:string ;
    schema:url "https://huggingface.co/unsloth"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/21e8f9819743fa391136eeebe9af8ccf402fb92306dd81a3e0b578785278c62a> a ns2:MLModel ;
    schema:name "YYYYYYibo/full_vanilla_doff_dpo_iter_2"^^xsd:string ;
    schema:url <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_2> .

<http://mlentory.zbmed.de/mlentory_graph/290a5935ba014c4301a9f430a5f8577ab587e52024ce0e77b49d8296fe2aa3e4> a schema:Person ;
    schema:name "fernandals"^^xsd:string ;
    schema:url "https://huggingface.co/fernandals"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/2f6977083cec992fd87ac4e63effc5723d1af4356346054007b916659b276565> a schema:DefinedTerm ;
    schema:name "bar"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/309322bdc7e7441b9f4608224edba0ae93e18d4f079f4721e80b9c830a543e9a> a schema:DefinedTerm ;
    schema:name "ceb"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/345644230f7e711605ef233127803f06659ae1bd200eadaf0bb48fd111eb8366> a schema:DefinedTerm ;
    schema:name "SpaceInvadersNoFrameskip-v4"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/3c14fb791ef79e48f95d78b8f9f8e233e1773421508cb537054e54ee68fac0fe> a ns2:MLModel ;
    schema:name "TinyLlama/TinyLlama-1.1B-Chat-v1.0"^^xsd:string ;
    schema:url <https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0> .

<http://mlentory.zbmed.de/mlentory_graph/3cf1ab8574d81c00233c30bd64e2562c80c2c4b9ba8c824b6f4e12904b13eac0> a schema:DefinedTerm ;
    schema:name "lm-head"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/40b331abbeb7a5faa9ee0a36b7b9b9c0748500ab183a2ef8af0f673230b1f45a> a ns2:MLModel ;
    schema:name "SG161222/RealVisXL_V2.0"^^xsd:string ;
    schema:url <https://huggingface.co/SG161222/RealVisXL_V2.0> .

<http://mlentory.zbmed.de/mlentory_graph/410526dc3d4eb3818f7e3b0c216684c878292ea29533262bc0ca37ef3921278a> a schema:Person ;
    schema:name "Fanbin"^^xsd:string ;
    schema:url "https://huggingface.co/Fanbin"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/44669cbedcb2f4b90028ef263e4c66e7ce1f20328981f664f9b6e2ffbfa286f7> a schema:Person ;
    schema:name "wellmadenametag"^^xsd:string ;
    schema:url "https://huggingface.co/wellmadenametag"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/461c5e3d5b0d3aa22f2cc8a7718ca67259b586f0e9f9c26fdf31874cab0e3a4f> a schema:DefinedTerm ;
    schema:name "token-classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/4895f1e40a05c534b7f3237f31f4536bbcbb6cad57b678d08591d76270974380> a ns2:MLModel ;
    schema:name "distilbert/distilbert-base-multilingual-cased"^^xsd:string ;
    schema:url <https://huggingface.co/distilbert/distilbert-base-multilingual-cased> .

<http://mlentory.zbmed.de/mlentory_graph/49bf7b39c8a2a4cb84ab43bc3c320de6e0c1da1253ebf022a5d0a84737581119> a schema:DefinedTerm ;
    schema:description "Converts image content into textual descriptions or captions."^^xsd:string ;
    schema:name "Image Text to Text"^^xsd:string,
        "image text to text"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/4b6121cea7734a3579dc04999a3b0a03ea19f5392b2dc776f08f69fe60adadd6> a schema:DefinedTerm ;
    schema:name "sco"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/4e2b489139fd9424e3deea62e2359c279e804cb631f5e3fd4c1685d0b1d55445> a schema:DefinedTerm ;
    schema:name "8-bit"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/527f8fccacd13d63f02189d2818818a852504fa8cd1b02edcdc062cb6ee12c5d> a ns2:MLModel ;
    schema:name "alignment-handbook/zephyr-7b-sft-full"^^xsd:string ;
    schema:url <https://huggingface.co/alignment-handbook/zephyr-7b-sft-full> .

<http://mlentory.zbmed.de/mlentory_graph/54ded1dc61acf1026f0859f850cee38bfbf5da27e35bcd3a46458a7b2e81985f> a schema:Person ;
    schema:name "mlewinski"^^xsd:string ;
    schema:url "https://huggingface.co/mlewinski"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/55bd15a0af8bef99be0b0832411b5afbbb83779bc0cd1795dffee7c3285baa1f> a schema:DefinedTerm ;
    schema:name "feature-extraction"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/55c2ef29f4fc34014e7c41f2439cd686ebcc0bd4d1a02299b7491148b03bd06b> a schema:DefinedTerm ;
    schema:description "Models with custom implementation code beyond standard library functionality."^^xsd:string ;
    schema:name "custom_code"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5753fbc4186ff7da2a349a9e6c953346d81338871b3af41695d0e0707dee467e> a schema:DefinedTerm ;
    schema:name "fry"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5e01547ae992de0d58dd96fcc38886851eff1ff264f9e77e475d524611e8e7a6> a schema:Person ;
    schema:name "wvangils"^^xsd:string ;
    schema:url "https://huggingface.co/wvangils"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/5e2174aea879a4806552b465a7caa98d3f94a7296126ee9f011b524911ed47cf> a schema:DefinedTerm ;
    schema:name "inc"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5e74eb4ff0b0e3cb5d0cf817eab16d7070fe8abf2bdfe5b5b62577201e86b1ea> a schema:DefinedTerm ;
    schema:name "exbert"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5f05cc43f75ab65c5a42a72971f2505a9613d2882a1d30b66b318a693985a9da> a schema:DefinedTerm ;
    schema:name "ast"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5f8c97c4aaca5a8c6afac1954a68be2dcb40698e64871f37e5fe5dc5849a3aab> a schema:DefinedTerm ;
    schema:description "Measures how similar two sentences are semantically."^^xsd:string ;
    schema:name "Sentence Similarity"^^xsd:string,
        "sentence similarity"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6b88490a63533b6426e4066081546474864e838077b33450890ba59f06225201> a schema:DefinedTerm ;
    schema:name "tflite"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6fa5b295a0420d9a3cc819e672ebdd61720c48761f8c2960d28ff7f7655875be> a schema:DefinedTerm ;
    schema:description "Provides answers to questions based on given context."^^xsd:string ;
    schema:name "Question Answering"^^xsd:string,
        "question answering"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/746e796ec31ad9ad2b517555a4e91b2e3dfff187021fd0255aae50476624430d> a schema:DefinedTerm ;
    schema:description "Extracts meaningful features from input data for downstream tasks."^^xsd:string ;
    schema:name "Feature Extraction"^^xsd:string,
        "feature extraction"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/7a3700557a9a837d2acbe1b2901694ff05af496b82967766e041c3ca88374601> a schema:DefinedTerm ;
    schema:name "News"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/832a1e8e47c996aa09f9b6062fcc27e185ca1c68b47cc9b02f8457a5971d04cd> a schema:DefinedTerm ;
    schema:name "qwen2_vl"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/83d1ebc775fee7fb0a812e8dca395fdcac7c0fe605940807d90a6d698dd912b8> a schema:DefinedTerm ;
    schema:name "QLoRA"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/8a7c0f53b968dbfecd34bbb37846ae3e47ed7434b4f28d23160e0df0bb79d5b3> a schema:DefinedTerm ;
    schema:name "aze"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/8c4a8f327d1fd4a4e809a099e5f7d5a743e4a5c0883b7dbd24a235b2d281ba8e> a schema:DefinedTerm ;
    schema:name "LunarLander-v2"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/8dda7455295f0a266a228afac693c1d9f5e021304637fea5132533854c579893> a schema:DefinedTerm ;
    schema:name "generated_from_keras_callback"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9029477f3163928162d1e889607fce71fdf85631fdee3951bb37114ce0203c57> a schema:DefinedTerm ;
    schema:name "causal-lm"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/93108d142ec8aa4540d03f40aa1cbc3eb5d79368400a280197fb230643230e00> a schema:DefinedTerm ;
    schema:name "mergekit"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/97a1e7815c8e610e583f7f2afd2724e05d2efa1398eeb798f80981c0d44b28dc> a schema:DefinedTerm ;
    schema:name "question-answering"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/97a327fc7cf14c750f068bdf3aab8cf61c95e2b04079bd1772d37c385f2b1088> a schema:DefinedTerm ;
    schema:name "llama-factory"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/99b4f0e72aa605a47bc337b0c848e25811755f9eb07361970b92f3f7e04747e6> a schema:DefinedTerm ;
    schema:name "image-text-to-text"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9abc397b7489422e962aeae20a5ed774806c8bd8f47e9008eef451699b8a61c1> a schema:DefinedTerm ;
    schema:description "Transforms input text into a different textual form."^^xsd:string ;
    schema:name "Text2Text Generation"^^xsd:string,
        "text2text generation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9b3d9b28e7c2070531e1d563836c54690a539654c89cf38e6a27635202a9f512> a schema:DefinedTerm ;
    schema:name "gpt2"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/a1943d610be97366c0b632005eeea8e493269f372ceb5ffb4ab1aefe2fde3d1f> a schema:Person ;
    schema:name "KyriaAnnwyn"^^xsd:string ;
    schema:url "https://huggingface.co/KyriaAnnwyn"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/a4045d1920b7eb5463ba285d7a39bd0751bf81195ee8a40f0e2d0607458bc0fe> a schema:DefinedTerm ;
    schema:name "lazymergekit"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/a76fe889292455ddd24101fcc9c7fb0bef245d176bb34cf8fc885e4fccaf4f35> a schema:Person ;
    schema:name "RehanAzam"^^xsd:string ;
    schema:url "https://huggingface.co/RehanAzam"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/a78f959c215edcbfd7197d95d2f6cbb591f9e010a628c097ae2631a706e27277> a schema:Person ;
    schema:name "h4rz3rk4s3"^^xsd:string ;
    schema:url "https://huggingface.co/h4rz3rk4s3"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/a8eae50bf40de8621698ed7e4943c7c010b710e5ab9ac9aae114dfab8527f239> a schema:DefinedTerm ;
    schema:name "roa"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/acdea979b59f74f49c8d4909658c7c387d308c1418907e643855f6839013271f> a schema:DefinedTerm ;
    schema:name "computer-vision"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ae6f8948011de4b50edd72fa4ba450988b135ac2718cd958ee93b06bd34289aa> a schema:DefinedTerm ;
    schema:name "pnb"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ax/hypothesis> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'hypothesis' from the Hugging Face parquet file."@en ;
    schema:name "ax/hypothesis"@en .

<http://mlentory.zbmed.de/mlentory_graph/ax/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "ax/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/ax/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
entailment (0), neutral (1), contradiction (2)"""@en ;
    schema:name "ax/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/ax/premise> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'premise' from the Hugging Face parquet file."@en ;
    schema:name "ax/premise"@en .

<http://mlentory.zbmed.de/mlentory_graph/ax/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "ax/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/b039d4f4ed1833320cf3d5167c9e63c2e5864d50c719d12ed5a1b61de7086184> a ns2:MLModel ;
    schema:name "distilbert/distilbert-base-uncased"^^xsd:string ;
    schema:url <https://huggingface.co/distilbert/distilbert-base-uncased> .

<http://mlentory.zbmed.de/mlentory_graph/b0c80490465c422162366747d4bb193299977b51cacddfa724ffc1e2de63c94b> a schema:DefinedTerm ;
    schema:name "uukuguy/speechless-mistral-six-in-one-7b-orth-1.0"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b191c4fd48c1c510469ee9111aee8a38ee126676e8f72937db5b30de066f635a> a schema:DefinedTerm ;
    schema:name "pms"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b27ff40fa7a2058157ffd2256f890a419f0528607ed9c66983c96e9f5aa17aa9> a schema:DefinedTerm ;
    schema:name "new"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b3764ab684e398f1cc0ce31596f300e8c8475b38bbf865201bca5e9aa964d8d8> a schema:DefinedTerm ;
    schema:name "nds"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b3a667bbf00faf3c261e35a53645ea532c3043c18a276bfc23c24105f625022d> a schema:Person ;
    schema:name "Shridipta-06"^^xsd:string ;
    schema:url "https://huggingface.co/Shridipta-06"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/b6726f6435b32ab359d8c48bd5f24d3180c4c1772a769d50d3b4c8cf8f0ecd46> a schema:DefinedTerm ;
    schema:description "Models created by merging multiple other models, often to combine their strengths or capabilities."^^xsd:string ;
    schema:name "Merge"^^xsd:string,
        "merge"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/b751528b59fc7a0677c52c99b85d41f69866dad58deffdbf1e33a510f27bc2f5> a schema:DefinedTerm ;
    schema:name "full"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/bb5a8d5565bb8b4e7a168aaa8cac657040496dc48337408cd7960e62d51eb863> a schema:DefinedTerm ;
    schema:name "ILKT"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/bf94889b67ecdd083fffcc97e7e51d92c6ffda086a4eca0658bf573797f1a573> a ns2:MLModel ;
    schema:name "unsloth/llama-3-8b-Instruct-bnb-4bit"^^xsd:string ;
    schema:url <https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit> .

<http://mlentory.zbmed.de/mlentory_graph/c16d609c230b6e73c505c48462613ec3adc58ea075e6951dd0863657a07b00d3> a schema:DefinedTerm ;
    schema:name "mistralai/Mistral-7B-Instruct-v0.2"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/c2cc99051d7ed76ff3bce040b0417e9769c6200e327ebd61cfddefbb7b100262> a schema:DefinedTerm ;
    schema:name "lyrics"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/c4d07fa0daab8d52ad9866cddd0f33417e35c2c0ef5a1c93787cc16c57ed38ec> a ns2:MLModel ;
    schema:name "YYYYYYibo/full_vanilla_doff_dpo_iter_1"^^xsd:string ;
    schema:url <https://huggingface.co/YYYYYYibo/full_vanilla_doff_dpo_iter_1> .

<http://mlentory.zbmed.de/mlentory_graph/ca449e33dbfb2eb8a94ef8f31fe870b192443910298d3cb6ca462aab9710b804> a schema:DefinedTerm ;
    schema:name "mteb"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/cb9bd15fb27d24d93383343fa6ee37e644b0a41da399de954b0a5cf93c7b3501> a schema:DefinedTerm ;
    schema:description "Labels individual tokens (words) in text with specific categories."^^xsd:string ;
    schema:name "Token Classification"^^xsd:string,
        "token classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ce33825b7c1e5d75f33a4642e9c0161cb4b86ef24e68f4f773320780da0d589d> a schema:ScholarlyArticle ;
    schema:abstract """Developing AI agents to autonomously manipulate graphical user interfaces is
a long challenging task. Recent advances in data scaling law inspire us to
train computer-use agents with a scaled instruction set, yet using behavior
cloning to train agents still requires immense high-quality trajectories. To
meet the scalability need, we designed STEVE, a step verification pipeline for
computer-use agent training. First, we establish a large instruction set for
computer-use agents and collect trajectory data with some suboptimal agents.
GPT-4o is used to verify the correctness of each step in the trajectories based
on the screens before and after the action execution, assigning each step with
a binary label. Last, we adopt the Kahneman and Tversky Optimization to
optimize the agent from the binary stepwise labels. Extensive experiments
manifest that our agent outperforms supervised finetuning by leveraging both
positive and negative actions within a trajectory. Also, STEVE enables us to
train a 7B vision-language model as a computer-use agent, achieving leading
performance in the challenging live desktop environment WinAgentArena with
great efficiency at a reduced cost. Code and data:
https://github.com/FanbinLu/STEVE."""^^xsd:string ;
    schema:author "{'name': 'Chi-Wing Fu', 'affiliation': None}"^^xsd:string,
        "{'name': 'Fanbin Lu', 'affiliation': None}"^^xsd:string,
        "{'name': 'Jiaya Jia', 'affiliation': None}"^^xsd:string,
        "{'name': 'Shu Liu', 'affiliation': None}"^^xsd:string,
        "{'name': 'Zhisheng Zhong', 'affiliation': None}"^^xsd:string,
        "{'name': 'Ziqin Wei', 'affiliation': None}"^^xsd:string ;
    schema:datePublished "2025-03-16"^^xsd:date ;
    schema:keywords "cs.AI"^^xsd:string,
        "cs.CV"^^xsd:string ;
    schema:name "STEVE: A Step Verification Pipeline for Computer-use Agent Training"^^xsd:string ;
    schema:url "https://arxiv.org/abs/2503.12532"^^xsd:anyURI,
        "https://arxiv.org/abs/2503.12532"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/cola/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "cola/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/cola/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
unacceptable (0), acceptable (1)"""@en ;
    schema:name "cola/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/cola/sentence> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence' from the Hugging Face parquet file."@en ;
    schema:name "cola/sentence"@en .

<http://mlentory.zbmed.de/mlentory_graph/cola/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "cola/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/d18f907263e8b85d9fa3da91cfbbf936203cf6077cef0d677afd97d2ebd3f68b> a schema:DefinedTerm ;
    schema:name "coreml"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d35473e099c177351c833e6dad49614522e57381763a759b112129fe6233ef94> a schema:DefinedTerm ;
    schema:name "min"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d4480d490771662bb85676cf29422964810080c65ef1bd8385fa451f7801acdb> a schema:Person ;
    schema:name "sidvash"^^xsd:string ;
    schema:url "https://huggingface.co/sidvash"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/d78e76dd2fd5d1649023f6aa571601076a8d9ed012895d9c79adaea965083691> a schema:DefinedTerm ;
    schema:name "mt5"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d8b4614866107c373b528b019390254efeae19141a9f693726edfc615b3d36aa> a schema:DefinedTerm ;
    schema:name "ctrl"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d8f2d587f7a2538c0b27b071b8686eeb4ca1c2d23df31d6e6808ce3d984a15b0> a schema:Person ;
    schema:name "ILKT"^^xsd:string ;
    schema:url "https://huggingface.co/ILKT"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/d97786edc8d2091920dae92570c4a793d367ed8ec0dff0c869b4d21fcd2e02e2> a schema:DefinedTerm ;
    schema:name "scn"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/dbe08386414358e8b5b6ded6d9d2c773a7bdb23b4885d932efffcbf2c43b03e5> a schema:DefinedTerm ;
    schema:name "reinforce"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/de8e27b7ddf241399ac5fa9721bb05a2ccbcd267141d9c8ccd10f0131ae1559b> a schema:DefinedTerm ;
    schema:name "war"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/default/album> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'album' from the Hugging Face parquet file."@en ;
    schema:name "default/album"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/artist> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'artist' from the Hugging Face parquet file."@en ;
    schema:name "default/artist"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/chosen> a ns3:Field ;
    ns3:repeated true ;
    ns3:subField <http://mlentory.zbmed.de/mlentory_graph/default/chosen/content>,
        <http://mlentory.zbmed.de/mlentory_graph/default/chosen/role> ;
    schema:description "Column 'chosen' from the Hugging Face parquet file."@en ;
    schema:name "default/chosen"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/chosen/content> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'chosen' from the Hugging Face parquet file."@en ;
    schema:name "default/chosen/content"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/chosen/role> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'chosen' from the Hugging Face parquet file."@en ;
    schema:name "default/chosen/role"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "default/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
negative (0), positive (1)"""@en ;
    schema:name "default/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/lyrics> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'lyrics' from the Hugging Face parquet file."@en ;
    schema:name "default/lyrics"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/lyrics_length> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'lyrics_length' from the Hugging Face parquet file."@en ;
    schema:name "default/lyrics_length"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/messages> a ns3:Field ;
    ns3:repeated true ;
    ns3:subField <http://mlentory.zbmed.de/mlentory_graph/default/messages/content>,
        <http://mlentory.zbmed.de/mlentory_graph/default/messages/role> ;
    schema:description "Column 'messages' from the Hugging Face parquet file."@en ;
    schema:name "default/messages"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/messages/content> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'messages' from the Hugging Face parquet file."@en ;
    schema:name "default/messages/content"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/messages/role> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'messages' from the Hugging Face parquet file."@en ;
    schema:name "default/messages/role"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/number> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'number' from the Hugging Face parquet file."@en ;
    schema:name "default/number"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/prompt> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'prompt' from the Hugging Face parquet file."@en ;
    schema:name "default/prompt"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/prompt_id> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'prompt_id' from the Hugging Face parquet file."@en ;
    schema:name "default/prompt_id"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/rejected> a ns3:Field ;
    ns3:repeated true ;
    ns3:subField <http://mlentory.zbmed.de/mlentory_graph/default/rejected/content>,
        <http://mlentory.zbmed.de/mlentory_graph/default/rejected/role> ;
    schema:description "Column 'rejected' from the Hugging Face parquet file."@en ;
    schema:name "default/rejected"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/rejected/content> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'rejected' from the Hugging Face parquet file."@en ;
    schema:name "default/rejected/content"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/rejected/role> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'rejected' from the Hugging Face parquet file."@en ;
    schema:name "default/rejected/role"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/score_chosen> a ns3:Field ;
    ns3:dataType schema:Float ;
    schema:description "Column 'score_chosen' from the Hugging Face parquet file."@en ;
    schema:name "default/score_chosen"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/score_rejected> a ns3:Field ;
    ns3:dataType schema:Float ;
    schema:description "Column 'score_rejected' from the Hugging Face parquet file."@en ;
    schema:name "default/score_rejected"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/sentence> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence' from the Hugging Face parquet file."@en ;
    schema:name "default/sentence"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "default/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/text> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'text' from the Hugging Face parquet file."@en ;
    schema:name "default/text"@en .

<http://mlentory.zbmed.de/mlentory_graph/default/title> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'title' from the Hugging Face parquet file."@en ;
    schema:name "default/title"@en .

<http://mlentory.zbmed.de/mlentory_graph/dff307b5bc08947d577a5c7be1c716f51aac0eca11095e8a4735db37d32a3404> a schema:Person ;
    schema:name "SidhaarthMurali"^^xsd:string ;
    schema:url "https://huggingface.co/SidhaarthMurali"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/e05dcf019c16b58ee18632feff8d896599b435fed50864784dfbbf57cfeb9f88> a schema:DefinedTerm ;
    schema:name "sentence-similarity"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/e2cb5f7dba65387f6ecd56e7a642d33df8fca1197b2ac918cd629a6cd9b294b2> a schema:DefinedTerm ;
    schema:description "Framework for computing dense vector representations of sentences and paragraphs using transformer models."^^xsd:string ;
    schema:name "sentence-transformers"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/e694cb482434e33f5aadafc7ba5607930722b2bd1b0cb252690679261c8692f4> a schema:DefinedTerm ;
    schema:name "text2text-generation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/e797b8ca7ecd2d359733673fd310f9cdfce603607602506828f69a8c2bc5dda5> a schema:DefinedTerm ;
    schema:name "agent"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/e812918f10f81efd761fb2c9c424c9f4d524482d4df92c83016dbd98b6dbf69e> a schema:DefinedTerm ;
    schema:name "huggingartists"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/e84176b0758aa4e827b29420a9d219dd2ca6c52f17244c62fb550d73c263a33a> a schema:Person ;
    schema:name "MaziyarPanahi"^^xsd:string ;
    schema:url "https://huggingface.co/MaziyarPanahi"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/ec80fcbc48ed63fb19725597c40817bdccd1afeefab08c05e9e4df8a60b2d5eb> a schema:DefinedTerm ;
    schema:name "stable-diffusion-xl-diffusers"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ed3cf312800bc7233c923d178ecf3bb18ab45b2969d4c652c0e1b0abbce9cc39> a schema:Person ;
    schema:name "TinyLlama"^^xsd:string ;
    schema:url "https://huggingface.co/TinyLlama"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/f5983965e0a2f764b153dc7a793f4806ba17fe7509997b4e09e9e39852c780af> a schema:DefinedTerm ;
    schema:name "deep-rl-class"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f64e9e2107d219122c90233f1c6d719235a80c4f8a73e0cda62a43c1b4d56c70> a schema:Person ;
    schema:name "huggingartists"^^xsd:string ;
    schema:url "https://huggingface.co/huggingartists"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/fab3808aee8413b2338df69bd63f0825aa16fbd16d9b162e95a945870b98b527> a schema:Person ;
    schema:name "mjm4dl"^^xsd:string ;
    schema:url "https://huggingface.co/mjm4dl"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/fbc5ef54567df2b9fee883b665a53480149a7458fe3709e106706905b8466e2c> a schema:DefinedTerm ;
    schema:name "Politics"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/fc7cf57f062cb9e0d1e9d259e47e9f36b464ea9c455e880a85e25d6281fc3268> a schema:Person ;
    schema:name "rangapriyabatchu"^^xsd:string ;
    schema:url "https://huggingface.co/rangapriyabatchu"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/mnli/hypothesis> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'hypothesis' from the Hugging Face parquet file."@en ;
    schema:name "mnli/hypothesis"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "mnli/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
entailment (0), neutral (1), contradiction (2)"""@en ;
    schema:name "mnli/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli/premise> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'premise' from the Hugging Face parquet file."@en ;
    schema:name "mnli/premise"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "mnli/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched/hypothesis> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'hypothesis' from the Hugging Face parquet file."@en ;
    schema:name "mnli_matched/hypothesis"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "mnli_matched/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
entailment (0), neutral (1), contradiction (2)"""@en ;
    schema:name "mnli_matched/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched/premise> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'premise' from the Hugging Face parquet file."@en ;
    schema:name "mnli_matched/premise"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "mnli_matched/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/hypothesis> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'hypothesis' from the Hugging Face parquet file."@en ;
    schema:name "mnli_mismatched/hypothesis"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "mnli_mismatched/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
entailment (0), neutral (1), contradiction (2)"""@en ;
    schema:name "mnli_mismatched/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/premise> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'premise' from the Hugging Face parquet file."@en ;
    schema:name "mnli_mismatched/premise"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "mnli_mismatched/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "mrpc/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
not_equivalent (0), equivalent (1)"""@en ;
    schema:name "mrpc/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc/sentence1> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence1' from the Hugging Face parquet file."@en ;
    schema:name "mrpc/sentence1"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc/sentence2> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence2' from the Hugging Face parquet file."@en ;
    schema:name "mrpc/sentence2"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "mrpc/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/answers> a ns3:Field ;
    ns3:repeated true ;
    ns3:subField <http://mlentory.zbmed.de/mlentory_graph/plain_text/answers/answer_start>,
        <http://mlentory.zbmed.de/mlentory_graph/plain_text/answers/text> ;
    schema:description "Column 'answers' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/answers"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/answers/answer_start> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'answers' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/answers/answer_start"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/answers/text> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'answers' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/answers/text"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/context> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'context' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/context"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/id> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'id' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/id"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
neg (0), pos (1)"""@en ;
    schema:name "plain_text/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/question> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'question' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/question"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "plain_text/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/text> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'text' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/text"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text/title> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'title' from the Hugging Face parquet file."@en ;
    schema:name "plain_text/title"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "qnli/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
entailment (0), not_entailment (1)"""@en ;
    schema:name "qnli/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli/question> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'question' from the Hugging Face parquet file."@en ;
    schema:name "qnli/question"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli/sentence> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence' from the Hugging Face parquet file."@en ;
    schema:name "qnli/sentence"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "qnli/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "qqp/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
not_duplicate (0), duplicate (1)"""@en ;
    schema:name "qqp/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp/question1> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'question1' from the Hugging Face parquet file."@en ;
    schema:name "qqp/question1"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp/question2> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'question2' from the Hugging Face parquet file."@en ;
    schema:name "qqp/question2"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "qqp/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "rte/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
entailment (0), not_entailment (1)"""@en ;
    schema:name "rte/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte/sentence1> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence1' from the Hugging Face parquet file."@en ;
    schema:name "rte/sentence1"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte/sentence2> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence2' from the Hugging Face parquet file."@en ;
    schema:name "rte/sentence2"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "rte/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/sst2/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "sst2/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/sst2/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
negative (0), positive (1)"""@en ;
    schema:name "sst2/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/sst2/sentence> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence' from the Hugging Face parquet file."@en ;
    schema:name "sst2/sentence"@en .

<http://mlentory.zbmed.de/mlentory_graph/sst2/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "sst2/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "stsb/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb/label> a ns3:Field ;
    ns3:dataType schema:Float ;
    schema:description "Column 'label' from the Hugging Face parquet file."@en ;
    schema:name "stsb/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb/sentence1> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence1' from the Hugging Face parquet file."@en ;
    schema:name "stsb/sentence1"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb/sentence2> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence2' from the Hugging Face parquet file."@en ;
    schema:name "stsb/sentence2"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "stsb/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli/idx> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description "Column 'idx' from the Hugging Face parquet file."@en ;
    schema:name "wnli/idx"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli/label> a ns3:Field ;
    ns3:dataType schema:Integer ;
    schema:description """ClassLabel column 'label' from the Hugging Face parquet file.
Labels:
not_entailment (0), entailment (1)"""@en ;
    schema:name "wnli/label"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli/sentence1> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence1' from the Hugging Face parquet file."@en ;
    schema:name "wnli/sentence1"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli/sentence2> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Column 'sentence2' from the Hugging Face parquet file."@en ;
    schema:name "wnli/sentence2"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli/split> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "Split to which the example belongs to."@en ;
    schema:name "wnli/split"@en .

<http://mlentory.zbmed.de/mlentory_graph/02d5fe1af629b32020e3f74d866aa66a5001c17461f60e2ed7bb4a7224a40870> a schema:DefinedTerm ;
    schema:name "fill-mask"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/03a8010a33b7e6718a7b9231b9c21ef9f07a99b444199c8533c744e5ddf1b77a> a schema:DefinedTerm ;
    schema:name "image-generation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/24d02c4c9af718667adb32f98890ccae3daa567e09e890fbcc6baa38ee130dd0> a schema:ScholarlyArticle ;
    schema:abstract """From an environmental standpoint, there are a few crucial aspects of training
a neural network that have a major impact on the quantity of carbon that it
emits. These factors include: the location of the server used for training and
the energy grid that it uses, the length of the training procedure, and even
the make and model of hardware on which the training takes place. In order to
approximate these emissions, we present our Machine Learning Emissions
Calculator, a tool for our community to better understand the environmental
impact of training ML models. We accompany this tool with an explanation of the
factors cited above, as well as concrete actions that individual practitioners
and organizations can take to mitigate their carbon emissions."""^^xsd:string ;
    schema:author "{'name': 'Alexandra Luccioni', 'affiliation': None}"^^xsd:string,
        "{'name': 'Alexandre Lacoste', 'affiliation': None}"^^xsd:string,
        "{'name': 'Thomas Dandres', 'affiliation': None}"^^xsd:string,
        "{'name': 'Victor Schmidt', 'affiliation': None}"^^xsd:string ;
    schema:datePublished "2019-10-21"^^xsd:date ;
    schema:keywords "cs.CY"^^xsd:string,
        "cs.LG"^^xsd:string ;
    schema:name "Quantifying the Carbon Emissions of Machine Learning"^^xsd:string ;
    schema:url "https://arxiv.org/abs/1910.09700"^^xsd:anyURI,
        "https://arxiv.org/abs/1910.09700"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/251690051accbb99d763e77f3aed9b94caecf82b9e2db7c17f8adfca42a4ca39> a schema:DefinedTerm ;
    schema:description "Creates concise summaries of longer texts."^^xsd:string ;
    schema:name "Summarization"^^xsd:string,
        "summarization"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2529572666d8ae917752f418f0b008b820cf821a3ba355927ca159454740dce7> a schema:DefinedTerm ;
    schema:name "deep-reinforcement-learning"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2f28cc27d9dac219caa56a5425741f7391153065f1558fe7418d6ccc1c598fb4> a schema:DefinedTerm ;
    schema:description "Applies AI for robot control, perception, and decision-making."^^xsd:string ;
    schema:name "Robotics"^^xsd:string,
        "robotics"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/647af92c158ef5819b55c9c11a9565f9e413cf785be01a1c5d583087394c22e7> a schema:Person ;
    schema:name "mistralai"^^xsd:string ;
    schema:url "https://huggingface.co/mistralai"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/7b94abd1a94f42759242e24f381b59455f00250bb42fe5e8479187485a809d0f> a ns2:MLModel ;
    schema:name "mistralai/Mistral-7B-Instruct-v0.3"^^xsd:string ;
    schema:url <https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3> .

<http://mlentory.zbmed.de/mlentory_graph/85fa58f915b932926d1bb143b0bca80547964254fc92a39517d34e5b7250f9bd> a schema:Person ;
    schema:name "grimjim"^^xsd:string ;
    schema:url "https://huggingface.co/grimjim"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/97b8857097478874bd8e9cb141256c1acde4789374ce4d72be5580b0da08f403> a schema:Person ;
    schema:name "black-forest-labs"^^xsd:string ;
    schema:url "https://huggingface.co/black-forest-labs"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/9c51ba772888465129d6bda3b0b520a3a16960b8c108a8cd2276eb4fb85ee5f2> a schema:DefinedTerm ;
    schema:description "High-performance numerical computing library that combines NumPy, automatic differentiation, and GPU/TPU support for ML research."^^xsd:string ;
    schema:name "JAX"^^xsd:string,
        "jax"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9eb6952b89569e3a3863aebca62fd4619833b31bda93a0d0b5678c6627ef9275> a ns2:MLModel ;
    schema:name "black-forest-labs/FLUX.1-dev"^^xsd:string ;
    schema:url <https://huggingface.co/black-forest-labs/FLUX.1-dev> .

<http://mlentory.zbmed.de/mlentory_graph/9fda77bf870c60518d915563657cd9c230951c55c7abb6395dadcb0c53747922> a ns2:MLModel ;
    schema:name "meta-llama/Llama-3.2-1B"^^xsd:string ;
    schema:url <https://huggingface.co/meta-llama/Llama-3.2-1B> .

<http://mlentory.zbmed.de/mlentory_graph/a3c9ca93ab7ec29d067dd25733037f174c1326ef487292e5e1c9eea5cb35feb1> a schema:DefinedTerm ;
    schema:description "Set of reliable implementations of reinforcement learning algorithms in PyTorch."^^xsd:string ;
    schema:name "stable-baselines3"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/a464d85abf29c8b2f8dc299e9fe390a547c8450e42b7ab0ccaac88fdcaeaaa84> a schema:DefinedTerm ;
    schema:description "Predicts missing words or tokens in masked text."^^xsd:string ;
    schema:name "Fill Mask"^^xsd:string,
        "fill mask"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/ax_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/bc53f85f6e664d1c6ece992a9d80b2e0705d1b3c75028563adb2ed9f36aedec9> a schema:DefinedTerm ;
    schema:description "Memory-safe, high-performance language used for building efficient ML systems and infrastructure."^^xsd:string ;
    schema:name "Rust"^^xsd:string,
        "rust"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/cola_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/default_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/e50ce491d4abac67bd512ffe733d1ed10412db83802a3b15e0cc40b0bcc5203a> a ns2:MLModel ;
    schema:name "mistralai/Mistral-7B-v0.3"^^xsd:string ;
    schema:url <https://huggingface.co/mistralai/Mistral-7B-v0.3> .

<http://mlentory.zbmed.de/mlentory_graph/mnli_matched_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_mismatched_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/mnli_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/mrpc_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/plain_text_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/qnli_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/qqp_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/rte_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/sst2_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/stsb_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/wnli_splits/split_name> a ns3:Field ;
    ns3:dataType schema:Text ;
    schema:description "The name of the split."@en ;
    schema:name "split_name"@en .

<http://mlentory.zbmed.de/mlentory_graph/0293985a9647a36f09130bb7b83e928f80516acf3bd2d2fe7e311efc911f3984> a ns2:Dataset,
        schema:Dataset ;
    schema:name "HuggingFaceH4/ultrafeedback_binarized"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/034a881c083ce582a4998bf12129f65097a143b5a5e28119457461204aaa636d> a ns2:Dataset ;
    schema:name "simonestradasch/NERcomp2lower"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/simonestradasch/NERcomp2lower"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/0b6c077d0e7e512fe4a09f8d65944ef93c47c0c822ba512f9d69a0205fbc4e04> a schema:DefinedTerm ;
    schema:name "lora"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/11b7c117f8cd33c560f4862966654c32eb570fe472aa173c6a54749d61d20636> a schema:Person ;
    schema:name "SG161222"^^xsd:string ;
    schema:url "https://huggingface.co/SG161222"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/20227b77ce090a21afa4ed043d9908132d4def622f31e14f1f3ef52047648a8c> a ns2:Dataset,
        schema:Dataset ;
    schema:name "cerebras/SlimPajama-627B"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/cerebras/SlimPajama-627B"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/267e04ff20e54a667a0c17e69b7dd3f07b14ad451d8a605d3b5b771693ba73ba> a schema:DefinedTerm ;
    schema:name "alignment-handbook"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2730e6211466c050884bcc521d8aaf9f0af8a2993b56ca3f9ab751a178b4f970> a schema:ScholarlyArticle ;
    schema:abstract """Post-training quantization (PTQ) techniques applied to weights, activations,
and the KV cache greatly reduce memory usage, latency, and power consumption of
Large Language Models (LLMs), but may lead to large quantization errors when
outliers are present. Rotating activation or weight matrices helps remove
outliers and benefits quantization. In this work, we identify a collection of
applicable rotation parameterizations that lead to identical outputs in
full-precision Transformer architectures while enhancing quantization accuracy.
In addition, we find that some random rotations lead to much better
quantization than others, with an up to 13 points difference in downstream
zero-shot reasoning performance. As a result, we propose SpinQuant, a novel
approach that incorporates learned rotation matrices for optimal quantized
network accuracy. With 4-bit quantization of weight, activation, and KV-cache,
SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full
precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by
19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also
outperforms concurrent work QuaRot, which applies random rotations to remove
outliers. In particular, for LLaMA-3 8B models that are hard to quantize,
SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.
Code is available at https://github.com/facebookresearch/SpinQuant."""^^xsd:string ;
    schema:author "{'name': 'Bilge Soran', 'affiliation': None}"^^xsd:string,
        "{'name': 'Changsheng Zhao', 'affiliation': None}"^^xsd:string,
        "{'name': 'Dhruv Choudhary', 'affiliation': None}"^^xsd:string,
        "{'name': 'Igor Fedorov', 'affiliation': None}"^^xsd:string,
        "{'name': 'Raghuraman Krishnamoorthi', 'affiliation': None}"^^xsd:string,
        "{'name': 'Tijmen Blankevoort', 'affiliation': None}"^^xsd:string,
        "{'name': 'Vikas Chandra', 'affiliation': None}"^^xsd:string,
        "{'name': 'Yuandong Tian', 'affiliation': None}"^^xsd:string,
        "{'name': 'Zechun Liu', 'affiliation': None}"^^xsd:string ;
    schema:datePublished "2024-05-26"^^xsd:date ;
    schema:keywords "cs.AI"^^xsd:string,
        "cs.CL"^^xsd:string,
        "cs.CV"^^xsd:string,
        "cs.LG"^^xsd:string ;
    schema:name "SpinQuant: LLM quantization with learned rotations"^^xsd:string ;
    schema:url "https://arxiv.org/abs/2405.16406"^^xsd:anyURI,
        "https://arxiv.org/abs/2405.16406"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/2976f620ce615218c4caaaa64d292b6c1cc2785cda2fe2fcc1fabefc0c73faef> a schema:DefinedTerm ;
    schema:name "4-bit"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/3208536fd5f5414cfa274461aae830a0b3b39e3ba2e9e782979c75d3f8277fec> a ns2:Dataset,
        schema:Dataset ;
    schema:name "HuggingFaceH4/ultrachat_200k"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/330695ad79106df6543d0dc74fe936e62af048d69f947d7923324fead03d8db2> a ns2:Dataset,
        schema:Dataset ;
    schema:name "cmotions/Beatles_lyrics"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/cmotions/Beatles_lyrics"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/3b4b981961ee604488ce9db2b6cea45a4fb777fb6ee8ef76bb274e14ff6cbdf4> a schema:DefinedTerm ;
    schema:description "Trains agents through interaction with environments using rewards."^^xsd:string ;
    schema:name "Reinforcement Learning"^^xsd:string,
        "reinforcement learning"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/402c5284d04ce29828735d46f4b53890fbf3cfc9f7c900018e7bea70762dc667> a ns2:Dataset,
        schema:Dataset ;
    schema:name "sst2"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/sst2"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/40c6f438362f1a92103a016e23ec429f4b4dc082c4d0b6b7f94f07db46fe1706> a schema:Person ;
    schema:name "RichardErkhov"^^xsd:string ;
    schema:url "https://huggingface.co/RichardErkhov"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/4e5443728f51b2323bf3a1c4a6a6d8612484cbca0a8b0beeb5fb0258d1df5196> a schema:DefinedTerm ;
    schema:description "Open Neural Network Exchange format that enables interoperability between different ML frameworks."^^xsd:string ;
    schema:name "ONNX"^^xsd:string,
        "onnx"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/547c2a6f152d09c8e3dbd3a857c5481c4c4d77accf781e6fe237d9c49f47d9a7> a schema:DefinedTerm ;
    schema:name "unsloth"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/558f0f0c846c9732fb258f139d8e03a6a976172ea778c1c3307e3a2eeb496834> a ns2:Dataset,
        schema:Dataset ;
    schema:name "squad"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/squad"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/594a6f3cf66106516dc4e80f44dbe528cdcf0fb397d75a65d1b3b2a81dfc0f92> a schema:DefinedTerm ;
    schema:name "reinforcement-learning"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/5a898586bcb0bb2e26a98d976b8f37baf784497ff2bf75b2249741a14680478c> a ns2:Dataset,
        schema:Dataset ;
    schema:name "imdb"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/imdb"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/5c500bb7c0078d862991837b70c69f9bce83ad76967f92453efb08964a791605> a ns2:Dataset ;
    schema:name "bookcorpus"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/bookcorpus"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/812d8303ed5720ece9efd43abaf31605194f7c778b2e76978276fbdaaa6d41be> a ns2:Dataset,
        schema:Dataset ;
    schema:name "huggingartists/fascinoma"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/huggingartists/fascinoma"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/89b5833251f8f95309bb1d81c7cbd5e686d9f1e1a94b81869c2661ff9301fb64> a schema:DefinedTerm ;
    schema:name "dpo"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/9c03090b68b1e45d6bebe9b9d1ea103b6a4c064ceb71f16f08a5ae59d9bac70e> a schema:Person ;
    schema:name "meta-llama"^^xsd:string ;
    schema:url "https://huggingface.co/meta-llama"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/a787507208799647a209772884d76edfc0a14e5f209b09e235fbff7797a1a812> a schema:ScholarlyArticle ;
    schema:abstract """Machine Learning (ML) workloads have rapidly grown in importance, but raised
concerns about their carbon footprint. Four best practices can reduce ML
training energy by up to 100x and CO2 emissions up to 1000x. By following best
practices, overall ML energy use (across research, development, and production)
held steady at <15% of Google's total energy use for the past three years. If
the whole ML field were to adopt best practices, total carbon emissions from
training would reduce. Hence, we recommend that ML papers include emissions
explicitly to foster competition on more than just model quality. Estimates of
emissions in papers that omitted them have been off 100x-100,000x, so
publishing emissions has the added benefit of ensuring accurate accounting.
Given the importance of climate change, we must get the numbers right to make
certain that we work on its biggest challenges."""^^xsd:string ;
    schema:author "{'name': 'Chen Liang', 'affiliation': None}"^^xsd:string,
        "{'name': 'Daniel Rothchild', 'affiliation': None}"^^xsd:string,
        "{'name': 'David Patterson', 'affiliation': None}"^^xsd:string,
        "{'name': 'David So', 'affiliation': None}"^^xsd:string,
        "{'name': 'Jeff Dean', 'affiliation': None}"^^xsd:string,
        "{'name': 'Joseph Gonzalez', 'affiliation': None}"^^xsd:string,
        "{'name': 'Lluis-Miquel Munguia', 'affiliation': None}"^^xsd:string,
        "{'name': 'Maud Texier', 'affiliation': None}"^^xsd:string,
        "{'name': 'Quoc Le', 'affiliation': None}"^^xsd:string,
        "{'name': 'Urs Hölzle', 'affiliation': None}"^^xsd:string ;
    schema:datePublished "2022-04-11"^^xsd:date ;
    schema:keywords "cs.AI"^^xsd:string,
        "cs.GL"^^xsd:string,
        "cs.LG"^^xsd:string ;
    schema:name "The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink"^^xsd:string ;
    schema:url "https://arxiv.org/abs/2204.05149"^^xsd:anyURI,
        "https://arxiv.org/abs/2204.05149"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/c372f0bca81211b35cd0974aabe8500d5ca4479cddab1685b44f24bd5523dc9e> a schema:Person ;
    schema:name "YYYYYYibo"^^xsd:string ;
    schema:url "https://huggingface.co/YYYYYYibo"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/c7504a968f417dfe1555958db6caabff413bf4f321bbb9c477d4ff3a9352a0e4> a schema:DefinedTerm ;
    schema:name "flux"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/daa54867c3884268e9ee1292bae85ade578214c2f224fae26233cc78919a1a20> a ns2:Dataset,
        schema:Dataset ;
    schema:name "glue"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/glue"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/e2bd5001a46246a7e8474accffaa2931c529f86701173c10c48e9befe9197517> a schema:DefinedTerm ;
    schema:name "sft"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f5b41a8e8b49e0f38dadd1a8bb57cddfa9aec7e09638c87319e76718ceb45770> a ns2:Dataset ;
    schema:name "bigcode/starcoderdata"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/bigcode/starcoderdata"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/0813ac63520e9dbe708eec9898230a9dbdb1c48971b07fa1ad53fb6dfaa7a564> a schema:DefinedTerm ;
    schema:name "bitsandbytes"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/11ee21b926795a2b7d2cb2d939044519ace34ac86e6b6a59a47dcb4bba5eb7a4> a schema:DefinedTerm ;
    schema:name "facebook"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/4407cda96af0afeaf4c1c5aaacc2cddb4eccb07bfeb1a5dded4a788a19846707> a schema:Person ;
    schema:name "distilbert"^^xsd:string ;
    schema:url "https://huggingface.co/distilbert"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/759dcfce11b186065b46fb94fef1c569da9001c123861ba72ba0708e554bd6c4> a schema:DefinedTerm ;
    schema:name "meta"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/7ad8435aa27d409e8692d8f6043742210150fe3cf420b9abaad32ffe8a63b7c7> a schema:DefinedTerm ;
    schema:name "text-classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/85f0e69a70ce025a206d6ddbf78d989a3cabbdae215f2045df9b67b21a9d6780> a schema:DefinedTerm ;
    schema:description "Categorizes text documents into predefined classes."^^xsd:string ;
    schema:name "Text Classification"^^xsd:string,
        "text classification"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/96b96e1fc1370c6454f55ef5bb235e3108a8e1b32665ef0b72dd54ba220b3a53> a schema:ScholarlyArticle ;
    schema:abstract """As Transfer Learning from large-scale pre-trained models becomes more
prevalent in Natural Language Processing (NLP), operating these large models in
on-the-edge and/or under constrained computational training or inference
budgets remains challenging. In this work, we propose a method to pre-train a
smaller general-purpose language representation model, called DistilBERT, which
can then be fine-tuned with good performances on a wide range of tasks like its
larger counterparts. While most prior work investigated the use of distillation
for building task-specific models, we leverage knowledge distillation during
the pre-training phase and show that it is possible to reduce the size of a
BERT model by 40%, while retaining 97% of its language understanding
capabilities and being 60% faster. To leverage the inductive biases learned by
larger models during pre-training, we introduce a triple loss combining
language modeling, distillation and cosine-distance losses. Our smaller, faster
and lighter model is cheaper to pre-train and we demonstrate its capabilities
for on-device computations in a proof-of-concept experiment and a comparative
on-device study."""^^xsd:string ;
    schema:author "{'name': 'Julien Chaumond', 'affiliation': None}"^^xsd:string,
        "{'name': 'Lysandre Debut', 'affiliation': None}"^^xsd:string,
        "{'name': 'Thomas Wolf', 'affiliation': None}"^^xsd:string,
        "{'name': 'Victor Sanh', 'affiliation': None}"^^xsd:string ;
    schema:datePublished "2019-10-02"^^xsd:date ;
    schema:keywords "cs.CL"^^xsd:string ;
    schema:name "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"^^xsd:string ;
    schema:url "https://arxiv.org/abs/1910.01108"^^xsd:anyURI,
        "https://arxiv.org/abs/1910.01108"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/974df5e3234c3601080dac4fa96eebf5d19d40df41e9e30cb61084d9fbefea1e> a schema:DefinedTerm ;
    schema:description "Visualization toolkit for machine learning experiments that helps track metrics, visualize graphs, and explore embeddings."^^xsd:string ;
    schema:name "TensorBoard"^^xsd:string,
        "tensorboard"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d77765c1bc2a45cffe4e1ace66ea68c61a819d17e7d6159e6840523d288dbed3> a schema:DefinedTerm ;
    schema:name "llama-3"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/28f6fc79227e13d54a24f63fe2a8971b6f281ac3ca778d4303f84baacbc18a18> a schema:DefinedTerm ;
    schema:name "trl"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f57740f97820ddb55f90898c07a60a736e9a9030d5eac125af5c595ad0638c8f> a schema:DefinedTerm ;
    schema:name "text-to-image"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/13e6e4095523310627190177e08c0206ca97e76f4c4565c15c2b35270daa2588> a schema:DefinedTerm ;
    schema:name "model-index"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/151c5c18796e684c7f0c77ded700ed4efddb5eb060b8924db2428896c9cebd7c> a ns2:Dataset ;
    schema:name "wikipedia"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/wikipedia"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/ac53aa24026b2fe33d07035764d57e7a2cd56ea149abf297a21d67644e6e5e21> a schema:DefinedTerm ;
    schema:name "distilbert"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/d462b48c96b732d754a938aa86dbd16bfb7478ca5b0c1f1f006c611c112094e9> a schema:DefinedTerm ;
    schema:description "State-of-the-art library for diffusion models across multiple modalities like vision and audio."^^xsd:string ;
    schema:name "Diffusers"^^xsd:string,
        "diffusers"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/660591e88f00e321049a76543ded76af3005cf92f0a834054184e0177278716c> a schema:DefinedTerm ;
    schema:description "Generates images from textual descriptions or prompts."^^xsd:string ;
    schema:name "Text to Image"^^xsd:string,
        "text to image"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/04151ef09fe56a2be18a7d842066720d6e7cee10ed5bf1a3363193444decd94d> a schema:DefinedTerm ;
    schema:name "generated_from_trainer"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/3f6c545843bdfc7b6a35a45763f23aed786b4f59001ae2cfa409462bbf9aa292> a ns2:Dataset ;
    schema:name "original"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/original"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/50940c98643679207479d4eef73d0b884d136dfc6a12d3ec40c38a2bac4e05c7> a ns2:Dataset ;
    schema:name "updated"^^xsd:string ;
    schema:url "https://huggingface.co/datasets/updated"^^xsd:anyURI .

<http://mlentory.zbmed.de/mlentory_graph/ddb15bfa0ef843723bc4c8a81ab1d826b1fc4ff36ed13faf8f8f3841261978b8> a schema:DefinedTerm ;
    schema:description "Models created by Mistral, a French artificial intelligence startup, headquartered in Paris. It specializes in open-weight large language models."^^xsd:string ;
    schema:name "mistral"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/1f1b91c653f20f0cf893aeb65fa83459983d2cb51e18030488be0e30b0e6db67> a schema:DefinedTerm ;
    schema:description "Models based on LLaMA (Large Language Model Meta AI), a family of large language models that use a transformer architecture."^^xsd:string ;
    schema:name "llama"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/3e9a4e1232f617b33fa8460d6c142ee49e69eb5c55acfd1e618d58e7495955e8> a schema:DefinedTerm ;
    schema:description "Deep learning framework that provides tensor computation with GPU acceleration and automatic differentiation. Popular for research and production ML models."^^xsd:string ;
    schema:name "PyTorch"^^xsd:string,
        "pytorch"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/61cce84de930f0da8c78ddef3203a2afac70cf5e46684f0407180738cd821965> a schema:DefinedTerm ;
    schema:name "conversational"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/f59cf55f3a86acf0bcfc020eac552a72c13f1212154211a7bf30504548661f8d> a schema:DefinedTerm ;
    schema:description "Optimized backend for deploying and serving Large Language Models (LLMs) with high performance."^^xsd:string ;
    schema:name "text-generation-inference"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/6925cb204ab4d5280c948ac7e23dd8f7b48d1de433cf9edc5c30a8d87ce3b317> a schema:DefinedTerm ;
    schema:description "Creates natural language text based on initial prompts."^^xsd:string ;
    schema:name "Text Generation"^^xsd:string,
        "text generation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/79cf083846a8329d2912fcd7f375233c37a54917af291bb945f0b58bc735b8ad> a schema:DefinedTerm ;
    schema:name "text-generation"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/87b927fef88a46248b3b208472aec7bfeb38667386b309f46772e26b40f6affb> a schema:DefinedTerm ;
    schema:description "Fast and safe serialization format for storing and distributing machine learning model weights."^^xsd:string ;
    schema:name "Safetensors"^^xsd:string,
        "safetensors"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/df4982af808ead1a57663cc7a83e60059f6a6ae1baf83a270fdf8bdfef11a3de> a schema:DefinedTerm ;
    schema:description "Library providing pretrained models for NLP and CV tasks, with implementations in PyTorch and TensorFlow."^^xsd:string ;
    schema:name "Transformers"^^xsd:string,
        "transformers"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/81c4f52fcccf6ef0d25453aedb7cebe30c7034e3b2fa752246d0e43da889a0e1> a schema:DefinedTerm ;
    schema:name "autotrain_compatible"^^xsd:string .

<http://mlentory.zbmed.de/mlentory_graph/0dce0f5e7319b6178cd79d5f2163f0e134d0064f4ea4f93e8afeec1e7d78fa8d> a schema:DefinedTerm ;
    schema:name "endpoints_compatible"^^xsd:string .

