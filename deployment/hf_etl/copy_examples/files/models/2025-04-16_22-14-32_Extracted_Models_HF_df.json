[
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4687122852,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5147310793,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The models can be used directly as a Bert-class token classification model following the [instructions from Huggingface](https:\/\/huggingface.co\/docs\/transformers\/tasks\/token_classification). Feel free to inspect [this file](https:\/\/github.com\/TalkBank\/batchalign\/blob\/73ec04761ed3ee2eba04ba0cf14dc898f88b72f7\/baln\/utokengine.py#L85-L94) for a sense of what the classes means. Alternatively, to get the full analysis possible with the model, it is best combined with the TalkBank Batchalign suite of analysis software, [available here](https:\/\/github.com\/talkbank\/batchalign2), using `transcribe` mode.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5637327135,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "token classification"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model Category is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.521242857,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5226614177,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"talkbank",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"The models can be used directly as a Bert-class token classification model following the [instructions from Huggingface](https:\/\/huggingface.co\/docs\/transformers\/tasks\/token_classification).",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5453028977,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6035332084,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5217501521,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5226069093,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/talkbank\/CHATUtterance-en\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/talkbank\/CHATUtterance-en\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6181327701,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported by the TalkBank Batchalign CHATUtterance are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5037397593,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5382139683,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage:\n- en\n---\n\n# TalkBank Batchalign CHATUtterance\nCHATUtterance is a series of Bert-derivative models designed for the task of Utterance Segmentation released by the TalkBank project, which is trained on the the utterance diarization samples given by [The Michigan Corpus of Academic Spoken English](https:\/\/ca.talkbank.org\/access\/MICASE.html).\n\n## Usage\nThe models can be used directly as a Bert-class token classification model following the [instructions from Huggingface](https:\/\/huggingface.co\/docs\/transformers\/tasks\/token_classification). Feel free to inspect [this file](https:\/\/github.com\/TalkBank\/batchalign\/blob\/73ec04761ed3ee2eba04ba0cf14dc898f88b72f7\/baln\/utokengine.py#L85-L94) for a sense of what the classes means. Alternatively, to get the full analysis possible with the model, it is best combined with the TalkBank Batchalign suite of analysis software, [available here](https:\/\/github.com\/talkbank\/batchalign2), using `transcribe` mode.\n\nTarget labels:\n\n- `0`: regular form\n- `1`: start of utterance\/capitalized word\n- `2`: end of declarative utterance (end this utterance with a `.`)\n- `3`: end of interrogative utterance (end this utterance with a `?`)\n- `4`: end of exclamatory utterance (end this utterance with a `!`)\n- `5`: break in the utterance; depending on orthography one can insert a `,`\n\n\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5755729973,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5335375369,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4945907295,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/talkbank\/CHATUtterance-en",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5828697681,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5808123648,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5056108683,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5677142143,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5541159213,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2023-12-04T00:39:45+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-02-25T21:58:47+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2023-12-04T00:39:45+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/talkbank\/CHATUtterance-en\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5257394016,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "bert",
                    "token-classification",
                    "en",
                    "autotrain_compatible",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "pytorch"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5312033892,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5797356665,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlanguage:\n- en\n---\n\n# TalkBank Batchalign CHATUtterance\nCHATUtterance is a series of Bert-derivative models designed for the task of Utterance Segmentation released by the TalkBank project, which is trained on the the utterance diarization samples given by [The Michigan Corpus of Academic Spoken English](https:\/\/ca.talkbank.org\/access\/MICASE.html).\n\n## Usage\nThe models can be used directly as a Bert-class token classification model following the [instructions from Huggingface](https:\/\/huggingface.co\/docs\/transformers\/tasks\/token_classification). Feel free to inspect [this file](https:\/\/github.com\/TalkBank\/batchalign\/blob\/73ec04761ed3ee2eba04ba0cf14dc898f88b72f7\/baln\/utokengine.py#L85-L94) for a sense of what the classes means. Alternatively, to get the full analysis possible with the model, it is best combined with the TalkBank Batchalign suite of analysis software, [available here](https:\/\/github.com\/talkbank\/batchalign2), using `transcribe` mode.\n\nTarget labels:\n\n- `0`: regular form\n- `1`: start of utterance\/capitalized word\n- `2`: end of declarative utterance (end this utterance with a `.`)\n- `3`: end of interrogative utterance (end this utterance with a `?`)\n- `4`: end of exclamatory utterance (end this utterance with a `!`)\n- `5`: break in the utterance; depending on orthography one can insert a `,`\n\n\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/talkbank\/CHATUtterance-en",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"CHATUtterance-en",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/talkbank\/CHATUtterance-en",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3952923268,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "marsyas\/gtzan"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "ntu-spml\/distilhubert"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4933754206,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended uses & limitations: distilhubert-finetuned-gtzan > Intended uses & limitations: distilhubert-finetuned-gtzan > Intended uses & limitations:\nMore information needed",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4508726746,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "audio classification"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4709799886,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5128698945,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"itsindro",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "marsyas\/gtzan"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "marsyas\/gtzan"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4911523014,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5586830676,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4862279594,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4938403368,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/itsindro\/distilhubert-finetuned-gtzan\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/itsindro\/distilhubert-finetuned-gtzan\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5299414098,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5143142641,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4974724352,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\nlicense: apache-2.0\nbase_model: ntu-spml\/distilhubert\ntags:\n- generated_from_trainer\ndatasets:\n- marsyas\/gtzan\nmetrics:\n- accuracy\nmodel-index:\n- name: distilhubert-finetuned-gtzan\n  results:\n  - task:\n      name: Audio Classification\n      type: audio-classification\n    dataset:\n      name: marsyas\/gtzan\n      type: marsyas\/gtzan\n      config: all\n      split: train\n      args: all\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.83\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilhubert-finetuned-gtzan\n\nThis model is a fine-tuned version of [ntu-spml\/distilhubert](https:\/\/huggingface.co\/ntu-spml\/distilhubert) on the marsyas\/gtzan dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5878\n- Accuracy: 0.83\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| 1.978         | 1.0   | 113  | 1.4803          | 0.6      |\n| 1.3816        | 2.0   | 226  | 1.0322          | 0.72     |\n| 1.0378        | 3.0   | 339  | 0.9786          | 0.75     |\n| 0.7748        | 4.0   | 452  | 0.7674          | 0.74     |\n| 0.6074        | 5.0   | 565  | 0.6434          | 0.81     |\n| 0.5075        | 6.0   | 678  | 0.5948          | 0.77     |\n| 0.3899        | 7.0   | 791  | 0.5878          | 0.83     |\n| 0.2387        | 8.0   | 904  | 0.5331          | 0.82     |\n| 0.1927        | 9.0   | 1017 | 0.5601          | 0.83     |\n| 0.1532        | 10.0  | 1130 | 0.5554          | 0.83     |\n\n\n### Framework versions\n\n- Transformers 4.48.0\n- Pytorch 2.5.1+cu124\n- Datasets 3.2.0\n- Tokenizers 0.21.0\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5075304359,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"The required software dependencies for the model are:\n\n- Transformers 4.48.0\n- Pytorch 2.5.1+cu124\n- Datasets 3.2.0\n- Tokenizers 0.21.0",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4809433222,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4675408304,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/itsindro\/distilhubert-finetuned-gtzan",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4557553381,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4209771156,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4389994293,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4630239904,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4509456307,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2025-01-13T20:17:55+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2025-01-13T20:18:37+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2025-01-13T20:17:55+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/itsindro\/distilhubert-finetuned-gtzan\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4439742118,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "hubert",
                    "audio-classification",
                    "generated_from_trainer",
                    "dataset:marsyas\/gtzan",
                    "base_model:ntu-spml\/distilhubert",
                    "base_model:finetune:ntu-spml\/distilhubert",
                    "license:apache-2.0",
                    "model-index",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4574479312,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4806791246,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\nlicense: apache-2.0\nbase_model: ntu-spml\/distilhubert\ntags:\n- generated_from_trainer\ndatasets:\n- marsyas\/gtzan\nmetrics:\n- accuracy\nmodel-index:\n- name: distilhubert-finetuned-gtzan\n  results:\n  - task:\n      name: Audio Classification\n      type: audio-classification\n    dataset:\n      name: marsyas\/gtzan\n      type: marsyas\/gtzan\n      config: all\n      split: train\n      args: all\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.83\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilhubert-finetuned-gtzan\n\nThis model is a fine-tuned version of [ntu-spml\/distilhubert](https:\/\/huggingface.co\/ntu-spml\/distilhubert) on the marsyas\/gtzan dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5878\n- Accuracy: 0.83\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\n- lr_scheduler_type: linear\n- num_epochs: 10\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| 1.978         | 1.0   | 113  | 1.4803          | 0.6      |\n| 1.3816        | 2.0   | 226  | 1.0322          | 0.72     |\n| 1.0378        | 3.0   | 339  | 0.9786          | 0.75     |\n| 0.7748        | 4.0   | 452  | 0.7674          | 0.74     |\n| 0.6074        | 5.0   | 565  | 0.6434          | 0.81     |\n| 0.5075        | 6.0   | 678  | 0.5948          | 0.77     |\n| 0.3899        | 7.0   | 791  | 0.5878          | 0.83     |\n| 0.2387        | 8.0   | 904  | 0.5331          | 0.82     |\n| 0.1927        | 9.0   | 1017 | 0.5601          | 0.83     |\n| 0.1532        | 10.0  | 1130 | 0.5554          | 0.83     |\n\n\n### Framework versions\n\n- Transformers 4.48.0\n- Pytorch 2.5.1+cu124\n- Datasets 3.2.0\n- Tokenizers 0.21.0\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/itsindro\/distilhubert-finetuned-gtzan",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"distilhubert-finetuned-gtzan",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/itsindro\/distilhubert-finetuned-gtzan",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.421280697,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "conll2003"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4950636327,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of distilbert-base-uncased-finetuned-ner is not explicitly stated in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4549634755,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "token classification"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The context provided does not contain information about the model category.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4614747763,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5047184378,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"indridinn",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "conll2003"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "conll2003"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4717402309,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.556853354,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4822757989,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4818117321,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/indridinn\/distilbert-base-uncased-finetuned-ner\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/indridinn\/distilbert-base-uncased-finetuned-ner\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The context provided does not contain any information about a distribution.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5153958499,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5290640295,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5168792903,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- conll2003\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      args: conll2003\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.9274720407485328\n    - name: Recall\n      type: recall\n      value: 0.9370175634858485\n    - name: F1\n      type: f1\n      value: 0.932220367278798\n    - name: Accuracy\n      type: accuracy\n      value: 0.9836370279759162\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-ner\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https:\/\/huggingface.co\/distilbert-base-uncased) on the conll2003 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0610\n- Precision: 0.9275\n- Recall: 0.9370\n- F1: 0.9322\n- Accuracy: 0.9836\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|\n| 0.2507        | 1.0   | 878  | 0.0714          | 0.9181    | 0.9243 | 0.9212 | 0.9813   |\n| 0.0516        | 2.0   | 1756 | 0.0617          | 0.9208    | 0.9325 | 0.9266 | 0.9828   |\n| 0.0306        | 3.0   | 2634 | 0.0610          | 0.9275    | 0.9370 | 0.9322 | 0.9836   |\n\n\n### Framework versions\n\n- Transformers 4.11.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4991595894,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"The required software dependencies for the model are:\n\n- Transformers 4.11.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4755427986,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4613135457,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/indridinn\/distilbert-base-uncased-finetuned-ner",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4572413266,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4344429523,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4318439066,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4634883851,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4487816244,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02T23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2021-10-01T22:29:15+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02T23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/indridinn\/distilbert-base-uncased-finetuned-ner\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.44099769,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "tensorboard",
                    "distilbert",
                    "token-classification",
                    "generated_from_trainer",
                    "dataset:conll2003",
                    "license:apache-2.0",
                    "model-index",
                    "autotrain_compatible",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "pytorch",
                    "tensorboard"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.44241485,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:version":[
            {
                "data":"The version of the CreativeWork embodied by a specified resource is Transformers 4.11.2.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4803909659,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- conll2003\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nmodel-index:\n- name: distilbert-base-uncased-finetuned-ner\n  results:\n  - task:\n      name: Token Classification\n      type: token-classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      args: conll2003\n    metrics:\n    - name: Precision\n      type: precision\n      value: 0.9274720407485328\n    - name: Recall\n      type: recall\n      value: 0.9370175634858485\n    - name: F1\n      type: f1\n      value: 0.932220367278798\n    - name: Accuracy\n      type: accuracy\n      value: 0.9836370279759162\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-ner\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https:\/\/huggingface.co\/distilbert-base-uncased) on the conll2003 dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0610\n- Precision: 0.9275\n- Recall: 0.9370\n- F1: 0.9322\n- Accuracy: 0.9836\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|\n| 0.2507        | 1.0   | 878  | 0.0714          | 0.9181    | 0.9243 | 0.9212 | 0.9813   |\n| 0.0516        | 2.0   | 1756 | 0.0617          | 0.9208    | 0.9325 | 0.9266 | 0.9828   |\n| 0.0306        | 3.0   | 2634 | 0.0610          | 0.9275    | 0.9370 | 0.9322 | 0.9836   |\n\n\n### Framework versions\n\n- Transformers 4.11.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/indridinn\/distilbert-base-uncased-finetuned-ner",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"distilbert-base-uncased-finetuned-ner",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/indridinn\/distilbert-base-uncased-finetuned-ner",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6796380877,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "CollectiveCognition\/chats-data-2023-09-27"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6027879417,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7432484031,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The context provided does not contain information about the model category.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6251566708,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6983520389,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"mncai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "CollectiveCognition\/chats-data-2023-09-27"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "CollectiveCognition\/chats-data-2023-09-27"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6779983044,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6999254823,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6392570138,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6325483322,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/mncai\/Mistral-7B-CollectiveCognition\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/mncai\/Mistral-7B-CollectiveCognition\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2306.02707"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.718637079,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported by the Llama-2 model are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5874275267,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6205078065,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\npipeline_tag: text-generation\nlicense: mit\nlanguage:\n- en\nlibrary_name: transformers\ntags:\n- MindsAndCompany\ndatasets:\n- CollectiveCognition\/chats-data-2023-09-27\n---\n\n## Model Details\n\n* **Developed by**: [Minds And Company](https:\/\/mnc.ai\/)\n* **Backbone Model**: [Mistral-7B-v0.1](mistralai\/Mistral-7B-v0.1)\n* **Library**: [HuggingFace Transformers](https:\/\/github.com\/huggingface\/transformers)\n\n\n## Dataset Details\n\n### Used Datasets\n- CollectiveCognition\/chats-data-2023-09-27\n\n### Prompt Template\n- Llama Prompt Template\n\n\n\n## Limitations & Biases:\n\nLlama2 and fine-tuned variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2 and any fine-tuned varient's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2 variants, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at https:\/\/ai.meta.com\/llama\/responsible-use-guide\/\n\n## License Disclaimer:\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\n\n## Contact Us\n\n- [Minds And Company](https:\/\/mnc.ai\/)\n\n## Citiation:\n\nPlease kindly cite using the following BibTeX:\n\n```bibtex\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```\n@misc{Orca-best,\n  title = {Orca-best: A filtered version of orca gpt4 dataset.},\n  author = {Shahul Es},\n  year = {2023},\n  publisher = {HuggingFace},\n  journal = {HuggingFace repository},\n  howpublished = {\\url{https:\/\/huggingface.co\/datasets\/shahules786\/orca-best\/},\n}\n```\n\n```\n@software{touvron2023llama2,\n  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n  author={Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\n Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith,\nRanjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu , Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom},\n  year={2023}\n}\n```\n\n> Readme format: [Riiid\/sheep-duck-llama-2-70b-v1.1](https:\/\/huggingface.co\/Riiid\/sheep-duck-llama-2-70b-v1.1)",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nLicense Disclaimer:: License Disclaimer::\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\nCitiation: - Par. 1: Citiation::\nPlease kindly cite using the following BibTeX:\n\nQuestion: Find the software Help in the following text, here is a description of",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.68369174,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6519713402,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.606790781,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/mncai\/Mistral-7B-CollectiveCognition",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is special in HTML 5 provides a special mechanism for indicating authorship via the rel tag.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7247718871,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7492290735,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6916864812,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"The text does not provide any information about contributors or related sections.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7125613391,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"The copyright holder is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7476888299,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2023-10-20T06:54:07+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2023-10-22T04:35:52+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2023-10-20T06:54:07+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/mncai\/Mistral-7B-CollectiveCognition\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6921952367,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "llama",
                    "text-generation",
                    "MindsAndCompany",
                    "en",
                    "dataset:CollectiveCognition\/chats-data-2023-09-27",
                    "arxiv:2306.02707",
                    "license:mit",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "pytorch"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6796233654,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7142108679,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\npipeline_tag: text-generation\nlicense: mit\nlanguage:\n- en\nlibrary_name: transformers\ntags:\n- MindsAndCompany\ndatasets:\n- CollectiveCognition\/chats-data-2023-09-27\n---\n\n## Model Details\n\n* **Developed by**: [Minds And Company](https:\/\/mnc.ai\/)\n* **Backbone Model**: [Mistral-7B-v0.1](mistralai\/Mistral-7B-v0.1)\n* **Library**: [HuggingFace Transformers](https:\/\/github.com\/huggingface\/transformers)\n\n\n## Dataset Details\n\n### Used Datasets\n- CollectiveCognition\/chats-data-2023-09-27\n\n### Prompt Template\n- Llama Prompt Template\n\n\n\n## Limitations & Biases:\n\nLlama2 and fine-tuned variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2 and any fine-tuned varient's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2 variants, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at https:\/\/ai.meta.com\/llama\/responsible-use-guide\/\n\n## License Disclaimer:\nThis model is bound by the license & usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.\n\n## Contact Us\n\n- [Minds And Company](https:\/\/mnc.ai\/)\n\n## Citiation:\n\nPlease kindly cite using the following BibTeX:\n\n```bibtex\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```\n@misc{Orca-best,\n  title = {Orca-best: A filtered version of orca gpt4 dataset.},\n  author = {Shahul Es},\n  year = {2023},\n  publisher = {HuggingFace},\n  journal = {HuggingFace repository},\n  howpublished = {\\url{https:\/\/huggingface.co\/datasets\/shahules786\/orca-best\/},\n}\n```\n\n```\n@software{touvron2023llama2,\n  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n  author={Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\n Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith,\nRanjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu , Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom},\n  year={2023}\n}\n```\n\n> Readme format: [Riiid\/sheep-duck-llama-2-70b-v1.1](https:\/\/huggingface.co\/Riiid\/sheep-duck-llama-2-70b-v1.1)",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/mncai\/Mistral-7B-CollectiveCognition",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"Mistral-7B-CollectiveCognition",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/mncai\/Mistral-7B-CollectiveCognition",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4934923649,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-Pure-RP",
                    "bunnycore\/Llama-3.2-3B-Long-Think",
                    "bunnycore\/Llama-3.2-3B-Prodigy",
                    "bunnycore\/Llama-3.2-3B-Mix-Skill",
                    "bunnycore\/Llama-3.2-3B-Booval",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "djuna-test-lab\/TEST-L3.2-ReWish-3B",
                    "bunnycore\/Llama-3.2-3B-CodeReactor",
                    "bunnycore\/Llama-3.2-3B-Sci-Think",
                    "bunnycore\/Llama-3.2-3B-All-Mix"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.538836807,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5914433897,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5917414427,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5925212801,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6237121224,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6400130689,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6271545887,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6046384275,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Apex\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Apex\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2403.19522"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.616710633,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5854815841,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6267724931,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Mix-Skill\n- bunnycore\/Llama-3.2-3B-Prodigy\n- bunnycore\/Llama-3.2-3B-Sci-Think\n- bunnycore\/Llama-3.2-3B-All-Mix\n- bunnycore\/Llama-3.2-3B-Booval\n- bunnycore\/Llama-3.2-3B-Long-Think\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- djuna-test-lab\/TEST-L3.2-ReWish-3B\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-CodeReactor\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [Model Stock](https:\/\/arxiv.org\/abs\/2403.19522) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Mix-Skill](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill)\n* [bunnycore\/Llama-3.2-3B-Prodigy](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy)\n* [bunnycore\/Llama-3.2-3B-Sci-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think)\n* [bunnycore\/Llama-3.2-3B-All-Mix](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix)\n* [bunnycore\/Llama-3.2-3B-Booval](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval)\n* [bunnycore\/Llama-3.2-3B-Long-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think)\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [djuna-test-lab\/TEST-L3.2-ReWish-3B](https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B)\n* [bunnycore\/Llama-3.2-3B-CodeReactor](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: djuna-test-lab\/TEST-L3.2-ReWish-3B\n  - model: bunnycore\/Llama-3.2-3B-All-Mix\n  - model: bunnycore\/Llama-3.2-3B-Mix-Skill\n  - model: bunnycore\/Llama-3.2-3B-Booval\n  - model: bunnycore\/Llama-3.2-3B-Sci-Think\n  - model: bunnycore\/Llama-3.2-3B-CodeReactor\n  - model: bunnycore\/Llama-3.2-3B-Long-Think\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n  - model: bunnycore\/Llama-3.2-3B-Prodigy\nmerge_method: model_stock\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6475779414,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6482552886,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5734918416,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Apex",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5849795938,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5540062785,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5753189027,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6104165018,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5899616182,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-31T09:43:32+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-31T09:47:21+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-31T09:43:32+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Apex\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5642807186,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "arxiv:2403.19522",
                    "base_model:bunnycore\/Llama-3.2-3B-All-Mix",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-All-Mix",
                    "base_model:bunnycore\/Llama-3.2-3B-Booval",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Booval",
                    "base_model:bunnycore\/Llama-3.2-3B-CodeReactor",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-CodeReactor",
                    "base_model:bunnycore\/Llama-3.2-3B-Long-Think",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Long-Think",
                    "base_model:bunnycore\/Llama-3.2-3B-Mix-Skill",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Mix-Skill",
                    "base_model:bunnycore\/Llama-3.2-3B-Prodigy",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Prodigy",
                    "base_model:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:bunnycore\/Llama-3.2-3B-Sci-Think",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Sci-Think",
                    "base_model:djuna-test-lab\/TEST-L3.2-ReWish-3B",
                    "base_model:merge:djuna-test-lab\/TEST-L3.2-ReWish-3B",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5986407995,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6516102552,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Mix-Skill\n- bunnycore\/Llama-3.2-3B-Prodigy\n- bunnycore\/Llama-3.2-3B-Sci-Think\n- bunnycore\/Llama-3.2-3B-All-Mix\n- bunnycore\/Llama-3.2-3B-Booval\n- bunnycore\/Llama-3.2-3B-Long-Think\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- djuna-test-lab\/TEST-L3.2-ReWish-3B\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-CodeReactor\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [Model Stock](https:\/\/arxiv.org\/abs\/2403.19522) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Mix-Skill](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill)\n* [bunnycore\/Llama-3.2-3B-Prodigy](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy)\n* [bunnycore\/Llama-3.2-3B-Sci-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think)\n* [bunnycore\/Llama-3.2-3B-All-Mix](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix)\n* [bunnycore\/Llama-3.2-3B-Booval](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval)\n* [bunnycore\/Llama-3.2-3B-Long-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think)\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [djuna-test-lab\/TEST-L3.2-ReWish-3B](https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B)\n* [bunnycore\/Llama-3.2-3B-CodeReactor](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: djuna-test-lab\/TEST-L3.2-ReWish-3B\n  - model: bunnycore\/Llama-3.2-3B-All-Mix\n  - model: bunnycore\/Llama-3.2-3B-Mix-Skill\n  - model: bunnycore\/Llama-3.2-3B-Booval\n  - model: bunnycore\/Llama-3.2-3B-Sci-Think\n  - model: bunnycore\/Llama-3.2-3B-CodeReactor\n  - model: bunnycore\/Llama-3.2-3B-Long-Think\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n  - model: bunnycore\/Llama-3.2-3B-Prodigy\nmerge_method: model_stock\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Apex",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Apex",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Apex",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4013942182,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4739878029,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4636817873,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "any to any"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model Category is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4741852582,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4783774316,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"TOMFORD79",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4704208076,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4982469678,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4743871242,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4667546898,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/TOMFORD79\/Platinum_3\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/TOMFORD79\/Platinum_3\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5469902307,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported by the mentioned entities are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4789194167,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5044724941,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: mit\ntags:\n- any-to-any\n- omega\n- omegalabs\n- bittensor\n- agi\n---\n\nThis is an Any-to-Any model checkpoint for the OMEGA Labs x Bittensor Any-to-Any subnet.\n\nCheck out the [git repo](https:\/\/github.com\/omegalabsinc\/omegalabs-anytoany-bittensor) and find OMEGA on X: [@omegalabsai](https:\/\/x.com\/omegalabsai).\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5244957209,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5119088292,
                "extraction_time":"2025-04-16_22-14-31"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4655567557,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/TOMFORD79\/Platinum_3",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4772336185,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4554946721,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4437268376,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5044181794,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5074303448,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2025-03-04T17:31:42+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2025-03-04T17:41:59+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2025-03-04T17:31:42+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/TOMFORD79\/Platinum_3\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4437292963,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "any-to-any",
                    "omega",
                    "omegalabs",
                    "bittensor",
                    "agi",
                    "license:mit",
                    "region:us"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4777479917,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5256509036,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlicense: mit\ntags:\n- any-to-any\n- omega\n- omegalabs\n- bittensor\n- agi\n---\n\nThis is an Any-to-Any model checkpoint for the OMEGA Labs x Bittensor Any-to-Any subnet.\n\nCheck out the [git repo](https:\/\/github.com\/omegalabsinc\/omegalabs-anytoany-bittensor) and find OMEGA on X: [@omegalabsai](https:\/\/x.com\/omegalabsai).\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/TOMFORD79\/Platinum_3",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"Platinum_3",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/TOMFORD79\/Platinum_3",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3531323373,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.41048491,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the text is to inform users that they have been rate limited and to provide them with an email address to send if they believe it is an error. The purpose of this information is to enable users to make a decision as to the suitability of the creative work for their intended use.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4468340874,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3897320926,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.381206125,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"StepLaw",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4244194031,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4239113033,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.435690701,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4192566276,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/StepLaw\/StepLaw-N_429M-D_19.0B-LR2.762e-03-BS1048576\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/StepLaw\/StepLaw-N_429M-D_19.0B-LR2.762e-03-BS1048576\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The distribution in the given text is a media object that encodes this creative work.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4972515404,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4578971863,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4215859175,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"<!DOCTYPE html>\n<html class=\"\" lang=\"en\">\n<head>\n    <meta charset=\"utf-8\" \/>\n    <meta\n            name=\"viewport\"\n            content=\"width=device-width, initial-scale=1.0, user-scalable=no\"\n    \/>\n    <meta\n            name=\"description\"\n            content=\"We're on a journey to advance and democratize artificial intelligence through open source and open science.\"\n    \/>\n    <meta property=\"fb:app_id\" content=\"1321688464574422\" \/>\n    <meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n    <meta name=\"twitter:site\" content=\"@huggingface\" \/>\n    <meta\n            property=\"og:title\"\n            content=\"Hugging Face - The AI community building the future.\"\n    \/>\n    <meta property=\"og:type\" content=\"website\" \/>\n\n    <title>Hugging Face - The AI community building the future.<\/title>\n    <style>\n        body {\n            margin: 0;\n        }\n\n        main {\n            background-color: white;\n            min-height: 100vh;\n            padding: 7rem 1rem 8rem 1rem;\n            text-align: center;\n            font-family: Source Sans Pro, ui-sans-serif, system-ui, -apple-system,\n            BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans,\n            sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol,\n            Noto Color Emoji;\n        }\n\n        img {\n            width: 6rem;\n            height: 6rem;\n            margin: 0 auto 1rem;\n        }\n\n        h1 {\n            font-size: 3.75rem;\n            line-height: 1;\n            color: rgba(31, 41, 55, 1);\n            font-weight: 700;\n            box-sizing: border-box;\n            margin: 0 auto;\n        }\n\n        p, a {\n            color: rgba(107, 114, 128, 1);\n            font-size: 1.125rem;\n            line-height: 1.75rem;\n            max-width: 28rem;\n            box-sizing: border-box;\n            margin: 0 auto;\n        }\n\n        .dark main {\n            background-color: rgb(11, 15, 25);\n        }\n        .dark h1 {\n            color: rgb(209, 213, 219);\n        }\n        .dark p, .dark a {\n            color: rgb(156, 163, 175);\n        }\n    <\/style>\n    <script>\n        \/\/ On page load or when changing themes, best to add inline in `head` to avoid FOUC\n        const key = \"_tb_global_settings\";\n        let theme = window.matchMedia(\"(prefers-color-scheme: dark)\").matches\n            ? \"dark\"\n            : \"light\";\n        try {\n            const storageTheme = JSON.parse(window.localStorage.getItem(key)).theme;\n            if (storageTheme) {\n                theme = storageTheme === \"dark\" ? \"dark\" : \"light\";\n            }\n        } catch (e) {}\n        if (theme === \"dark\") {\n            document.documentElement.classList.add(\"dark\");\n        } else {\n            document.documentElement.classList.remove(\"dark\");\n        }\n    <\/script>\n<\/head>\n\n<body>\n<main>\n    <img\n            src=\"https:\/\/cdn-media.huggingface.co\/assets\/huggingface_logo.svg\"\n            alt=\"\"\n    \/>\n    <div>\n        <h1>429<\/h1>\n        <p>We had to rate limit you. If you think it's an error, send us <a href=\"mailto:website@huggingface.co\">an email<\/a><\/p>\n    <\/div>\n<\/main>\n<\/body>\n<\/html>",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4610694349,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4268815815,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4027913511,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/StepLaw\/StepLaw-N_429M-D_19.0B-LR2.762e-03-BS1048576",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not explicitly mentioned in the provided HTML code.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4993157089,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4227408767,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3865485787,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4405804276,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4250630438,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2025-04-10T14:54:33+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2025-04-15T15:34:50+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2025-04-10T14:54:33+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/StepLaw\/StepLaw-N_429M-D_19.0B-LR2.762e-03-BS1048576\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4146262109,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "step1",
                    "text-generation",
                    "StepLaw",
                    "causal-lm",
                    "en",
                    "license:apache-2.0",
                    "autotrain_compatible",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4185670912,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4602920115,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:description":[
            {
                "data":"<!DOCTYPE html>\n<html class=\"\" lang=\"en\">\n<head>\n    <meta charset=\"utf-8\" \/>\n    <meta\n            name=\"viewport\"\n            content=\"width=device-width, initial-scale=1.0, user-scalable=no\"\n    \/>\n    <meta\n            name=\"description\"\n            content=\"We're on a journey to advance and democratize artificial intelligence through open source and open science.\"\n    \/>\n    <meta property=\"fb:app_id\" content=\"1321688464574422\" \/>\n    <meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n    <meta name=\"twitter:site\" content=\"@huggingface\" \/>\n    <meta\n            property=\"og:title\"\n            content=\"Hugging Face - The AI community building the future.\"\n    \/>\n    <meta property=\"og:type\" content=\"website\" \/>\n\n    <title>Hugging Face - The AI community building the future.<\/title>\n    <style>\n        body {\n            margin: 0;\n        }\n\n        main {\n            background-color: white;\n            min-height: 100vh;\n            padding: 7rem 1rem 8rem 1rem;\n            text-align: center;\n            font-family: Source Sans Pro, ui-sans-serif, system-ui, -apple-system,\n            BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans,\n            sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol,\n            Noto Color Emoji;\n        }\n\n        img {\n            width: 6rem;\n            height: 6rem;\n            margin: 0 auto 1rem;\n        }\n\n        h1 {\n            font-size: 3.75rem;\n            line-height: 1;\n            color: rgba(31, 41, 55, 1);\n            font-weight: 700;\n            box-sizing: border-box;\n            margin: 0 auto;\n        }\n\n        p, a {\n            color: rgba(107, 114, 128, 1);\n            font-size: 1.125rem;\n            line-height: 1.75rem;\n            max-width: 28rem;\n            box-sizing: border-box;\n            margin: 0 auto;\n        }\n\n        .dark main {\n            background-color: rgb(11, 15, 25);\n        }\n        .dark h1 {\n            color: rgb(209, 213, 219);\n        }\n        .dark p, .dark a {\n            color: rgb(156, 163, 175);\n        }\n    <\/style>\n    <script>\n        \/\/ On page load or when changing themes, best to add inline in `head` to avoid FOUC\n        const key = \"_tb_global_settings\";\n        let theme = window.matchMedia(\"(prefers-color-scheme: dark)\").matches\n            ? \"dark\"\n            : \"light\";\n        try {\n            const storageTheme = JSON.parse(window.localStorage.getItem(key)).theme;\n            if (storageTheme) {\n                theme = storageTheme === \"dark\" ? \"dark\" : \"light\";\n            }\n        } catch (e) {}\n        if (theme === \"dark\") {\n            document.documentElement.classList.add(\"dark\");\n        } else {\n            document.documentElement.classList.remove(\"dark\");\n        }\n    <\/script>\n<\/head>\n\n<body>\n<main>\n    <img\n            src=\"https:\/\/cdn-media.huggingface.co\/assets\/huggingface_logo.svg\"\n            alt=\"\"\n    \/>\n    <div>\n        <h1>429<\/h1>\n        <p>We had to rate limit you. If you think it's an error, send us <a href=\"mailto:website@huggingface.co\">an email<\/a><\/p>\n    <\/div>\n<\/main>\n<\/body>\n<\/html>",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/StepLaw\/StepLaw-N_429M-D_19.0B-LR2.762e-03-BS1048576",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"StepLaw-N_429M-D_19.0B-LR2.762e-03-BS1048576",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/StepLaw\/StepLaw-N_429M-D_19.0B-LR2.762e-03-BS1048576",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3848507255,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "voxpopuli"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "microsoft\/speecht5_tts"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.471603781,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended uses & limitations section provides information on the intended uses and limitations of the custom_speecht5_finetuned_voxpopuli_nl model.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4379123151,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text to audio"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4712661803,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4697419852,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"zendeer",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "voxpopuli"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "voxpopuli"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4749259204,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5321844816,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4836868048,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4604780376,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/zendeer\/custom_speecht5_finetuned_voxpopuli_nl\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/zendeer\/custom_speecht5_finetuned_voxpopuli_nl\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4932467192,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5153659582,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5105654001,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: mit\nbase_model: microsoft\/speecht5_tts\ntags:\n- generated_from_trainer\ndatasets:\n- voxpopuli\nmodel-index:\n- name: custom_speecht5_finetuned_voxpopuli_nl\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# custom_speecht5_finetuned_voxpopuli_nl\n\nThis model is a fine-tuned version of [microsoft\/speecht5_tts](https:\/\/huggingface.co\/microsoft\/speecht5_tts) on the voxpopuli dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 4\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- training_steps: 4000\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.41.2\n- Pytorch 2.3.0+cu121\n- Datasets 2.20.0\n- Tokenizers 0.19.1\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4959870428,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"The required software dependencies for the custom_speecht5_finetuned_voxpopuli_nl training procedure are:\n\n- Transformers 4.41.2\n- Pytorch 2.3.0+cu121\n- Datasets 2.20.0\n- Tokenizers 0.19.1",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4747215807,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4335947931,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/zendeer\/custom_speecht5_finetuned_voxpopuli_nl",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4552763849,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4151860625,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3869182318,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4473576695,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4463301301,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-06-18T17:41:38+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-06-18T17:42:53+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-06-18T17:41:38+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/zendeer\/custom_speecht5_finetuned_voxpopuli_nl\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4223073572,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "tensorboard",
                    "safetensors",
                    "speecht5",
                    "text-to-audio",
                    "generated_from_trainer",
                    "dataset:voxpopuli",
                    "base_model:microsoft\/speecht5_tts",
                    "base_model:finetune:microsoft\/speecht5_tts",
                    "license:mit",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "tensorboard",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4297067821,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4830869138,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlicense: mit\nbase_model: microsoft\/speecht5_tts\ntags:\n- generated_from_trainer\ndatasets:\n- voxpopuli\nmodel-index:\n- name: custom_speecht5_finetuned_voxpopuli_nl\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# custom_speecht5_finetuned_voxpopuli_nl\n\nThis model is a fine-tuned version of [microsoft\/speecht5_tts](https:\/\/huggingface.co\/microsoft\/speecht5_tts) on the voxpopuli dataset.\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 4\n- eval_batch_size: 2\n- seed: 42\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_steps: 500\n- training_steps: 4000\n- mixed_precision_training: Native AMP\n\n### Framework versions\n\n- Transformers 4.41.2\n- Pytorch 2.3.0+cu121\n- Datasets 2.20.0\n- Tokenizers 0.19.1\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/zendeer\/custom_speecht5_finetuned_voxpopuli_nl",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"custom_speecht5_finetuned_voxpopuli_nl",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/zendeer\/custom_speecht5_finetuned_voxpopuli_nl",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7200186849,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6682118177,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of Project Indus LLM is for question and answer and conversation in Hindi and Dialects. The model would be reward tuned to be used across various industries.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.8075675964,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The context provided does not contain information about the model category.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6725843549,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7543638051,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"nickmalhotra",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"To begin using Project Indus LLM for your projects, follow these steps to set up and run the model:\n\n1. Preprocessing: Extensive preprocessing is required to clean and standardize the text data. This involves removing irrelevant information, handling missing values, and converting text into a format suitable for the model.\n\n2. Supervised Learning: Once the text data is preprocessed, supervised learning can be applied to train the model. This involves feeding the preprocessed data into the model and adjusting its parameters to minimize the error between the predicted and actual outputs.\n\n3. High-Performance Computing Setup: To ensure efficient training and inference, a high-performance computing setup is necessary. This may involve using specialized hardware, such as GPUs, and optimizing the model's architecture and training algorithm.\n\n4. Model Deployment: After training, the model can be deployed for use in various applications. This may involve integrating the model into a web application, mobile app, or other software system.\n\n5. Evaluation and Monitoring: It is important to evaluate the model's performance and monitor its behavior over time. This may involve using metrics such as accuracy, precision, and recall, as well as analyzing the model's outputs and identifying any potential issues or areas for improvement.\n\nOverall, the usage instructions for Project Indus LLM involve a combination of preprocessing, supervised learning, high-performance computing, model deployment, and evaluation and monitoring.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7540212274,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7677930593,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7333391309,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7228713036,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The context provided does not contain information about the distribution of the media object that encodes the creative work.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7662982643,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6954072416,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7653562427,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: osl-3.0\nmodel-index:\n- name: indus_1.175B\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: AI2 Reasoning Challenge (25-Shot)\n      type: ai2_arc\n      config: ARC-Challenge\n      split: test\n      args:\n        num_few_shot: 25\n    metrics:\n    - type: acc_norm\n      value: 22.7\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/ProjectIndus\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: HellaSwag (10-Shot)\n      type: hellaswag\n      split: validation\n      args:\n        num_few_shot: 10\n    metrics:\n    - type: acc_norm\n      value: 25.04\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU (5-Shot)\n      type: cais\/mmlu\n      config: all\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.12\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: TruthfulQA (0-shot)\n      type: truthful_qa\n      config: multiple_choice\n      split: validation\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: mc2\n      value: 0.0\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: Winogrande (5-shot)\n      type: winogrande\n      config: winogrande_xl\n      split: validation\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 49.57\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GSM8k (5-shot)\n      type: gsm8k\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 0.0\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\nwidget:\n  - example_title: \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092a\u094d\u0930\u0927\u093e\u0928\u092e\u0902\u0924\u094d\u0930\u0940\n    messages:\n      - role: user\n        content: >-\n          \u092d\u093e\u0930\u0924 \u0915\u0947 \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092a\u094d\u0930\u0927\u093e\u0928\u092e\u0902\u0924\u094d\u0930\u0940 \u0915\u094c\u0928 \u0939\u0948\u0902?\n  - example_title: \u0939\u094b\u0932\u0940 \u0915\u093e \u092e\u0939\u0924\u094d\u0935\n    messages:\n      - role: user\n        content: >-\n          \u0939\u094b\u0932\u0940 \u0915\u093e \u092e\u0939\u0924\u094d\u0935 \u0915\u094d\u092f\u093e \u0939\u0948?\n---\n\n# Model Card for Project Indus\n\n<!-- Provide a quick summary of what the model is\/does. -->\nProject Indus LLM is a groundbreaking open-source language model tailored for Hindi and its dialects, designed to enhance natural language processing and generation across diverse Indian linguistic applications.\n\n# Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [Model Details](#model-details)\n  - [Model Description](#model-description)\n- [Uses](#uses)\n  - [Direct Use](#direct-use)\n  - [Downstream Use](#downstream-use)\n  - [Out-of-Scope Use](#out-of-scope-use)\n- [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n  - [Recommendations](#recommendations)\n- [Training Details](#training-details)\n  - [Training Data](#training-data)\n  - [Training Procedure](#training-procedure)\n    - [Preprocessing](#preprocessing)\n- [Evaluation](#evaluation)\n  - [Testing Data, Factors & Metrics](#testing-data-factors--metrics)\n    - [Testing Data](#testing-data)\n    - [Factors](#factors)\n    - [Metrics](#metrics)\n  - [Results](#results)\n- [Model Examination](#model-examination)\n- [Technical Specifications](#technical-specifications)\n  - [Model Architecture and Objective](#model-architecture-and-objective)\n  - [Compute Infrastructure](#compute-infrastructure)\n    - [Hardware](#hardware)\n    - [Software](#software)\n- [Citation](#citation)\n- [Glossary](#glossary)\n- [More Information](#more-information)\n- [Model Card Authors](#model-card-authors)\n- [Model Card Contact](#model-card-contact)\n- [How to Get Started with the Model](#how-to-get-started-with-the-model)\n\n# Model Details\n\n## Model Description\n\nProject Indus LLM aims to provide a robust language model for Indian languages, starting with Hindi and its dialects. This open-source foundational model, hosted on Hugging Face, is tailored for easy integration and further development by researchers and developers focusing on Indian linguistic diversity.\n\n<!-- Provide a longer summary of what this model is\/does. -->\nThe model is a pretrained model in Hindi and dialects which is instruct tuned.\n\n- **Developed by:** Nikhil Malhotra, Nilesh Brahme, Satish Mishra, Vinay Sharma (Makers Lab, TechMahindra)\n- **Model type:** Foundational Language model\n- **Language(s) (NLP):** hin, bho, mai, doi\n- **License:** other\n- **Parent Model:** It is a grounds up model built on GPT-2 architecture starting from tokenizer to decoder\n- **Resources for more information:** <https:\/\/www.techmahindra.com\/en-in\/innovation\/the-indus-project\/>\n  \n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\nUses include question and answeting and conversation in Hindi and Dialects. The model would be reward tuned to be used across various industries\n\n1. Call center\n2. Healthcare\n3. Automotive\n4. Telecom\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem\/app. -->\n<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say \"more info needed.\" -->\nProject Indus can be directly used for generating text, simulating conversation, and other text generation tasks without additional training.\n\n## Downstream Use\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem\/app -->\n<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say \"more info needed.\" -->\n\nUses include question and answeting and conversation in Hindi and Dialects. The model would be reward tuned to be used across various industries\n\n1. Call center\n2. Healthcare\n3. Automotive\n4. Telecom\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say \"more info needed.\" -->\nProject Indus is not designed for high-stakes decision-making tasks such as medical diagnosis or legal advice, nor can it be used for fill-in-the-blank exercises, multiple Q&A, and similar applications at the moment.\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nSignificant research has explored bias and fairness issues with language models\n(see, e.g., [Sheng et al. (2021)](https:\/\/aclanthology.org\/2021.acl-long.330.pdf) and [Bender et al. (2021)](https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/3442188.3445922)).\nPredictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\nWe have taken care across various biases by trying to remove them from training data. However since the model is a generative model, it would tend to produce hallucinations.\nAny disturbing or harmful sterotype produced by the model is purely un-intentional and coincidental.\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nIt is recommended to avoid biases and negative connotations in the model, and regular updates along with community feedback are crucial for addressing any emergent bias or misuse scenarios.\n\n# Training Details\n\nThe model was trained on a curated dataset comprising various sources of Hindi text, including literature, news articles, and web content.\n\n## Infrastructure\n\n- **Training Infrastructure:** Utilized high-performance computing resources provided by CDAC, featuring NVIDIA A100 GPUs.\n- **Running Infrastructure:** Tested for both GPU (NVIDIA GeForce RTX 3070 or higher) and CPU (Intel Xeon Platinum 8580) environments.\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\nThe Project Indus LLM was trained on a diverse and extensive dataset comprising various sources of Hindi text and its dialects. The data collection and curation process was meticulously designed to cater to the linguistic diversity and complexity of Indian languages, particularly focusing on Hindi and its 37 dialects.\n\n### Data Sources and Collection\n\nData was collected in three main buckets:\n\n1. **Open-Source Hindi Data**: This included publicly available sources from the internet across different categories such as news, and non-news. Automated scripts were used to scrape and extract text from web pages. Here are some of the sources:\n   - **News**: Articles from news portals.\n   - **Non-News**: Diverse sources including Wikipedia, commoncrawl.org, and other culturally significant content like 'Man ki Baat' from AIR.\n\n2. **Translated Data**: A portion of the Pile dataset, which is a large English dataset used for training AI models, was translated into Hindi using three different translation models. IndicTrans2 (AI4Bharat) was selected as the best model for this purpose based on its accuracy and efficiency.\n\n3. **Dialects**: Data collection for dialects presented a unique challenge due to the limited material available on the internet. Data for major dialects like Maithili, Bhojpuri, Magahi, and Braj Bhasha was collected from multiple sources, including fieldwork where representatives collected old books and other texts, which were then digitized and converted into text data.\n\n## Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\n\n- **Pre-training:** Conducted on a dataset of 22 billion tokens using advanced tokenization techniques.\n- **Fine-Tuning:** Supervised fine-tuning performed with a focus on Indian languages, utilizing datasets specifically tailored for cultural, political, and social contexts.\n\nBelow is a table summarizing the datasets used for pre-training and fine-tuning the model:\n\n| Phase         | Data Source                             | Tokens    | Notes                                               |\n|---------------|-----------------------------------------|-----------|-----------------------------------------------------|\n| Pre-training  | Cleaned dataset of Hindi and dialects   | 22 billion| Utilized advanced tokenization                      |\n| Fine-tuning   | Custom datasets tailored for Indian languages | Varied    | Focus on cultural, political, and social contexts   |\n\n- **Training Infrastructure:** Utilized high-performance computing resources provided by CDAC, featuring NVIDIA A100 GPUs.\n- **Running Infrastructure:** Tested for both GPU (NVIDIA GeForce RTX 3070 or higher) and CPU (Intel Xeon Platinum 8580) environments.\n\n### Preprocessing\n\nThe collected data underwent several stages of cleaning and preprocessing to ensure high quality and usability for training:\n\n- **Cleaning**: The data was cleaned of unwanted text, characters, and personal information like mobile numbers. Transliteration was performed where necessary, and unwanted tags from scraped web pages were removed.\n- **Bias Removal**: A Bias Removal Toolkit was developed to detect and remove biased language from the training data. This toolkit helped in ensuring that the text used for training the model was ethical, correct, and socially responsible.\n- **Tokenization**: The data was tokenized using a custom tokenizer developed specifically for Hindi and its dialects. This tokenizer was based on Byte Pair Encoding (BPE) with additional mechanisms like byte fallback to handle the peculiarities of Hindi script efficiently.\n\n#### Summary\n\nThe final dataset used for training consisted of:\n\n- **Raw Data Size**: Over 500 GB of raw data collected.\n- **Cleaned and Curated Data**: Approximately 200 GB of clean Hindi and dialect text data.\n- **Tokenization**: Utilized 22 billion tokens created from the cleaned data for pre-training.\n\nThis diverse and extensive training data foundation allowed Project Indus LLM to develop robust capabilities for understanding and generating Hindi text, making it a powerful tool for applications requiring Indian language processing.\n\n# Evaluation\n\n### Indic LLM Leaderboard Results\n\nProject Indus LLM has been evaluated using the Indic LLM Leaderboard, which employs the `indic_eval` evaluation framework specifically designed for assessing models on Indian language tasks. This framework provides a comprehensive view of model performance across a variety of benchmarks tailored to Indian languages.\n\nDetailed results from the Indic LLM Leaderboard (\u03b1), accessible at [Hugging Face Indic LLM Leaderboard](https:\/\/huggingface.co\/spaces\/Cognitive-Lab\/indic_llm_leaderboard), are shown below:\n\n|              Task              | Version |  Metric  | Value |   | Stderr |\n|--------------------------------|---------|----------|-------|---|--------|\n| All                            |         | acc      | 0.2891| \u00b1 | 0.0109 |\n|                                |         | acc_norm | 0.3013| \u00b1 | 0.0112 |\n| indiceval:ARC-Challenge:hindi:10 |    0    | acc      | 0.2167| \u00b1 | 0.0120 |\n|                                |         | acc_norm | 0.2474| \u00b1 | 0.0126 |\n| indiceval:ARC-Easy:hindi:5     |    0    | acc      | 0.3615| \u00b1 | 0.0099 |\n|                                |         | acc_norm | 0.3552| \u00b1 | 0.0098 |\n\nThese results highlight the model's capabilities in understanding and generating Hindi language text under controlled testing conditions. The standard error values indicate the variance observed during the evaluation, providing insights into the consistency of the model's performance across different evaluation runs.\n\n### Open LLM Leaderboard Evaluation Results\n\nAdditionally, Project Indus LLM has been evaluated on the Open LLM Leaderboard, which provides another layer of benchmarking by comparing the model's performance against other state-of-the-art language models. Below are the summarized results from the Open LLM Leaderboard:\n\n|             Metric              |Value|\n|---------------------------------|----:|\n|Avg.                             |20.07|\n|AI2 Reasoning Challenge (25-Shot)|22.70|\n|HellaSwag (10-Shot)              |25.04|\n|MMLU (5-Shot)                    |23.12|\n|Winogrande (5-shot)              |49.57|\n\nThese benchmark results can be explored further on [Hugging Face Open LLM Leaderboard](https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard).\n\n### Evaluation Context\n\nThe evaluation metrics `acc` (accuracy) and `acc_norm` (normalized accuracy) are used to quantify the model's performance. The tasks are differentiated by their difficulty and the specific dataset used, such as the ARC Challenge and ARC Easy sets, both adapted to Hindi language conditions to ensure relevant assessment. This structured evaluation ensures that the Indus LLM not only performs well in generalized text generation tasks but also in more specialized, context-specific scenarios pertinent to the Indian linguistic framework.\n\n## Results\n\nProject Indus demonstrates competitive performance, particularly in text generation tasks, as evidenced by its scores on standardized benchmarks.\n\n# Technical Specifications\n\n## Model Architecture and Objective\n\nProject Indus LLM is based on a GPT-2.0-like architecture, tailored to handle the complexities of the Hindi language and its dialects. This model was designed to serve as a foundational model that can be fine-tuned for various applications, making it highly versatile and adaptable to different domains within the Indian context.\n\n- **Architecture Details**:\n  - **Layers**: 22 transformer layers, which provide a deep neural network capable of understanding complex language patterns.\n  - **Heads**: 32 attention heads per layer, facilitating a broad attention mechanism across different parts of the input data.\n  - **Embedding Size**: 2048, which allows the model to represent a wide variety of information and nuances in the data.\n  - **Vocabulary Size**: 32,300, tailored to include a comprehensive set of Hindi words and common phrases found in the training data.\n\nThe objective of this model is to provide a robust tool for text generation and understanding in Hindi and its dialects, supporting the development of applications that require natural language processing in these languages. It also aims to bridge the gap in technology where Indian languages are underrepresented, providing a platform for further linguistic research and technological inclusion.\n\n## Compute Infrastructure\n\n##### Hardware\n\nThe pre-training and fine-tuning of Project Indus LLM were conducted on high-performance computing infrastructure provided by the Centre for Development of Advanced Computing (CDAC). This setup included:\n\n- **Nodes and GPUs**: Utilization of six nodes, each equipped with eight NVIDIA A100 GPUs. These GPUs are state-of-the-art for machine learning tasks and provide the necessary computational power to handle the large volumes of data and complex model architectures.\n- **Memory and Storage**: Each node was equipped with ample memory and storage to handle the datasets and model parameters efficiently. Specific configurations included 40 GB of GPU memory per card, essential for training large models.\n\n\nInference performance was tested on GPU as well as CPU.\n- **GPU**: On GPU NVIDIA GeForce RTX 3070  we have seen for 250-350 tokens inference time around ~5-10s.\n- **CPU**: On Intel CPU Xeon(R) Platinum 8580 we have seen performance comparable to GPU with throughput of > 30 token\/second.\n\n##### Software\n\nThe software environment was crucial for efficiently training and running the model. Key components included:\n\n- **Operating System**: Linux, chosen for its stability and support for high-performance computing tasks.\n- **Machine Learning Frameworks**: PyTorch, used for its flexibility and efficiency in training deep learning models. It supports extensive parallel processing and GPU acceleration, which are critical for training large models like Project Indus LLM.\n- **Job Scheduler**: SLURM (Simple Linux Utility for Resource Management) was used to manage and allocate resources effectively across the distributed system. This ensured optimal scheduling of training jobs without resource contention.\n\n# Citation\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\nThe detailed citation information will help in acknowledging the work and efforts of the team behind Project Indus LLM when it is used or referenced in academic or professional settings.\n\n```bibtex\n@article{malhotra2024projectindus,\n  title={Project Indus: A Foundational Model for Indian Languages},\n  author={Malhotra, Nikhil and Brahme, Nilesh and Mishra, Satish and Sharma, Vinay},\n  journal={Tech Mahindra Makers Lab},\n  year={2024},\n  url={https:\/\/www.techmahindra.com\/en-in\/innovation\/the-indus-project\/}\n}\n```\n\n**APA:**\nMalhotra, N., Brahme, N., Mishra, S., & Sharma, V. (2024). Project Indus: A Foundational Model for Indian Languages. *Tech Mahindra Makers Lab*. Available at <https:\/\/www.techmahindra.com\/en-in\/innovation\/the-indus-project\/>\n\n# Glossary\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\nThis glossary section explains key terms used throughout the model documentation and technical details, helping users unfamiliar with certain concepts to better understand the content.\n\n- **Transformer Layers**: Part of a neural network architecture that uses self-attention mechanisms to process sequential data such as text. Essential for NLP tasks.\n- **Attention Heads**: Sub-units of a model layer that allow the model to focus on different parts of the input sequence when making predictions.\n- **Embedding Size**: The size of the vector used to represent each token or word in a dense numerical form. Larger embeddings can capture more detailed information.\n- **Block Size**: The maximum length of the input tokens the model can process in one operation.\n- **Vocabulary Size**: The total number of unique words or tokens that the model can understand and generate.\n\n# More Information\n\nFor further details on Project Indus LLM, including additional documentation, tutorials, and community discussions, visit the following resources:\n\n- **Project Repository**: [Hugging Face Repository](https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus)\n- **Tech Mahindra Makers Lab**: Insights into the research and development behind Project Indus can be found on the [Tech Mahindra Innovation page](https:\/\/www.techmahindra.com\/en-in\/innovation\/makers-lab\/).\n- **Community Forums**: Engage with the community on [Hugging Face Forums](https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus\/discussions?status=open&type=discussion) for support, brainstorming, and sharing of new ideas related to Project Indus.\n\n# Model Card Authors\n\n<!-- This section provides another layer of transparency and accountability. Whose views is this model card representing? How many voices were included in its construction? Etc. -->\n\nThe model card and documentation for Project Indus LLM were collaboratively authored by:\n\n- **Nikhil Malhotra**: Chief Innovation Officer at Tech Mahindra.\n- **Nilesh Brahme**: Senior AI Research Scientist and one of the primary contributors to the Project Indus development.\n- **Satish Mishra**: AI Architect, whose insights have significantly shaped the model's capabilities.\n- **Vinay Sharma**: LLM Engineer focused on the linguistic data processing and model training aspects of Project Indus.\n\n# Model Card Contact\n\nFor inquiries, support, or further information regarding Project Indus LLM, please reach out through the following channels:\n\n- **Email**: [projectindus@techmahindra.com](mailto:projectindus@techmahindra.com) - For direct queries and professional engagements.\n- **GitHub Issues**: For technical issues, feature requests, or contributions, please use the Issues section of the [Project Indus GitHub repository](https:\/\/github.com\/Tech-Mahindra-Makers-Lab\/Indus-1.1B).\n- **Hugging Face Spaces**: Questions and discussions related to model implementation and community projects can be posted in our dedicated space on Hugging Face.\n\n# How to Get Started with the Model\n\nTo begin using Project Indus LLM for your projects, follow these steps to set up and run the model:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"nickmalhotra\/ProjectIndus\")\ntokenizer = AutoTokenizer.from_pretrained(\"nickmalhotra\/ProjectIndus\")\n\n# Example inference\ndef format_template(user_prompt):\n    messages = [\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    response = tokenizer.apply_chat_template(messages, tokenize=True,  add_generation_prompt=True, return_tensors=\"pt\")\n    return  response\n\nuser_prompt = \"\"\"\u092d\u093e\u0930\u0924 \u0915\u0947 \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092a\u094d\u0930\u0927\u093e\u0928\u092e\u0902\u0924\u094d\u0930\u0940 \u0915\u094c\u0928 \u0939\u0948\u0902?\"\"\"\n\ninput_ids = format_template(user_prompt)\n\n# Generate text using the model\noutput = model.generate(input_ids,\n                        eos_token_id=tokenizer.eos_token_id,\n                        pad_token_id=tokenizer.eos_token_id,\n                        max_length=1024,\n                        num_beams=5,\n                        do_sample=True,\n                        early_stopping=True,\n                        temperature=0.7,\n                        top_k=50,\n                        top_p=0.95,\n                        repetition_penalty=1.2,\n                        no_repeat_ngram_size=3,\n                        num_return_sequences=1,\n                        )\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n```\n## Disclaimer\n\n#### Model Limitations\n\nProject Indus LLM is trained with single instruction tuning, which may result in hallucinations\u2014instances where the model generates plausible but inaccurate information. Users should exercise caution, especially in scenarios requiring high factual accuracy.\n\n#### Adaptation for Specific Use Cases\n\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\n#### Recommendations for Fine-Tuning\n\n- **Identify Specific Needs**: Clearly define the requirements of your use case to guide the fine-tuning process.\n- **Curate Targeted Data**: Ensure the training data is relevant and of high quality to improve model performance.\n- **Continuous Evaluation**: Regularly assess the model's performance during and after fine-tuning to maintain accuracy and reduce biases.\n\nThis disclaimer aims to provide users with a clear understanding of the model's capabilities and limitations, facilitating its effective application and development.",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7433389127,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"The software environment was crucial for efficiently training and running the model. Key components included:",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7605938017,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7019693851,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not explicitly mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.767750144,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7857187986,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nTraining Details > Training Procedure - Par. 1: Training Details > Training Procedure:\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\nHow to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases: How to Get Started with the Model > Disclaimer > Adaptation for Specific Use Cases:\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7393344641,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7573561966,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7479945421,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-04-18T10:54:29+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-07-21T09:20:46+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-04-18T10:54:29+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7593722343,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "gpt2",
                    "text-generation",
                    "conversational",
                    "license:osl-3.0",
                    "model-index",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "osl-3.0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7211659849,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7423383296,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlicense: osl-3.0\nmodel-index:\n- name: indus_1.175B\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: AI2 Reasoning Challenge (25-Shot)\n      type: ai2_arc\n      config: ARC-Challenge\n      split: test\n      args:\n        num_few_shot: 25\n    metrics:\n    - type: acc_norm\n      value: 22.7\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/ProjectIndus\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: HellaSwag (10-Shot)\n      type: hellaswag\n      split: validation\n      args:\n        num_few_shot: 10\n    metrics:\n    - type: acc_norm\n      value: 25.04\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU (5-Shot)\n      type: cais\/mmlu\n      config: all\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.12\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: TruthfulQA (0-shot)\n      type: truthful_qa\n      config: multiple_choice\n      split: validation\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: mc2\n      value: 0.0\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: Winogrande (5-shot)\n      type: winogrande\n      config: winogrande_xl\n      split: validation\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 49.57\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GSM8k (5-shot)\n      type: gsm8k\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 0.0\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard?query=nickmalhotra\/indus_1.175B\n      name: Open LLM Leaderboard\nwidget:\n  - example_title: \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092a\u094d\u0930\u0927\u093e\u0928\u092e\u0902\u0924\u094d\u0930\u0940\n    messages:\n      - role: user\n        content: >-\n          \u092d\u093e\u0930\u0924 \u0915\u0947 \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092a\u094d\u0930\u0927\u093e\u0928\u092e\u0902\u0924\u094d\u0930\u0940 \u0915\u094c\u0928 \u0939\u0948\u0902?\n  - example_title: \u0939\u094b\u0932\u0940 \u0915\u093e \u092e\u0939\u0924\u094d\u0935\n    messages:\n      - role: user\n        content: >-\n          \u0939\u094b\u0932\u0940 \u0915\u093e \u092e\u0939\u0924\u094d\u0935 \u0915\u094d\u092f\u093e \u0939\u0948?\n---\n\n# Model Card for Project Indus\n\n<!-- Provide a quick summary of what the model is\/does. -->\nProject Indus LLM is a groundbreaking open-source language model tailored for Hindi and its dialects, designed to enhance natural language processing and generation across diverse Indian linguistic applications.\n\n# Table of Contents\n\n- [Table of Contents](#table-of-contents)\n- [Model Details](#model-details)\n  - [Model Description](#model-description)\n- [Uses](#uses)\n  - [Direct Use](#direct-use)\n  - [Downstream Use](#downstream-use)\n  - [Out-of-Scope Use](#out-of-scope-use)\n- [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n  - [Recommendations](#recommendations)\n- [Training Details](#training-details)\n  - [Training Data](#training-data)\n  - [Training Procedure](#training-procedure)\n    - [Preprocessing](#preprocessing)\n- [Evaluation](#evaluation)\n  - [Testing Data, Factors & Metrics](#testing-data-factors--metrics)\n    - [Testing Data](#testing-data)\n    - [Factors](#factors)\n    - [Metrics](#metrics)\n  - [Results](#results)\n- [Model Examination](#model-examination)\n- [Technical Specifications](#technical-specifications)\n  - [Model Architecture and Objective](#model-architecture-and-objective)\n  - [Compute Infrastructure](#compute-infrastructure)\n    - [Hardware](#hardware)\n    - [Software](#software)\n- [Citation](#citation)\n- [Glossary](#glossary)\n- [More Information](#more-information)\n- [Model Card Authors](#model-card-authors)\n- [Model Card Contact](#model-card-contact)\n- [How to Get Started with the Model](#how-to-get-started-with-the-model)\n\n# Model Details\n\n## Model Description\n\nProject Indus LLM aims to provide a robust language model for Indian languages, starting with Hindi and its dialects. This open-source foundational model, hosted on Hugging Face, is tailored for easy integration and further development by researchers and developers focusing on Indian linguistic diversity.\n\n<!-- Provide a longer summary of what this model is\/does. -->\nThe model is a pretrained model in Hindi and dialects which is instruct tuned.\n\n- **Developed by:** Nikhil Malhotra, Nilesh Brahme, Satish Mishra, Vinay Sharma (Makers Lab, TechMahindra)\n- **Model type:** Foundational Language model\n- **Language(s) (NLP):** hin, bho, mai, doi\n- **License:** other\n- **Parent Model:** It is a grounds up model built on GPT-2 architecture starting from tokenizer to decoder\n- **Resources for more information:** <https:\/\/www.techmahindra.com\/en-in\/innovation\/the-indus-project\/>\n  \n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\nUses include question and answeting and conversation in Hindi and Dialects. The model would be reward tuned to be used across various industries\n\n1. Call center\n2. Healthcare\n3. Automotive\n4. Telecom\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem\/app. -->\n<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say \"more info needed.\" -->\nProject Indus can be directly used for generating text, simulating conversation, and other text generation tasks without additional training.\n\n## Downstream Use\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem\/app -->\n<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say \"more info needed.\" -->\n\nUses include question and answeting and conversation in Hindi and Dialects. The model would be reward tuned to be used across various industries\n\n1. Call center\n2. Healthcare\n3. Automotive\n4. Telecom\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say \"more info needed.\" -->\nProject Indus is not designed for high-stakes decision-making tasks such as medical diagnosis or legal advice, nor can it be used for fill-in-the-blank exercises, multiple Q&A, and similar applications at the moment.\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nSignificant research has explored bias and fairness issues with language models\n(see, e.g., [Sheng et al. (2021)](https:\/\/aclanthology.org\/2021.acl-long.330.pdf) and [Bender et al. (2021)](https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/3442188.3445922)).\nPredictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\nWe have taken care across various biases by trying to remove them from training data. However since the model is a generative model, it would tend to produce hallucinations.\nAny disturbing or harmful sterotype produced by the model is purely un-intentional and coincidental.\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nIt is recommended to avoid biases and negative connotations in the model, and regular updates along with community feedback are crucial for addressing any emergent bias or misuse scenarios.\n\n# Training Details\n\nThe model was trained on a curated dataset comprising various sources of Hindi text, including literature, news articles, and web content.\n\n## Infrastructure\n\n- **Training Infrastructure:** Utilized high-performance computing resources provided by CDAC, featuring NVIDIA A100 GPUs.\n- **Running Infrastructure:** Tested for both GPU (NVIDIA GeForce RTX 3070 or higher) and CPU (Intel Xeon Platinum 8580) environments.\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\nThe Project Indus LLM was trained on a diverse and extensive dataset comprising various sources of Hindi text and its dialects. The data collection and curation process was meticulously designed to cater to the linguistic diversity and complexity of Indian languages, particularly focusing on Hindi and its 37 dialects.\n\n### Data Sources and Collection\n\nData was collected in three main buckets:\n\n1. **Open-Source Hindi Data**: This included publicly available sources from the internet across different categories such as news, and non-news. Automated scripts were used to scrape and extract text from web pages. Here are some of the sources:\n   - **News**: Articles from news portals.\n   - **Non-News**: Diverse sources including Wikipedia, commoncrawl.org, and other culturally significant content like 'Man ki Baat' from AIR.\n\n2. **Translated Data**: A portion of the Pile dataset, which is a large English dataset used for training AI models, was translated into Hindi using three different translation models. IndicTrans2 (AI4Bharat) was selected as the best model for this purpose based on its accuracy and efficiency.\n\n3. **Dialects**: Data collection for dialects presented a unique challenge due to the limited material available on the internet. Data for major dialects like Maithili, Bhojpuri, Magahi, and Braj Bhasha was collected from multiple sources, including fieldwork where representatives collected old books and other texts, which were then digitized and converted into text data.\n\n## Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\nTraining involved extensive preprocessing to clean and standardize the text, followed by supervised learning on a high-performance computing setup.\n\n- **Pre-training:** Conducted on a dataset of 22 billion tokens using advanced tokenization techniques.\n- **Fine-Tuning:** Supervised fine-tuning performed with a focus on Indian languages, utilizing datasets specifically tailored for cultural, political, and social contexts.\n\nBelow is a table summarizing the datasets used for pre-training and fine-tuning the model:\n\n| Phase         | Data Source                             | Tokens    | Notes                                               |\n|---------------|-----------------------------------------|-----------|-----------------------------------------------------|\n| Pre-training  | Cleaned dataset of Hindi and dialects   | 22 billion| Utilized advanced tokenization                      |\n| Fine-tuning   | Custom datasets tailored for Indian languages | Varied    | Focus on cultural, political, and social contexts   |\n\n- **Training Infrastructure:** Utilized high-performance computing resources provided by CDAC, featuring NVIDIA A100 GPUs.\n- **Running Infrastructure:** Tested for both GPU (NVIDIA GeForce RTX 3070 or higher) and CPU (Intel Xeon Platinum 8580) environments.\n\n### Preprocessing\n\nThe collected data underwent several stages of cleaning and preprocessing to ensure high quality and usability for training:\n\n- **Cleaning**: The data was cleaned of unwanted text, characters, and personal information like mobile numbers. Transliteration was performed where necessary, and unwanted tags from scraped web pages were removed.\n- **Bias Removal**: A Bias Removal Toolkit was developed to detect and remove biased language from the training data. This toolkit helped in ensuring that the text used for training the model was ethical, correct, and socially responsible.\n- **Tokenization**: The data was tokenized using a custom tokenizer developed specifically for Hindi and its dialects. This tokenizer was based on Byte Pair Encoding (BPE) with additional mechanisms like byte fallback to handle the peculiarities of Hindi script efficiently.\n\n#### Summary\n\nThe final dataset used for training consisted of:\n\n- **Raw Data Size**: Over 500 GB of raw data collected.\n- **Cleaned and Curated Data**: Approximately 200 GB of clean Hindi and dialect text data.\n- **Tokenization**: Utilized 22 billion tokens created from the cleaned data for pre-training.\n\nThis diverse and extensive training data foundation allowed Project Indus LLM to develop robust capabilities for understanding and generating Hindi text, making it a powerful tool for applications requiring Indian language processing.\n\n# Evaluation\n\n### Indic LLM Leaderboard Results\n\nProject Indus LLM has been evaluated using the Indic LLM Leaderboard, which employs the `indic_eval` evaluation framework specifically designed for assessing models on Indian language tasks. This framework provides a comprehensive view of model performance across a variety of benchmarks tailored to Indian languages.\n\nDetailed results from the Indic LLM Leaderboard (\u03b1), accessible at [Hugging Face Indic LLM Leaderboard](https:\/\/huggingface.co\/spaces\/Cognitive-Lab\/indic_llm_leaderboard), are shown below:\n\n|              Task              | Version |  Metric  | Value |   | Stderr |\n|--------------------------------|---------|----------|-------|---|--------|\n| All                            |         | acc      | 0.2891| \u00b1 | 0.0109 |\n|                                |         | acc_norm | 0.3013| \u00b1 | 0.0112 |\n| indiceval:ARC-Challenge:hindi:10 |    0    | acc      | 0.2167| \u00b1 | 0.0120 |\n|                                |         | acc_norm | 0.2474| \u00b1 | 0.0126 |\n| indiceval:ARC-Easy:hindi:5     |    0    | acc      | 0.3615| \u00b1 | 0.0099 |\n|                                |         | acc_norm | 0.3552| \u00b1 | 0.0098 |\n\nThese results highlight the model's capabilities in understanding and generating Hindi language text under controlled testing conditions. The standard error values indicate the variance observed during the evaluation, providing insights into the consistency of the model's performance across different evaluation runs.\n\n### Open LLM Leaderboard Evaluation Results\n\nAdditionally, Project Indus LLM has been evaluated on the Open LLM Leaderboard, which provides another layer of benchmarking by comparing the model's performance against other state-of-the-art language models. Below are the summarized results from the Open LLM Leaderboard:\n\n|             Metric              |Value|\n|---------------------------------|----:|\n|Avg.                             |20.07|\n|AI2 Reasoning Challenge (25-Shot)|22.70|\n|HellaSwag (10-Shot)              |25.04|\n|MMLU (5-Shot)                    |23.12|\n|Winogrande (5-shot)              |49.57|\n\nThese benchmark results can be explored further on [Hugging Face Open LLM Leaderboard](https:\/\/huggingface.co\/spaces\/HuggingFaceH4\/open_llm_leaderboard).\n\n### Evaluation Context\n\nThe evaluation metrics `acc` (accuracy) and `acc_norm` (normalized accuracy) are used to quantify the model's performance. The tasks are differentiated by their difficulty and the specific dataset used, such as the ARC Challenge and ARC Easy sets, both adapted to Hindi language conditions to ensure relevant assessment. This structured evaluation ensures that the Indus LLM not only performs well in generalized text generation tasks but also in more specialized, context-specific scenarios pertinent to the Indian linguistic framework.\n\n## Results\n\nProject Indus demonstrates competitive performance, particularly in text generation tasks, as evidenced by its scores on standardized benchmarks.\n\n# Technical Specifications\n\n## Model Architecture and Objective\n\nProject Indus LLM is based on a GPT-2.0-like architecture, tailored to handle the complexities of the Hindi language and its dialects. This model was designed to serve as a foundational model that can be fine-tuned for various applications, making it highly versatile and adaptable to different domains within the Indian context.\n\n- **Architecture Details**:\n  - **Layers**: 22 transformer layers, which provide a deep neural network capable of understanding complex language patterns.\n  - **Heads**: 32 attention heads per layer, facilitating a broad attention mechanism across different parts of the input data.\n  - **Embedding Size**: 2048, which allows the model to represent a wide variety of information and nuances in the data.\n  - **Vocabulary Size**: 32,300, tailored to include a comprehensive set of Hindi words and common phrases found in the training data.\n\nThe objective of this model is to provide a robust tool for text generation and understanding in Hindi and its dialects, supporting the development of applications that require natural language processing in these languages. It also aims to bridge the gap in technology where Indian languages are underrepresented, providing a platform for further linguistic research and technological inclusion.\n\n## Compute Infrastructure\n\n##### Hardware\n\nThe pre-training and fine-tuning of Project Indus LLM were conducted on high-performance computing infrastructure provided by the Centre for Development of Advanced Computing (CDAC). This setup included:\n\n- **Nodes and GPUs**: Utilization of six nodes, each equipped with eight NVIDIA A100 GPUs. These GPUs are state-of-the-art for machine learning tasks and provide the necessary computational power to handle the large volumes of data and complex model architectures.\n- **Memory and Storage**: Each node was equipped with ample memory and storage to handle the datasets and model parameters efficiently. Specific configurations included 40 GB of GPU memory per card, essential for training large models.\n\n\nInference performance was tested on GPU as well as CPU.\n- **GPU**: On GPU NVIDIA GeForce RTX 3070  we have seen for 250-350 tokens inference time around ~5-10s.\n- **CPU**: On Intel CPU Xeon(R) Platinum 8580 we have seen performance comparable to GPU with throughput of > 30 token\/second.\n\n##### Software\n\nThe software environment was crucial for efficiently training and running the model. Key components included:\n\n- **Operating System**: Linux, chosen for its stability and support for high-performance computing tasks.\n- **Machine Learning Frameworks**: PyTorch, used for its flexibility and efficiency in training deep learning models. It supports extensive parallel processing and GPU acceleration, which are critical for training large models like Project Indus LLM.\n- **Job Scheduler**: SLURM (Simple Linux Utility for Resource Management) was used to manage and allocate resources effectively across the distributed system. This ensured optimal scheduling of training jobs without resource contention.\n\n# Citation\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\nThe detailed citation information will help in acknowledging the work and efforts of the team behind Project Indus LLM when it is used or referenced in academic or professional settings.\n\n```bibtex\n@article{malhotra2024projectindus,\n  title={Project Indus: A Foundational Model for Indian Languages},\n  author={Malhotra, Nikhil and Brahme, Nilesh and Mishra, Satish and Sharma, Vinay},\n  journal={Tech Mahindra Makers Lab},\n  year={2024},\n  url={https:\/\/www.techmahindra.com\/en-in\/innovation\/the-indus-project\/}\n}\n```\n\n**APA:**\nMalhotra, N., Brahme, N., Mishra, S., & Sharma, V. (2024). Project Indus: A Foundational Model for Indian Languages. *Tech Mahindra Makers Lab*. Available at <https:\/\/www.techmahindra.com\/en-in\/innovation\/the-indus-project\/>\n\n# Glossary\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\nThis glossary section explains key terms used throughout the model documentation and technical details, helping users unfamiliar with certain concepts to better understand the content.\n\n- **Transformer Layers**: Part of a neural network architecture that uses self-attention mechanisms to process sequential data such as text. Essential for NLP tasks.\n- **Attention Heads**: Sub-units of a model layer that allow the model to focus on different parts of the input sequence when making predictions.\n- **Embedding Size**: The size of the vector used to represent each token or word in a dense numerical form. Larger embeddings can capture more detailed information.\n- **Block Size**: The maximum length of the input tokens the model can process in one operation.\n- **Vocabulary Size**: The total number of unique words or tokens that the model can understand and generate.\n\n# More Information\n\nFor further details on Project Indus LLM, including additional documentation, tutorials, and community discussions, visit the following resources:\n\n- **Project Repository**: [Hugging Face Repository](https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus)\n- **Tech Mahindra Makers Lab**: Insights into the research and development behind Project Indus can be found on the [Tech Mahindra Innovation page](https:\/\/www.techmahindra.com\/en-in\/innovation\/makers-lab\/).\n- **Community Forums**: Engage with the community on [Hugging Face Forums](https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus\/discussions?status=open&type=discussion) for support, brainstorming, and sharing of new ideas related to Project Indus.\n\n# Model Card Authors\n\n<!-- This section provides another layer of transparency and accountability. Whose views is this model card representing? How many voices were included in its construction? Etc. -->\n\nThe model card and documentation for Project Indus LLM were collaboratively authored by:\n\n- **Nikhil Malhotra**: Chief Innovation Officer at Tech Mahindra.\n- **Nilesh Brahme**: Senior AI Research Scientist and one of the primary contributors to the Project Indus development.\n- **Satish Mishra**: AI Architect, whose insights have significantly shaped the model's capabilities.\n- **Vinay Sharma**: LLM Engineer focused on the linguistic data processing and model training aspects of Project Indus.\n\n# Model Card Contact\n\nFor inquiries, support, or further information regarding Project Indus LLM, please reach out through the following channels:\n\n- **Email**: [projectindus@techmahindra.com](mailto:projectindus@techmahindra.com) - For direct queries and professional engagements.\n- **GitHub Issues**: For technical issues, feature requests, or contributions, please use the Issues section of the [Project Indus GitHub repository](https:\/\/github.com\/Tech-Mahindra-Makers-Lab\/Indus-1.1B).\n- **Hugging Face Spaces**: Questions and discussions related to model implementation and community projects can be posted in our dedicated space on Hugging Face.\n\n# How to Get Started with the Model\n\nTo begin using Project Indus LLM for your projects, follow these steps to set up and run the model:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"nickmalhotra\/ProjectIndus\")\ntokenizer = AutoTokenizer.from_pretrained(\"nickmalhotra\/ProjectIndus\")\n\n# Example inference\ndef format_template(user_prompt):\n    messages = [\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    response = tokenizer.apply_chat_template(messages, tokenize=True,  add_generation_prompt=True, return_tensors=\"pt\")\n    return  response\n\nuser_prompt = \"\"\"\u092d\u093e\u0930\u0924 \u0915\u0947 \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u092a\u094d\u0930\u0927\u093e\u0928\u092e\u0902\u0924\u094d\u0930\u0940 \u0915\u094c\u0928 \u0939\u0948\u0902?\"\"\"\n\ninput_ids = format_template(user_prompt)\n\n# Generate text using the model\noutput = model.generate(input_ids,\n                        eos_token_id=tokenizer.eos_token_id,\n                        pad_token_id=tokenizer.eos_token_id,\n                        max_length=1024,\n                        num_beams=5,\n                        do_sample=True,\n                        early_stopping=True,\n                        temperature=0.7,\n                        top_k=50,\n                        top_p=0.95,\n                        repetition_penalty=1.2,\n                        no_repeat_ngram_size=3,\n                        num_return_sequences=1,\n                        )\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n```\n## Disclaimer\n\n#### Model Limitations\n\nProject Indus LLM is trained with single instruction tuning, which may result in hallucinations\u2014instances where the model generates plausible but inaccurate information. Users should exercise caution, especially in scenarios requiring high factual accuracy.\n\n#### Adaptation for Specific Use Cases\n\nProject Indus LLM is designed as a foundational model suitable for further development and fine-tuning. Users are encouraged to adapt and refine the model to meet specific requirements of their applications.\n\n#### Recommendations for Fine-Tuning\n\n- **Identify Specific Needs**: Clearly define the requirements of your use case to guide the fine-tuning process.\n- **Curate Targeted Data**: Ensure the training data is relevant and of high quality to improve model performance.\n- **Continuous Evaluation**: Regularly assess the model's performance during and after fine-tuning to maintain accuracy and reduce biases.\n\nThis disclaimer aims to provide users with a clear understanding of the model's capabilities and limitations, facilitating its effective application and development.",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"ProjectIndus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/nickmalhotra\/ProjectIndus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.2895247042,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "mozilla-foundation\/common_voice_7_0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4142157435,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the fine-tuned [facebook\/wav2vec2-large-100k-voxpopuli] model for speech recognition is to enable users to make a decision as to the suitability of this creative work for their intended use. The model has been fine-tuned for the train split of [Common Voice 7.0 (de)] and is intended for use with speech input sampled at 16kHz.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3428428173,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "automatic speech recognition"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model category is not explicitly mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3789242208,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3689838946,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"jonatasgrosman",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "mozilla-foundation\/common_voice_7_0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "mozilla-foundation\/common_voice_7_0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3718383312,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"The validated dataset used for fine-tuning the model is [Common Voice 7.0 (de)](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0).",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4531916082,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3771212101,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3543135524,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The context provided does not contain any information about the distribution of the model.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4496519864,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3770575821,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3962677121,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage:\n- de\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- de\ndatasets:\n- mozilla-foundation\/common_voice_7_0\n---\n# exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564\n\nFine-tuned [facebook\/wav2vec2-large-100k-voxpopuli](https:\/\/huggingface.co\/facebook\/wav2vec2-large-100k-voxpopuli) for speech recognition using the train split of [Common Voice 7.0 (de)](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned by the [HuggingSound](https:\/\/github.com\/jonatasgrosman\/huggingsound) tool.\n    ",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.396417886,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.355854094,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"The storage requirements for the fine-tuned model are not explicitly mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3414525986,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is Jonatas Grosman.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4004722238,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3557203114,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.2962529063,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3939172924,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3786837459,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2022-07-25T04:48:01+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2022-07-25T04:48:13+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-07-25T04:48:01+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3430698514,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":[
                    "de"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "wav2vec2",
                    "automatic-speech-recognition",
                    "de",
                    "dataset:mozilla-foundation\/common_voice_7_0",
                    "license:apache-2.0",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "pytorch"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3769268394,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:version":[
            {
                "data":"exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4202061594,
                "extraction_time":"2025-04-16_22-14-32"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlanguage:\n- de\nlicense: apache-2.0\ntags:\n- automatic-speech-recognition\n- de\ndatasets:\n- mozilla-foundation\/common_voice_7_0\n---\n# exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564\n\nFine-tuned [facebook\/wav2vec2-large-100k-voxpopuli](https:\/\/huggingface.co\/facebook\/wav2vec2-large-100k-voxpopuli) for speech recognition using the train split of [Common Voice 7.0 (de)](https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_7_0).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned by the [HuggingSound](https:\/\/github.com\/jonatasgrosman\/huggingsound) tool.\n    ",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:name":[
            {
                "data":"exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/jonatasgrosman\/exp_w2v2r_de_vp-100k_gender_male-8_female-2_s564",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-11-13"
            }
        ]
    }
]