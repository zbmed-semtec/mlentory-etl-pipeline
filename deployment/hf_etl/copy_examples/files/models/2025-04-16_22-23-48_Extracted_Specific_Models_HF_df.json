[
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4934923649,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-Pure-RP",
                    "bunnycore\/Llama-3.2-3B-Sci-Think",
                    "bunnycore\/Llama-3.2-3B-Mix-Skill",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.538836807,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5914433897,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5917414427,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5925212801,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6237121224,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6400130689,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6271545887,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6046384275,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2306.01708"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.616710633,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5854815841,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6267724931,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- bunnycore\/Llama-3.2-3B-Mix-Skill\n- bunnycore\/Llama-3.2-3B-Sci-Think\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [bunnycore\/Llama-3.2-3B-Mix-Skill](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill)\n* [bunnycore\/Llama-3.2-3B-Sci-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: bunnycore\/Llama-3.2-3B-Sci-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Mix-Skill\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6475779414,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6482552886,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5734918416,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5849795938,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5540062785,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5753189027,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6104165018,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5899616182,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-24T20:22:30+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-24T20:24:23+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-24T20:22:30+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5642807186,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "arxiv:2306.01708",
                    "base_model:bunnycore\/Llama-3.2-3B-Mix-Skill",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Mix-Skill",
                    "base_model:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:bunnycore\/Llama-3.2-3B-Sci-Think",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Sci-Think",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5986407995,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6516102552,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- bunnycore\/Llama-3.2-3B-Mix-Skill\n- bunnycore\/Llama-3.2-3B-Sci-Think\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [bunnycore\/Llama-3.2-3B-Mix-Skill](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill)\n* [bunnycore\/Llama-3.2-3B-Sci-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: bunnycore\/Llama-3.2-3B-Sci-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Mix-Skill\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Prodigy",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4934923649,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-Pure-RP",
                    "bunnycore\/Llama-3.2-3B-Long-Think",
                    "bunnycore\/Llama-3.2-3B-Prodigy",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "bunnycore\/Llama-3.2-3B-CodeReactor",
                    "bunnycore\/Llama-3.2-3B-Sci-Think"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.538836807,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5914433897,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5917414427,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5925212801,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6237121224,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6400130689,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6271545887,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6046384275,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-ProdigyPlus\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-ProdigyPlus\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2306.01708"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.616710633,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5854815841,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6267724931,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Sci-Think\n- bunnycore\/Llama-3.2-3B-Long-Think\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-CodeReactor\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- bunnycore\/Llama-3.2-3B-Prodigy\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n\n\n![image\/png](https:\/\/cdn-uploads.huggingface.co\/production\/uploads\/63284f86cbc744f197050300\/Z-S-zWVosxRli0ip3LBu-.png)\n\nLlama-3.2-3B-ProdigyPlus is a fine-tuned version of the original Llama-3.2-3B model. It has been trained on a diverse range of datasets to enhance its capabilities in several areas, including:\n\n- Instruction Following: The model is adept at understanding and completing a wide array of tasks when given clear instructions.\n- Roleplay: It can take on different personas and engage in creative role-playing scenarios.\n- Creative Writing: The model can generate creative text formats like poems, scripts, musical pieces, email, letters, etc.\n- Coding: While limited, the model can assist with basic coding tasks and code generation.\n\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Sci-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think)\n* [bunnycore\/Llama-3.2-3B-Long-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think)\n* [bunnycore\/Llama-3.2-3B-CodeReactor](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor)\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [bunnycore\/Llama-3.2-3B-Prodigy](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: bunnycore\/Llama-3.2-3B-Sci-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-CodeReactor\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Long-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Prodigy\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6475779414,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6482552886,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5734918416,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-ProdigyPlus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5849795938,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5540062785,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5753189027,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6104165018,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5899616182,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-25T09:15:01+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-26T12:54:10+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-25T09:15:01+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-ProdigyPlus\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5642807186,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "arxiv:2306.01708",
                    "base_model:bunnycore\/Llama-3.2-3B-CodeReactor",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-CodeReactor",
                    "base_model:bunnycore\/Llama-3.2-3B-Long-Think",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Long-Think",
                    "base_model:bunnycore\/Llama-3.2-3B-Prodigy",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Prodigy",
                    "base_model:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:bunnycore\/Llama-3.2-3B-Sci-Think",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Sci-Think",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5986407995,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6516102552,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Sci-Think\n- bunnycore\/Llama-3.2-3B-Long-Think\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-CodeReactor\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- bunnycore\/Llama-3.2-3B-Prodigy\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n\n\n![image\/png](https:\/\/cdn-uploads.huggingface.co\/production\/uploads\/63284f86cbc744f197050300\/Z-S-zWVosxRli0ip3LBu-.png)\n\nLlama-3.2-3B-ProdigyPlus is a fine-tuned version of the original Llama-3.2-3B model. It has been trained on a diverse range of datasets to enhance its capabilities in several areas, including:\n\n- Instruction Following: The model is adept at understanding and completing a wide array of tasks when given clear instructions.\n- Roleplay: It can take on different personas and engage in creative role-playing scenarios.\n- Creative Writing: The model can generate creative text formats like poems, scripts, musical pieces, email, letters, etc.\n- Coding: While limited, the model can assist with basic coding tasks and code generation.\n\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Sci-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think)\n* [bunnycore\/Llama-3.2-3B-Long-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think)\n* [bunnycore\/Llama-3.2-3B-CodeReactor](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor)\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [bunnycore\/Llama-3.2-3B-Prodigy](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Prodigy)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: bunnycore\/Llama-3.2-3B-Sci-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-CodeReactor\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Long-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Prodigy\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-ProdigyPlus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-ProdigyPlus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-ProdigyPlus",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4934923649,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-Long-Think-lora_model",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.538836807,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5914433897,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5917414427,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5925212801,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6237121224,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6400130689,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6271545887,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6046384275,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.616710633,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5854815841,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6267724931,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-Long-Think-lora_model\nmodel-index:\n- name: Llama-3.2-3B-Long-Think\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 54.73\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 24.23\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.92\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 1.45\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 1.21\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 22.75\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-Long-Think-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Long-Think-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Long-Think-lora_model\n\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_bunnycore__Llama-3.2-3B-Long-Think)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |19.55|\n|IFEval (0-Shot)    |54.73|\n|BBH (3-Shot)       |24.23|\n|MATH Lvl 5 (4-Shot)|12.92|\n|GPQA (0-shot)      | 1.45|\n|MuSR (0-shot)      | 1.21|\n|MMLU-PRO (5-shot)  |22.75|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6475779414,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6482552886,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5734918416,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5849795938,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5540062785,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5753189027,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6104165018,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5899616182,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-24T09:24:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-25T08:54:32+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-24T09:24:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5642807186,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "base_model:bunnycore\/Llama-3.2-3B-Long-Think-lora_model",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Long-Think-lora_model",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "model-index",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5986407995,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6516102552,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-Long-Think-lora_model\nmodel-index:\n- name: Llama-3.2-3B-Long-Think\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 54.73\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 24.23\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.92\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 1.45\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 1.21\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 22.75\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Long-Think\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-Long-Think-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Long-Think-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Long-Think-lora_model\n\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_bunnycore__Llama-3.2-3B-Long-Think)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |19.55|\n|IFEval (0-Shot)    |54.73|\n|BBH (3-Shot)       |24.23|\n|MATH Lvl 5 (4-Shot)|12.92|\n|GPQA (0-shot)      | 1.45|\n|MuSR (0-shot)      | 1.21|\n|MMLU-PRO (5-shot)  |22.75|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Long-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6411774755,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-Pure-RP",
                    "bunnycore\/Llama-3.2-3B-Long-Think",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6207228005,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the model is to generate stories, poems, scripts, and other forms of creative text, provide comprehensive and informative answers to a wide range of questions, engage in interactive role-playing scenarios with users, and complete tasks and generate text based on specific prompts or instructions.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7993126214,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6479079425,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6908402145,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the usage Instructions in the following text, here is a description of the property: (Instructions for using the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7334293723,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.682315886,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.686740607,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.65100649,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2306.01708"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7314845622,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6331831813,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6779129505,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- bunnycore\/Llama-3.2-3B-Long-Think\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-Pure-RP\nmodel-index:\n- name: Llama-3.2-3B-Mix-Skill\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 64.04\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 23.78\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.69\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 1.57\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 2.75\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.56\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n---\nThis language model is a merged version of several pre-trained models, designed to excel in roleplay, long-form question answering, and prompt following tasks. It was created using the TIES merge method with huihui-ai\/Llama-3.2-3B-Instruct-abliterated as the base model.\n\n# Intended Use:\n\nThis model is suitable for a variety of applications, including:\n\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Long-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think)\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: bunnycore\/Llama-3.2-3B-Long-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_bunnycore__Llama-3.2-3B-Mix-Skill)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |21.40|\n|IFEval (0-Shot)    |64.04|\n|BBH (3-Shot)       |23.78|\n|MATH Lvl 5 (4-Shot)|12.69|\n|GPQA (0-shot)      | 1.57|\n|MuSR (0-shot)      | 2.75|\n|MMLU-PRO (5-shot)  |23.56|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7004508078,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\nQuestion: Find the software Requirements in the following text, here is a description of the property: (Required software dependencies.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nIntended Use: - Par. 1: Intended Use::\nThis model is suitable for a variety of applications, including:\nIntended Use:: Intended Use::\nThis model is suitable for a variety of applications, including:\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6878592372,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6727980971,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6826664209,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the citation in the following text, here is a description of the property: (A citation or reference to another creative work, such as another publication, web page, scholarly article, etc.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6956564784,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7170079947,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6765497327,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6533347964,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-24T09:41:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-29T19:01:21+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-24T09:41:04+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6992380023,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "arxiv:2306.01708",
                    "base_model:bunnycore\/Llama-3.2-3B-Long-Think",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Long-Think",
                    "base_model:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "model-index",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6617713273,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.673420459,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- bunnycore\/Llama-3.2-3B-Long-Think\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-Pure-RP\nmodel-index:\n- name: Llama-3.2-3B-Mix-Skill\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 64.04\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 23.78\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.69\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 1.57\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 2.75\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.56\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Mix-Skill\n      name: Open LLM Leaderboard\n---\nThis language model is a merged version of several pre-trained models, designed to excel in roleplay, long-form question answering, and prompt following tasks. It was created using the TIES merge method with huihui-ai\/Llama-3.2-3B-Instruct-abliterated as the base model.\n\n# Intended Use:\n\nThis model is suitable for a variety of applications, including:\n\n- Creative Writing: Generating stories, poems, scripts, and other forms of creative text.\n- Question Answering: Providing comprehensive and informative answers to a wide range of questions.\n- Role-Playing: Engaging in interactive role-playing scenarios with users.\n- Prompt Following: Completing tasks and generating text based on specific prompts or instructions.\n\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Long-Think](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Long-Think)\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: bunnycore\/Llama-3.2-3B-Long-Think\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_bunnycore__Llama-3.2-3B-Mix-Skill)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |21.40|\n|IFEval (0-Shot)    |64.04|\n|BBH (3-Shot)       |23.78|\n|MATH Lvl 5 (4-Shot)|12.69|\n|GPQA (0-shot)      | 1.57|\n|MuSR (0-shot)      | 2.75|\n|MMLU-PRO (5-shot)  |23.56|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Mix-Skill",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Mix-Skill",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4933788478,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3b-lora_model",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5386676788,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5912938118,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.591593653,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5923655033,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6235636473,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6398843229,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6270539165,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6045265496,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6165635288,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5853861868,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6266424358,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3b-lora_model\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3b-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3b-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3b-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3b-lora_model\n\n\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.647446841,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6481674016,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5733431578,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5848668814,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5538733304,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5751701593,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6102774143,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5898098946,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-20T10:00:31+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-20T10:02:13+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-20T10:00:31+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5641268492,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "base_model:bunnycore\/Llama-3.2-3b-lora_model",
                    "base_model:merge:bunnycore\/Llama-3.2-3b-lora_model",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5985437632,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6515239179,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3b-lora_model\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3b-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3b-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3b-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3b-lora_model\n\n\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Pure-RP",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the ethical Legal Social in the following text, here is a description of the property: (Considerations with regards to ethical legal and social aspects.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.3963243514,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "librispeech_asr"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4675019383,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the DistilHuBERT model is for speech representation learning and fine-tuning on labeled text data for speech recognition tasks. The model can be used for various speech processing tasks, and it requires little training time and data, making it accessible for researchers in academia and small companies. The model can be fine-tuned on labeled text data to improve its performance in different speech processing tasks.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4793211818,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "feature extraction"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model category is not explicitly mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4556955397,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4716938138,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"ntu-spml",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "librispeech_asr"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "librispeech_asr"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5000735521,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5396763682,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.\n\nQuestion: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions\/documentation.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions\/documentation.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions\/documentation.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the build Instructions in the following text, here is a description of the property: (Link to installation instructions\/documentation.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4844235927,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4230738729,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/ntu-spml\/distilhubert\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/ntu-spml\/distilhubert\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2110.01900"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The distribution in the given text is a media object that encodes this creative work.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5605612099,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4513515383,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4965201169,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\ndatasets:\n- librispeech_asr\ntags:\n- speech\nlicense: apache-2.0\n---\n\n# DistilHuBERT\n\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\n\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\n\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\n\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\n\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller .\n\n# Usage\n\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the software Help in the following text, here is a description of the property: (Software application help.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5059787482,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4689379185,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"The storage requirements for the DistilHuBERT model are not explicitly mentioned in the provided context. However, it is mentioned that the model does not have a tokenizer as it was pretrained on audio alone. To use this model for speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4554784894,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/ntu-spml\/distilhubert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4935416877,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4719195813,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the conditions Of Access in the following text, here is a description of the property: (Conditions that affect the availability of, or method(s) of access to, an item.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4272316843,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller.\nUsage: Usage:\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n\nQuestion: Find the contributor in the following text, here is a description of the property: (A secondary contributor to the CreativeWork.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nContext:\nDistilHuBERT: DistilHuBERT:\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4823360741,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4569398314,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2022-03-02T23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2023-07-24T18:30:45+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2022-03-02T23:29:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/ntu-spml\/distilhubert\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4450861067,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "safetensors",
                    "hubert",
                    "feature-extraction",
                    "speech",
                    "en",
                    "dataset:librispeech_asr",
                    "arxiv:2110.01900",
                    "license:apache-2.0",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "pytorch",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "apache-2.0"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4744514078,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4992813468,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlanguage: en\ndatasets:\n- librispeech_asr\ntags:\n- speech\nlicense: apache-2.0\n---\n\n# DistilHuBERT\n\n[DistilHuBERT by NTU Speech Processing & Machine Learning Lab](https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller)\n\nThe base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n\n**Note**: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model **speech recognition**, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more in-detail explanation of how to fine-tune the model.\n\nPaper: [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https:\/\/arxiv.org\/abs\/2110.01900)\n\nAuthors: Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee\n\n**Abstract**\nSelf-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.\n\nThe original model can be found under https:\/\/github.com\/s3prl\/s3prl\/tree\/master\/s3prl\/upstream\/distiller .\n\n# Usage\n\nSee [this blog](https:\/\/huggingface.co\/blog\/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/ntu-spml\/distilhubert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"distilhubert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/ntu-spml\/distilhubert",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4934923649,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-General-lora_model",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.538836807,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5914433897,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5917414427,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5925212801,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6237121224,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6400130689,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6271545887,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6046384275,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.616710633,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5854815841,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6267724931,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-General-lora_model\nmodel-index:\n- name: Llama-3.2-3B-Booval\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 66.69\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 22.52\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 11.1\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 2.24\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 2.4\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 22.86\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-General-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-General-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-General-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-General-lora_model\n\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_bunnycore__Llama-3.2-3B-Booval)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |21.30|\n|IFEval (0-Shot)    |66.69|\n|BBH (3-Shot)       |22.52|\n|MATH Lvl 5 (4-Shot)|11.10|\n|GPQA (0-shot)      | 2.24|\n|MuSR (0-shot)      | 2.40|\n|MMLU-PRO (5-shot)  |22.86|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6475779414,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6482552886,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5734918416,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5849795938,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5540062785,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5753189027,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6104165018,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5899616182,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-27T18:49:20+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-29T18:58:00+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-27T18:49:20+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5642807186,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "base_model:bunnycore\/Llama-3.2-3B-General-lora_model",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-General-lora_model",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "model-index",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5986407995,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6516102552,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-General-lora_model\nmodel-index:\n- name: Llama-3.2-3B-Booval\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 66.69\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 22.52\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 11.1\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 2.24\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 2.4\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 22.86\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=bunnycore\/Llama-3.2-3B-Booval\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-General-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-General-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-General-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-General-lora_model\n\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_bunnycore__Llama-3.2-3B-Booval)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |21.30|\n|IFEval (0-Shot)    |66.69|\n|BBH (3-Shot)       |22.52|\n|MATH Lvl 5 (4-Shot)|11.10|\n|GPQA (0-shot)      | 2.24|\n|MuSR (0-shot)      | 2.40|\n|MMLU-PRO (5-shot)  |22.86|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Booval",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Booval",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7116666436,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "libritts"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7918999195,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the model is not explicitly stated in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.788012743,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text to audio",
                    "text to speech"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6605247557,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7854522467,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"microsoft",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "libritts"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "libritts"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.729183346,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7345752716,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7036257386,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6787337959,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/microsoft\/speecht5_tts\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/microsoft\/speecht5_tts\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2110.07205",
                    "https:\/\/arxiv.org\/abs\/1910.09700"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7464481294,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6680327654,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7605810463,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: mit\ntags:\n- audio\n- text-to-speech\ndatasets:\n- libritts\n---\n\n# SpeechT5 (TTS task)\n\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n\nThis model was introduced in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https:\/\/arxiv.org\/abs\/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nSpeechT5 was first released in [this repository](https:\/\/github.com\/microsoft\/SpeechT5\/), [original weights](https:\/\/huggingface.co\/mechanicalsea\/speecht5-tts). The license used is [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE).\n\n\n\n## Model Description\n\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech\/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets. After preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech\/text states with latent units as the interface between encoder and decoder.\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n- **Developed by:** Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n- **Shared by [optional]:** [Matthijs Hollemans](https:\/\/huggingface.co\/Matthijs)\n- **Model type:** text-to-speech\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE)\n- **Finetuned from model [optional]:** [More Information Needed]\n\n\n## Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [https:\/\/github.com\/microsoft\/SpeechT5\/]\n- **Paper:** [https:\/\/arxiv.org\/pdf\/2110.07205.pdf]\n- **Blog Post:** [https:\/\/huggingface.co\/blog\/speecht5]\n- **Demo:** [https:\/\/huggingface.co\/spaces\/Matthijs\/speecht5-tts-demo]\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n## \ud83e\udd17 Transformers Usage\n\nYou can run SpeechT5 TTS locally with the \ud83e\udd17 Transformers library.\n\n1. First install the \ud83e\udd17 [Transformers library](https:\/\/github.com\/huggingface\/transformers), sentencepiece, soundfile and datasets(optional):\n\n```\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\n\nsynthesiser = pipeline(\"text-to-speech\", \"microsoft\/speecht5_tts\")\n\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\n\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n\nsf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n```\n\n3. Run inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\n\nprocessor = SpeechT5Processor.from_pretrained(\"microsoft\/speecht5_tts\")\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft\/speecht5_tts\")\nvocoder = SpeechT5HifiGan.from_pretrained(\"microsoft\/speecht5_hifigan\")\n\ninputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n\n# load xvector containing speaker's voice characteristics from a dataset\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n\nspeech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n\nsf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\n```\n\n### Fine-tuning the Model\n\nRefer to [this Colab notebook](https:\/\/colab.research.google.com\/drive\/1i7I5pzBcU3WDFarDnzweIj4-sVVoIUFJ) for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem\/app. -->\n\nYou can use this model for speech synthesis. See the [model hub](https:\/\/huggingface.co\/models?search=speecht5) to look for fine-tuned versions on a task that interests you.\n\n## Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem\/app -->\n\n[More Information Needed]\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nLibriTTS\n\n## Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n### Preprocessing [optional]\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\n\n\n### Training hyperparameters\n- **Precision:** [More Information Needed] <!--fp16, bf16, fp8, fp32 -->\n- **Regime:** [More Information Needed] <!--mixed precision or not -->\n\n### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start\/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n# Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n## Testing Data, Factors & Metrics\n\n### Testing Data\n\n<!-- This should link to a Data Card if possible. -->\n\n[More Information Needed]\n\n### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n## Results\n\n[More Information Needed]\n\n### Summary\n\n\n\n# Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n# Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https:\/\/mlco2.github.io\/impact#compute) presented in [Lacoste et al. (2019)](https:\/\/arxiv.org\/abs\/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n# Technical Specifications [optional]\n\n## Model Architecture and Objective\n\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets.\n\nAfter preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\n## Compute Infrastructure\n\n[More Information Needed]\n\n### Hardware\n\n[More Information Needed]\n\n### Software\n\n[More Information Needed]\n\n# Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@inproceedings{ao-etal-2022-speecht5,\n    title = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n    author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n    month = {May},\n    year = {2022},\n    pages={5723--5738},\n}\n```\n\n# Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n- **text-to-speech** to synthesize audio\n\n# More Information [optional]\n\n[More Information Needed]\n\n# Model Card Authors [optional]\n\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n# Model Card Contact\n\n[More Information Needed]\n\n\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7277882695,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7127556205,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6780243516,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/microsoft\/speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7584893107,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7723909318,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7360978127,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7354392111,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7286503017,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2023-02-02T12:56:54+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2023-11-08T14:37:23+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2023-02-02T12:56:54+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/microsoft\/speecht5_tts\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7027944326,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "pytorch",
                    "speecht5",
                    "text-to-audio",
                    "audio",
                    "text-to-speech",
                    "dataset:libritts",
                    "arxiv:2110.07205",
                    "arxiv:1910.09700",
                    "license:mit",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "pytorch"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.696118772,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7393971682,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlicense: mit\ntags:\n- audio\n- text-to-speech\ndatasets:\n- libritts\n---\n\n# SpeechT5 (TTS task)\n\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n\nThis model was introduced in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https:\/\/arxiv.org\/abs\/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nSpeechT5 was first released in [this repository](https:\/\/github.com\/microsoft\/SpeechT5\/), [original weights](https:\/\/huggingface.co\/mechanicalsea\/speecht5-tts). The license used is [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE).\n\n\n\n## Model Description\n\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech\/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets. After preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech\/text states with latent units as the interface between encoder and decoder.\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n- **Developed by:** Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n- **Shared by [optional]:** [Matthijs Hollemans](https:\/\/huggingface.co\/Matthijs)\n- **Model type:** text-to-speech\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE)\n- **Finetuned from model [optional]:** [More Information Needed]\n\n\n## Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [https:\/\/github.com\/microsoft\/SpeechT5\/]\n- **Paper:** [https:\/\/arxiv.org\/pdf\/2110.07205.pdf]\n- **Blog Post:** [https:\/\/huggingface.co\/blog\/speecht5]\n- **Demo:** [https:\/\/huggingface.co\/spaces\/Matthijs\/speecht5-tts-demo]\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n## \ud83e\udd17 Transformers Usage\n\nYou can run SpeechT5 TTS locally with the \ud83e\udd17 Transformers library.\n\n1. First install the \ud83e\udd17 [Transformers library](https:\/\/github.com\/huggingface\/transformers), sentencepiece, soundfile and datasets(optional):\n\n```\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\n\nsynthesiser = pipeline(\"text-to-speech\", \"microsoft\/speecht5_tts\")\n\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\n\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n\nsf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n```\n\n3. Run inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\n\nprocessor = SpeechT5Processor.from_pretrained(\"microsoft\/speecht5_tts\")\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft\/speecht5_tts\")\nvocoder = SpeechT5HifiGan.from_pretrained(\"microsoft\/speecht5_hifigan\")\n\ninputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n\n# load xvector containing speaker's voice characteristics from a dataset\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n\nspeech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n\nsf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\n```\n\n### Fine-tuning the Model\n\nRefer to [this Colab notebook](https:\/\/colab.research.google.com\/drive\/1i7I5pzBcU3WDFarDnzweIj4-sVVoIUFJ) for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem\/app. -->\n\nYou can use this model for speech synthesis. See the [model hub](https:\/\/huggingface.co\/models?search=speecht5) to look for fine-tuned versions on a task that interests you.\n\n## Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem\/app -->\n\n[More Information Needed]\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nLibriTTS\n\n## Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n### Preprocessing [optional]\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\n\n\n### Training hyperparameters\n- **Precision:** [More Information Needed] <!--fp16, bf16, fp8, fp32 -->\n- **Regime:** [More Information Needed] <!--mixed precision or not -->\n\n### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start\/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n# Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n## Testing Data, Factors & Metrics\n\n### Testing Data\n\n<!-- This should link to a Data Card if possible. -->\n\n[More Information Needed]\n\n### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n## Results\n\n[More Information Needed]\n\n### Summary\n\n\n\n# Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n# Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https:\/\/mlco2.github.io\/impact#compute) presented in [Lacoste et al. (2019)](https:\/\/arxiv.org\/abs\/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n# Technical Specifications [optional]\n\n## Model Architecture and Objective\n\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets.\n\nAfter preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\n## Compute Infrastructure\n\n[More Information Needed]\n\n### Hardware\n\n[More Information Needed]\n\n### Software\n\n[More Information Needed]\n\n# Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@inproceedings{ao-etal-2022-speecht5,\n    title = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n    author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n    month = {May},\n    year = {2022},\n    pages={5723--5738},\n}\n```\n\n# Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n- **text-to-speech** to synthesize audio\n\n# More Information [optional]\n\n[More Information Needed]\n\n# Model Card Authors [optional]\n\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n# Model Card Contact\n\n[More Information Needed]\n\n\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/microsoft\/speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/microsoft\/speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4186157584,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5144173503,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the model is not explicitly stated in the given context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5123895854,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text to audio"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model category is not explicitly mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.508338809,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5185536444,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"eligapris",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5513141751,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"The validated ON in the text refers to the ONNX (Open Neural Network Exchange) format.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5832082033,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5504375994,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4942946136,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/eligapris\/microsoft-speecht5_tts-ONNX\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/eligapris\/microsoft-speecht5_tts-ONNX\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The distribution in the given text is a media object that encodes this creative work.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5627765954,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported by the model are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.515645355,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5734291971,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlanguage: en\ntags:\n- onnx\n- transformers.js\n---\n\n# eligapris\/microsoft-speecht5_tts-ONNX\n\nThis is an ONNX-converted version of the model for use with [\ud83e\udd17 Transformers.js](https:\/\/github.com\/xenova\/transformers.js).\n\n## Usage\n\n```javascript\nimport { pipeline } from '@xenova\/transformers';\n\n\/\/ Load the model\nconst model = await pipeline('task-type', 'eligapris\/microsoft-speecht5_tts-ONNX');\n\n\/\/ Use the model\n\/\/ Check the original model's documentation for specific usage instructions\n```\n\n## Original Model\n\nThis model was converted from the original HuggingFace model. Please refer to the original model's documentation for detailed information about its architecture, training, and usage.\n\n## Conversion Details\n\nThis model was converted to ONNX format using the Transformers.js conversion script. The conversion includes quantization for optimal performance.\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5712617338,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5430466235,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.481756717,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/eligapris\/microsoft-speecht5_tts-ONNX",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is special in HTML 5 provides a special mechanism for indicating authorship via the rel tag.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5313377976,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4768645465,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4634504765,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"The information provided does not contain the name of the contributor.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5233451873,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5196456909,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-11-19T07:33:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-11-19T07:33:51+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-11-19T07:33:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/eligapris\/microsoft-speecht5_tts-ONNX\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4702803642,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":[
                    "en"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers.js",
                    "onnx",
                    "speecht5",
                    "text-to-audio",
                    "en",
                    "region:us",
                    "transformers.js",
                    "onnx"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5200805664,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5573445559,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlanguage: en\ntags:\n- onnx\n- transformers.js\n---\n\n# eligapris\/microsoft-speecht5_tts-ONNX\n\nThis is an ONNX-converted version of the model for use with [\ud83e\udd17 Transformers.js](https:\/\/github.com\/xenova\/transformers.js).\n\n## Usage\n\n```javascript\nimport { pipeline } from '@xenova\/transformers';\n\n\/\/ Load the model\nconst model = await pipeline('task-type', 'eligapris\/microsoft-speecht5_tts-ONNX');\n\n\/\/ Use the model\n\/\/ Check the original model's documentation for specific usage instructions\n```\n\n## Original Model\n\nThis model was converted from the original HuggingFace model. Please refer to the original model's documentation for detailed information about its architecture, training, and usage.\n\n## Conversion Details\n\nThis model was converted to ONNX format using the Transformers.js conversion script. The conversion includes quantization for optimal performance.\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/eligapris\/microsoft-speecht5_tts-ONNX",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"microsoft-speecht5_tts-ONNX",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/eligapris\/microsoft-speecht5_tts-ONNX",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7116666436,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":[
                    "libritts"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7918999195,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the model is not explicitly stated in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.788012743,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text to speech"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6605247557,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7854522467,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"seckmaster",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":[
                    "libritts"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":[
                    "libritts"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.729183346,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7345752716,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7036257386,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6787337959,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/seckmaster\/microsoft-speecht5_tts\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/seckmaster\/microsoft-speecht5_tts\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2110.07205",
                    "https:\/\/arxiv.org\/abs\/1910.09700"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7464481294,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6680327654,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7605810463,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlicense: mit\ntags:\n- audio\n- text-to-speech\ndatasets:\n- libritts\n---\n\n# SpeechT5 (TTS task)\n\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n\nThis model was introduced in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https:\/\/arxiv.org\/abs\/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nSpeechT5 was first released in [this repository](https:\/\/github.com\/microsoft\/SpeechT5\/), [original weights](https:\/\/huggingface.co\/mechanicalsea\/speecht5-tts). The license used is [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE).\n\n\n\n## Model Description\n\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech\/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets. After preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech\/text states with latent units as the interface between encoder and decoder.\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n- **Developed by:** Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n- **Shared by [optional]:** [Matthijs Hollemans](https:\/\/huggingface.co\/Matthijs)\n- **Model type:** text-to-speech\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE)\n- **Finetuned from model [optional]:** [More Information Needed]\n\n\n## Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [https:\/\/github.com\/microsoft\/SpeechT5\/]\n- **Paper:** [https:\/\/arxiv.org\/pdf\/2110.07205.pdf]\n- **Blog Post:** [https:\/\/huggingface.co\/blog\/speecht5]\n- **Demo:** [https:\/\/huggingface.co\/spaces\/Matthijs\/speecht5-tts-demo]\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n## \ud83e\udd17 Transformers Usage\n\nYou can run SpeechT5 TTS locally with the \ud83e\udd17 Transformers library.\n\n1. First install the \ud83e\udd17 [Transformers library](https:\/\/github.com\/huggingface\/transformers), sentencepiece, soundfile and datasets(optional):\n\n```\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\n\nsynthesiser = pipeline(\"text-to-speech\", \"microsoft\/speecht5_tts\")\n\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\n\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n\nsf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n```\n\n3. Run inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\n\nprocessor = SpeechT5Processor.from_pretrained(\"microsoft\/speecht5_tts\")\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft\/speecht5_tts\")\nvocoder = SpeechT5HifiGan.from_pretrained(\"microsoft\/speecht5_hifigan\")\n\ninputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n\n# load xvector containing speaker's voice characteristics from a dataset\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n\nspeech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n\nsf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\n```\n\n### Fine-tuning the Model\n\nRefer to [this Colab notebook](https:\/\/colab.research.google.com\/drive\/1i7I5pzBcU3WDFarDnzweIj4-sVVoIUFJ) for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem\/app. -->\n\nYou can use this model for speech synthesis. See the [model hub](https:\/\/huggingface.co\/models?search=speecht5) to look for fine-tuned versions on a task that interests you.\n\n## Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem\/app -->\n\n[More Information Needed]\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nLibriTTS\n\n## Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n### Preprocessing [optional]\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\n\n\n### Training hyperparameters\n- **Precision:** [More Information Needed] <!--fp16, bf16, fp8, fp32 -->\n- **Regime:** [More Information Needed] <!--mixed precision or not -->\n\n### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start\/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n# Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n## Testing Data, Factors & Metrics\n\n### Testing Data\n\n<!-- This should link to a Data Card if possible. -->\n\n[More Information Needed]\n\n### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n## Results\n\n[More Information Needed]\n\n### Summary\n\n\n\n# Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n# Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https:\/\/mlco2.github.io\/impact#compute) presented in [Lacoste et al. (2019)](https:\/\/arxiv.org\/abs\/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n# Technical Specifications [optional]\n\n## Model Architecture and Objective\n\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets.\n\nAfter preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\n## Compute Infrastructure\n\n[More Information Needed]\n\n### Hardware\n\n[More Information Needed]\n\n### Software\n\n[More Information Needed]\n\n# Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@inproceedings{ao-etal-2022-speecht5,\n    title = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n    author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n    month = {May},\n    year = {2022},\n    pages={5723--5738},\n}\n```\n\n# Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n- **text-to-speech** to synthesize audio\n\n# More Information [optional]\n\n[More Information Needed]\n\n# Model Card Authors [optional]\n\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n# Model Card Contact\n\n[More Information Needed]\n\n\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7277882695,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7127556205,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6780243516,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/seckmaster\/microsoft-speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7584893107,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7723909318,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7360978127,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7354392111,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7286503017,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-12-03T21:22:19+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-12-03T21:22:19+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-12-03T21:22:19+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/seckmaster\/microsoft-speecht5_tts\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7027944326,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "pytorch",
                    "speecht5",
                    "audio",
                    "text-to-speech",
                    "dataset:libritts",
                    "arxiv:2110.07205",
                    "arxiv:1910.09700",
                    "license:mit",
                    "region:us",
                    "pytorch"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "mit"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.696118772,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7393971682,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlicense: mit\ntags:\n- audio\n- text-to-speech\ndatasets:\n- libritts\n---\n\n# SpeechT5 (TTS task)\n\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n\nThis model was introduced in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https:\/\/arxiv.org\/abs\/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nSpeechT5 was first released in [this repository](https:\/\/github.com\/microsoft\/SpeechT5\/), [original weights](https:\/\/huggingface.co\/mechanicalsea\/speecht5-tts). The license used is [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE).\n\n\n\n## Model Description\n\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech\/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets. After preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech\/text states with latent units as the interface between encoder and decoder.\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n- **Developed by:** Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n- **Shared by [optional]:** [Matthijs Hollemans](https:\/\/huggingface.co\/Matthijs)\n- **Model type:** text-to-speech\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [MIT](https:\/\/github.com\/microsoft\/SpeechT5\/blob\/main\/LICENSE)\n- **Finetuned from model [optional]:** [More Information Needed]\n\n\n## Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [https:\/\/github.com\/microsoft\/SpeechT5\/]\n- **Paper:** [https:\/\/arxiv.org\/pdf\/2110.07205.pdf]\n- **Blog Post:** [https:\/\/huggingface.co\/blog\/speecht5]\n- **Demo:** [https:\/\/huggingface.co\/spaces\/Matthijs\/speecht5-tts-demo]\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n## \ud83e\udd17 Transformers Usage\n\nYou can run SpeechT5 TTS locally with the \ud83e\udd17 Transformers library.\n\n1. First install the \ud83e\udd17 [Transformers library](https:\/\/github.com\/huggingface\/transformers), sentencepiece, soundfile and datasets(optional):\n\n```\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\n\nsynthesiser = pipeline(\"text-to-speech\", \"microsoft\/speecht5_tts\")\n\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\n\nspeech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n\nsf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n```\n\n3. Run inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\n\nprocessor = SpeechT5Processor.from_pretrained(\"microsoft\/speecht5_tts\")\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft\/speecht5_tts\")\nvocoder = SpeechT5HifiGan.from_pretrained(\"microsoft\/speecht5_hifigan\")\n\ninputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n\n# load xvector containing speaker's voice characteristics from a dataset\nembeddings_dataset = load_dataset(\"Matthijs\/cmu-arctic-xvectors\", split=\"validation\")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n\nspeech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n\nsf.write(\"speech.wav\", speech.numpy(), samplerate=16000)\n```\n\n### Fine-tuning the Model\n\nRefer to [this Colab notebook](https:\/\/colab.research.google.com\/drive\/1i7I5pzBcU3WDFarDnzweIj4-sVVoIUFJ) for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem\/app. -->\n\nYou can use this model for speech synthesis. See the [model hub](https:\/\/huggingface.co\/models?search=speecht5) to look for fine-tuned versions on a task that interests you.\n\n## Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem\/app -->\n\n[More Information Needed]\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nLibriTTS\n\n## Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n### Preprocessing [optional]\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\n\n\n### Training hyperparameters\n- **Precision:** [More Information Needed] <!--fp16, bf16, fp8, fp32 -->\n- **Regime:** [More Information Needed] <!--mixed precision or not -->\n\n### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start\/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n# Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n## Testing Data, Factors & Metrics\n\n### Testing Data\n\n<!-- This should link to a Data Card if possible. -->\n\n[More Information Needed]\n\n### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n## Results\n\n[More Information Needed]\n\n### Summary\n\n\n\n# Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n# Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https:\/\/mlco2.github.io\/impact#compute) presented in [Lacoste et al. (2019)](https:\/\/arxiv.org\/abs\/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n# Technical Specifications [optional]\n\n## Model Architecture and Objective\n\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets.\n\nAfter preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder.\n\n## Compute Infrastructure\n\n[More Information Needed]\n\n### Hardware\n\n[More Information Needed]\n\n### Software\n\n[More Information Needed]\n\n# Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@inproceedings{ao-etal-2022-speecht5,\n    title = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n    author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n    month = {May},\n    year = {2022},\n    pages={5723--5738},\n}\n```\n\n# Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n- **text-to-speech** to synthesize audio\n\n# More Information [optional]\n\n[More Information Needed]\n\n# Model Card Authors [optional]\n\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n# Model Card Contact\n\n[More Information Needed]\n\n\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/seckmaster\/microsoft-speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"microsoft-speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/seckmaster\/microsoft-speecht5_tts",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4933788478,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-science-lora_model",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5386676788,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5912938118,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.591593653,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5923655033,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6235636473,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6398843229,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6270539165,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6045265496,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6165635288,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5853861868,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6266424358,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-science-lora_model\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-science-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-science-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-science-lora_model\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-science-lora_model\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.647446841,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6481674016,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5733431578,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5848668814,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5538733304,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5751701593,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6102774143,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5898098946,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-24T20:13:54+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-24T20:15:39+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-24T20:13:54+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5641268492,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "base_model:bunnycore\/Llama-3.2-3B-science-lora_model",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-science-lora_model",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5985437632,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6515239179,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-science-lora_model\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-science-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-science-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-science-lora_model\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-science-lora_model\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Sci-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Sci-Think",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4934923649,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "djuna\/ReWiz-Llama-3.2-3B-fix-config",
                    "SicariusSicariiStuff\/Impish_LLAMA_3B",
                    "unsloth\/Llama-3.2-3B"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.538836807,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5914433897,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5917414427,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5925212801,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"djuna-test-lab",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6237121224,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6400130689,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6271545887,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6046384275,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2311.03099"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.616710633,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5854815841,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6267724931,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- djuna\/ReWiz-Llama-3.2-3B-fix-config\n- SicariusSicariiStuff\/Impish_LLAMA_3B\n- unsloth\/Llama-3.2-3B\nmodel-index:\n- name: TEST-L3.2-ReWish-3B\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 63.68\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 22.07\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.92\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 4.47\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 7.92\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.62\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the linear [DARE](https:\/\/arxiv.org\/abs\/2311.03099) merge method using [unsloth\/Llama-3.2-3B](https:\/\/huggingface.co\/unsloth\/Llama-3.2-3B) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [djuna\/ReWiz-Llama-3.2-3B-fix-config](https:\/\/huggingface.co\/djuna\/ReWiz-Llama-3.2-3B-fix-config)\n* [SicariusSicariiStuff\/Impish_LLAMA_3B](https:\/\/huggingface.co\/SicariusSicariiStuff\/Impish_LLAMA_3B)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: unsloth\/Llama-3.2-3B\n  - model: SicariusSicariiStuff\/Impish_LLAMA_3B\n    parameters:\n      weight: 1\n  - model: djuna\/ReWiz-Llama-3.2-3B-fix-config\n    parameters:\n      weight: 1\nmerge_method: dare_linear\nbase_model: unsloth\/Llama-3.2-3B\ntokenizer_source: djuna\/ReWiz-Llama-3.2-3B-fix-config\nparameters:\n  normalize: true\n  int8_mask: true\ndtype: float32\nname: rewish\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_djuna-test-lab__TEST-L3.2-ReWish-3B)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |22.45|\n|IFEval (0-Shot)    |63.68|\n|BBH (3-Shot)       |22.07|\n|MATH Lvl 5 (4-Shot)|12.92|\n|GPQA (0-shot)      | 4.47|\n|MuSR (0-shot)      | 7.92|\n|MMLU-PRO (5-shot)  |23.62|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6475779414,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6482552886,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5734918416,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5849795938,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5540062785,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5753189027,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6104165018,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5899616182,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-23T12:21:12+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-30T22:28:48+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-23T12:21:12+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5642807186,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "arxiv:2311.03099",
                    "base_model:SicariusSicariiStuff\/Impish_LLAMA_3B",
                    "base_model:merge:SicariusSicariiStuff\/Impish_LLAMA_3B",
                    "base_model:djuna\/ReWiz-Llama-3.2-3B-fix-config",
                    "base_model:merge:djuna\/ReWiz-Llama-3.2-3B-fix-config",
                    "base_model:unsloth\/Llama-3.2-3B",
                    "base_model:merge:unsloth\/Llama-3.2-3B",
                    "model-index",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5986407995,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6516102552,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- djuna\/ReWiz-Llama-3.2-3B-fix-config\n- SicariusSicariiStuff\/Impish_LLAMA_3B\n- unsloth\/Llama-3.2-3B\nmodel-index:\n- name: TEST-L3.2-ReWish-3B\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 63.68\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 22.07\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.92\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 4.47\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 7.92\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.62\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the linear [DARE](https:\/\/arxiv.org\/abs\/2311.03099) merge method using [unsloth\/Llama-3.2-3B](https:\/\/huggingface.co\/unsloth\/Llama-3.2-3B) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [djuna\/ReWiz-Llama-3.2-3B-fix-config](https:\/\/huggingface.co\/djuna\/ReWiz-Llama-3.2-3B-fix-config)\n* [SicariusSicariiStuff\/Impish_LLAMA_3B](https:\/\/huggingface.co\/SicariusSicariiStuff\/Impish_LLAMA_3B)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: unsloth\/Llama-3.2-3B\n  - model: SicariusSicariiStuff\/Impish_LLAMA_3B\n    parameters:\n      weight: 1\n  - model: djuna\/ReWiz-Llama-3.2-3B-fix-config\n    parameters:\n      weight: 1\nmerge_method: dare_linear\nbase_model: unsloth\/Llama-3.2-3B\ntokenizer_source: djuna\/ReWiz-Llama-3.2-3B-fix-config\nparameters:\n  normalize: true\n  int8_mask: true\ndtype: float32\nname: rewish\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_djuna-test-lab__TEST-L3.2-ReWish-3B)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |22.45|\n|IFEval (0-Shot)    |63.68|\n|BBH (3-Shot)       |22.07|\n|MATH Lvl 5 (4-Shot)|12.92|\n|GPQA (0-shot)      | 4.47|\n|MuSR (0-shot)      | 7.92|\n|MMLU-PRO (5-shot)  |23.62|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"TEST-L3.2-ReWish-3B",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4371907562,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "unsloth\/Llama-3.2-3B",
                    "djuna-test-lab\/TEST-L3.2-ReWish-3B"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5112728626,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the model is not explicitly stated in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5286525637,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model Category is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5188578367,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5311542302,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"djuna-test-lab",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5616875589,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5780885518,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.576311484,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5346174836,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2306.01708"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5808678567,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5395729542,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5820080042,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\n- unsloth\/Llama-3.2-3B\nmodel-index:\n- name: TEST-L3.2-ReWish-3B-ties-w-base\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 63.53\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 22.07\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.92\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 4.47\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 7.92\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.62\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [unsloth\/Llama-3.2-3B](https:\/\/huggingface.co\/unsloth\/Llama-3.2-3B) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [djuna-test-lab\/TEST-L3.2-3B-ReWish-8B](https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-3B-ReWish-8B)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: unsloth\/Llama-3.2-3B\n  - model: djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\n    parameters:\n      weight: 1\n  - model: djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\n    parameters:\n      weight: 1\nmerge_method: ties\nbase_model: unsloth\/Llama-3.2-3B\ntokenizer_source: djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\nparameters:\n  density: 1\n  normalize: true\n  int8_mask: true\ndtype: float32\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_djuna-test-lab__TEST-L3.2-ReWish-3B-ties-w-base)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |22.42|\n|IFEval (0-Shot)    |63.53|\n|BBH (3-Shot)       |22.07|\n|MATH Lvl 5 (4-Shot)|12.92|\n|GPQA (0-shot)      | 4.47|\n|MuSR (0-shot)      | 7.92|\n|MMLU-PRO (5-shot)  |23.62|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5893680453,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.573862195,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5132197142,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5433948785,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:citation":[
            {
                "data":"The citation for the model was produced using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [unsloth\/Llama-3.2-3B](https:\/\/huggingface.co\/unsloth\/Llama-3.2-3B) as a base.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5129739791,
                "extraction_time":"2025-04-16_22-23-47"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4984616339,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5607678592,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.536136061,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-23T12:29:18+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-24T22:51:37+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-23T12:29:18+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5074403435,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "arxiv:2306.01708",
                    "base_model:djuna-test-lab\/TEST-L3.2-ReWish-3B",
                    "base_model:merge:djuna-test-lab\/TEST-L3.2-ReWish-3B",
                    "base_model:unsloth\/Llama-3.2-3B",
                    "base_model:merge:unsloth\/Llama-3.2-3B",
                    "model-index",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5449967086,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5905849338,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\nbase_model:\n- djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\n- unsloth\/Llama-3.2-3B\nmodel-index:\n- name: TEST-L3.2-ReWish-3B-ties-w-base\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: IFEval (0-Shot)\n      type: HuggingFaceH4\/ifeval\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: inst_level_strict_acc and prompt_level_strict_acc\n      value: 63.53\n      name: strict accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: BBH (3-Shot)\n      type: BBH\n      args:\n        num_few_shot: 3\n    metrics:\n    - type: acc_norm\n      value: 22.07\n      name: normalized accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MATH Lvl 5 (4-Shot)\n      type: hendrycks\/competition_math\n      args:\n        num_few_shot: 4\n    metrics:\n    - type: exact_match\n      value: 12.92\n      name: exact match\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GPQA (0-shot)\n      type: Idavidrein\/gpqa\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 4.47\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MuSR (0-shot)\n      type: TAUR-Lab\/MuSR\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: acc_norm\n      value: 7.92\n      name: acc_norm\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU-PRO (5-shot)\n      type: TIGER-Lab\/MMLU-Pro\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 23.62\n      name: accuracy\n    source:\n      url: https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard?query=djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base\n      name: Open LLM Leaderboard\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [unsloth\/Llama-3.2-3B](https:\/\/huggingface.co\/unsloth\/Llama-3.2-3B) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [djuna-test-lab\/TEST-L3.2-3B-ReWish-8B](https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-3B-ReWish-8B)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: unsloth\/Llama-3.2-3B\n  - model: djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\n    parameters:\n      weight: 1\n  - model: djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\n    parameters:\n      weight: 1\nmerge_method: ties\nbase_model: unsloth\/Llama-3.2-3B\ntokenizer_source: djuna-test-lab\/TEST-L3.2-3B-ReWish-8B\nparameters:\n  density: 1\n  normalize: true\n  int8_mask: true\ndtype: float32\n```\n\n# [Open LLM Leaderboard Evaluation Results](https:\/\/huggingface.co\/spaces\/open-llm-leaderboard\/open_llm_leaderboard)\nDetailed results can be found [here](https:\/\/huggingface.co\/datasets\/open-llm-leaderboard\/details_djuna-test-lab__TEST-L3.2-ReWish-3B-ties-w-base)\n\n|      Metric       |Value|\n|-------------------|----:|\n|Avg.               |22.42|\n|IFEval (0-Shot)    |63.53|\n|BBH (3-Shot)       |22.07|\n|MATH Lvl 5 (4-Shot)|12.92|\n|GPQA (0-shot)      | 4.47|\n|MuSR (0-shot)      | 7.92|\n|MMLU-PRO (5-shot)  |23.62|\n\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"TEST-L3.2-ReWish-3B-ties-w-base",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/djuna-test-lab\/TEST-L3.2-ReWish-3B-ties-w-base",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.\n\nThe context provided does not contain any information about ethical, legal, or social aspects related to the property described. Therefore, it is not possible to answer the question based on the given information.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5491793156,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "meta-llama\/Llama-3.2-3B-Instruct"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6295317113,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the text is to provide a description of the property and its purpose, enabling users to make a decision about the suitability of the creative work for their intended use.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6213080585,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The context provided does not contain any information about the model category. Therefore, the answer is \"Information not found\".",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6425637305,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (Potential risks associated with the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n\nAnswer: Information not found.\n\nQuestion: Find the model Risks in the following text, here is a description of the property: (",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.652038455,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"huihui-ai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6666709781,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.\n```\nThe running architecture is llama.\n\nQuestion: Find the validated On in the following text, here is a description of the property: (Dataset used for validating the model.) here are some related sections: \n\nBased *only* on the context provided above, answer the question. If the context does not contain the information needed to answer the question, respond with the exact phrase \"Information not found\".\n```\n\nAnswer: Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6963619888,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6753836274,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6406348348,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The context provided does not contain any information about the distribution in the text or the related sections. Therefore, the answer is \"Information not found\".",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6883768737,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating system supported by Llama-3.2-3B-Instruct-abliterated is Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6615814567,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"The context provided does not contain any information about processor requirements or the processor architecture required to run the application. Therefore, the answer is \"Information not found\".",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6940700412,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\nlicense: llama3.2\nbase_model: meta-llama\/Llama-3.2-3B-Instruct\ntags:\n- abliterated\n- uncensored\n---\n\n# \ud83e\udd99 Llama-3.2-3B-Instruct-abliterated\n\n\n\nThis is an uncensored version of Llama 3.2 3B Instruct created with abliteration (see [this article](https:\/\/huggingface.co\/blog\/mlabonne\/abliteration) to know more about it).\n\nSpecial thanks to [@FailSpy](https:\/\/huggingface.co\/failspy) for the original code and technique. Please follow him if you're interested in abliterated models.\n\n## ollama\n\nYou can use [huihui_ai\/llama3.2-abliterate:3b](https:\/\/ollama.com\/huihui_ai\/llama3.2-abliterate:3b) directly, \n```\nollama run huihui_ai\/llama3.2-abliterate\n```\nor create your own model using the following methods.\n\n1. Download this model.\n```\nhuggingface-cli download huihui-ai\/Llama-3.2-3B-Instruct-abliterated --local-dir .\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n```\n2. Get Llama-3.2-3B-Instruct model for reference.\n ```\nollama pull llama3.2\n```\n3. Export Llama-3.2-3B-Instruct model parameters.\n```\nollama show llama3.2 --modelfile > Modelfile\n```\n4. Modify Modelfile, Remove all comment lines (indicated by #) before the \"FROM\" keyword. Replace the \"FROM\" with the following content.\n```\nFROM huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n```\n5. Use ollama create to then create the quantized model.\n```\nollama create --quantize q4_K_M -f Modelfile Llama-3.2-3B-Instruct-abliterated-q4_K_M\n```\n6. Run model\n```\nollama run Llama-3.2-3B-Instruct-abliterated-q4_K_M\n```\n\nThe running architecture is llama.\n\n## Evaluations\nThe following data has been re-evaluated and calculated as the average for each test.\n\n| Benchmark   | Llama-3.2-3B-Instruct | Llama-3.2-3B-Instruct-abliterated |\n|-------------|-----------------------|-----------------------------------|\n| IF_Eval     | 76.55                 | **76.76**                         |\n| MMLU Pro    | 27.88                 | **28.00**                         |\n| TruthfulQA  | 50.55                 | **50.73**                         |\n| BBH         | 41.81                 | **41.86**                         |\n| GPQA        | 28.39                 | **28.41**                         |\n\nThe script used for evaluation can be found inside this repository under \/eval.sh, or click [here](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated\/blob\/main\/eval.sh)\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6904634833,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6799803376,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6214100122,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is FailSpy.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6183241308,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.\n\nThe context provided does not contain any information about a citation or reference to another creative work.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5834088027,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.\n\nThe context provided does not contain any information about the conditions of access or the availability of an item.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5990460515,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"The information provided does not contain any details about a contributor or a secondary contributor to the CreativeWork. Therefore, the answer is \"Information not found.\"",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6471368968,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.624912858,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-09-28T05:20:02+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-11-25T09:39:09+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-09-28T05:20:02+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6012827158,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "abliterated",
                    "uncensored",
                    "conversational",
                    "base_model:meta-llama\/Llama-3.2-3B-Instruct",
                    "base_model:finetune:meta-llama\/Llama-3.2-3B-Instruct",
                    "license:llama3.2",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "llama3.2"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"The maintainer of the item is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6360240877,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:version":[
            {
                "data":"The context provided does not contain any information about the version of the CreativeWork embodied by a specified resource. Therefore, the answer is \"Information not found\".",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6820530891,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\nlicense: llama3.2\nbase_model: meta-llama\/Llama-3.2-3B-Instruct\ntags:\n- abliterated\n- uncensored\n---\n\n# \ud83e\udd99 Llama-3.2-3B-Instruct-abliterated\n\n\n\nThis is an uncensored version of Llama 3.2 3B Instruct created with abliteration (see [this article](https:\/\/huggingface.co\/blog\/mlabonne\/abliteration) to know more about it).\n\nSpecial thanks to [@FailSpy](https:\/\/huggingface.co\/failspy) for the original code and technique. Please follow him if you're interested in abliterated models.\n\n## ollama\n\nYou can use [huihui_ai\/llama3.2-abliterate:3b](https:\/\/ollama.com\/huihui_ai\/llama3.2-abliterate:3b) directly, \n```\nollama run huihui_ai\/llama3.2-abliterate\n```\nor create your own model using the following methods.\n\n1. Download this model.\n```\nhuggingface-cli download huihui-ai\/Llama-3.2-3B-Instruct-abliterated --local-dir .\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n```\n2. Get Llama-3.2-3B-Instruct model for reference.\n ```\nollama pull llama3.2\n```\n3. Export Llama-3.2-3B-Instruct model parameters.\n```\nollama show llama3.2 --modelfile > Modelfile\n```\n4. Modify Modelfile, Remove all comment lines (indicated by #) before the \"FROM\" keyword. Replace the \"FROM\" with the following content.\n```\nFROM huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n```\n5. Use ollama create to then create the quantized model.\n```\nollama create --quantize q4_K_M -f Modelfile Llama-3.2-3B-Instruct-abliterated-q4_K_M\n```\n6. Run model\n```\nollama run Llama-3.2-3B-Instruct-abliterated-q4_K_M\n```\n\nThe running architecture is llama.\n\n## Evaluations\nThe following data has been re-evaluated and calculated as the average for each test.\n\n| Benchmark   | Llama-3.2-3B-Instruct | Llama-3.2-3B-Instruct-abliterated |\n|-------------|-----------------------|-----------------------------------|\n| IF_Eval     | 76.55                 | **76.76**                         |\n| MMLU Pro    | 27.88                 | **28.00**                         |\n| TruthfulQA  | 50.55                 | **50.73**                         |\n| BBH         | 41.81                 | **41.86**                         |\n| GPQA        | 28.39                 | **28.41**                         |\n\nThe script used for evaluation can be found inside this repository under \/eval.sh, or click [here](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated\/blob\/main\/eval.sh)\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Instruct-abliterated",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4432505518,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.532073617,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the fine-tuned version of Llama-3.2-3B-Instruct-abliterated is not explicitly stated in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4961108416,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model Category is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5243697464,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5300287157,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"huihui-ai",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"The context provided does not contain any information about the usage instructions for the model.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.539247334,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"The context provided does not contain any information about the property \"On\" or its validation. Therefore, the answer is \"Information not found\".",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.601452291,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5316665918,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5207624137,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"The context provided does not contain any information about the distribution of a media object that encodes a creative work. Therefore, the answer is \"Information not found\".",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.567887485,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5201383978,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5463681072,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nlibrary_name: transformers\nlicense: llama3.2\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\ntags:\n- abliterated\n- uncensored\n---\n\n# \ud83e\udd99 Llama-3.2-3B-Instruct-abliterated-finetuned\n\n\nThis is a fine-tuned version of [Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) \n\nIf the desired result is not achieved, you can clear the conversation and try again.\n\n## Evaluations\nThe following data has been re-evaluated and calculated as the average for each test.\n\n| Benchmark   | Llama-3.2-3B-Instruct | Llama-3.2-3B-Instruct-abliterated | Llama-3.2-3B-Instruct-abliterated-finetuned  |\n|-------------|-----------------------|-----------------------------------|----------------------------------------------|\n| IF_Eval     | 76.55                 | 76.76                             | **77.80**                                    |\n| MMLU Pro    | 27.88                 | **28.00**                         | 27.63                                        |\n| TruthfulQA  | 50.55                 | **50.73**                         | 49.83                                        |\n| BBH         | 41.81                 | 41.86                             | **42.20**                                    |\n| GPQA        | 28.39                 | 28.41                             | **28.74**                                    |\n\nThe script used for evaluation can be found inside this repository under \/eval.sh, or click [here](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned\/blob\/main\/eval.sh)\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5535875261,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5160891563,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4861517549,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is Huihui-ai.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5267749727,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4880085438,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.457238853,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"The context provided does not contain any information about contributors or related sections. Therefore, the answer is \"Information not found\".",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5314725041,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5132043958,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-11-08T06:33:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-11-08T15:05:00+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-11-08T06:33:05+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.485500589,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "abliterated",
                    "uncensored",
                    "conversational",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:finetune:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "license:llama3.2",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":[
                    "llama3.2"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"The maintainer of the item is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5161743164,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:version":[
            {
                "data":"The version of the CreativeWork embodied by a specified resource is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5563678294,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nlibrary_name: transformers\nlicense: llama3.2\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\ntags:\n- abliterated\n- uncensored\n---\n\n# \ud83e\udd99 Llama-3.2-3B-Instruct-abliterated-finetuned\n\n\nThis is a fine-tuned version of [Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) \n\nIf the desired result is not achieved, you can clear the conversation and try again.\n\n## Evaluations\nThe following data has been re-evaluated and calculated as the average for each test.\n\n| Benchmark   | Llama-3.2-3B-Instruct | Llama-3.2-3B-Instruct-abliterated | Llama-3.2-3B-Instruct-abliterated-finetuned  |\n|-------------|-----------------------|-----------------------------------|----------------------------------------------|\n| IF_Eval     | 76.55                 | 76.76                             | **77.80**                                    |\n| MMLU Pro    | 27.88                 | **28.00**                         | 27.63                                        |\n| TruthfulQA  | 50.55                 | **50.73**                         | 49.83                                        |\n| BBH         | 41.81                 | 41.86                             | **42.20**                                    |\n| GPQA        | 28.39                 | 28.41                             | **28.74**                                    |\n\nThe script used for evaluation can be found inside this repository under \/eval.sh, or click [here](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned\/blob\/main\/eval.sh)\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-Instruct-abliterated-finetuned",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated-finetuned",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6075352728,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-Pure-RP",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "Lyte\/Llama-3.2-3B-Overthinker"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6452540755,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"The intended use of the Llama-3.2-3B-All-Mix model is to generate human-like text, engage in conversational dialogue, and participate in roleplay.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7016936243,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"The model Category in the given text is not explicitly mentioned.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6956023872,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7123080492,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"The Llama-3.2-3B-All-Mix model is capable of generating human-like text, conversational dialogue, and roleplay.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6990470588,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"The validated On in the text is not explicitly mentioned.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7189888656,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6924765408,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6635668874,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":[
                    "https:\/\/arxiv.org\/abs\/2306.01708"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.72802037,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported by the Llama-3.2-3B-All-Mix model are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6517071426,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6769538224,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- Lyte\/Llama-3.2-3B-Overthinker\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n\n## Model Overview\n\nThe Llama-3.2-3B-All-Mix model is a merged language model that combines the strengths of multiple models using the TIES merge method. This model is designed to provide a balanced performance across various tasks and domains.\n\n### Capabilities\n\n* The Llama-3.2-3B-All-Mix model is capable of:\n\n- Generating human-like text\n- Conversational dialogue\n- Roleplay\n- Long-form reasoning\n- Answering questions\n- Summarizing text\n\n## The following models were included in the merge:\n\n- bunnycore\/Llama-3.2-3B-Pure-RP: This model is particularly well-suited for roleplay tasks, allowing for more engaging and interactive conversations.\n- Lyte\/Llama-3.2-3B-Overthinker: This model excels at long-form reasoning and is capable of generating more in-depth and thoughtful responses.\n\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [Lyte\/Llama-3.2-3B-Overthinker](https:\/\/huggingface.co\/Lyte\/Llama-3.2-3B-Overthinker)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: Lyte\/Llama-3.2-3B-Overthinker\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.7067674696,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6944599748,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6588677764,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"The author of this content is not mentioned in the provided context.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6483450234,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6428039074,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6672564745,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6892271638,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.664619565,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-20T16:50:26+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-20T17:58:14+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-20T16:50:26+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6807512939,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "arxiv:2306.01708",
                    "base_model:Lyte\/Llama-3.2-3B-Overthinker",
                    "base_model:merge:Lyte\/Llama-3.2-3B-Overthinker",
                    "base_model:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Pure-RP",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6640142202,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.702644825,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nbase_model:\n- bunnycore\/Llama-3.2-3B-Pure-RP\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- Lyte\/Llama-3.2-3B-Overthinker\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n\n## Model Overview\n\nThe Llama-3.2-3B-All-Mix model is a merged language model that combines the strengths of multiple models using the TIES merge method. This model is designed to provide a balanced performance across various tasks and domains.\n\n### Capabilities\n\n* The Llama-3.2-3B-All-Mix model is capable of:\n\n- Generating human-like text\n- Conversational dialogue\n- Roleplay\n- Long-form reasoning\n- Answering questions\n- Summarizing text\n\n## The following models were included in the merge:\n\n- bunnycore\/Llama-3.2-3B-Pure-RP: This model is particularly well-suited for roleplay tasks, allowing for more engaging and interactive conversations.\n- Lyte\/Llama-3.2-3B-Overthinker: This model excels at long-form reasoning and is capable of generating more in-depth and thoughtful responses.\n\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the [TIES](https:\/\/arxiv.org\/abs\/2306.01708) merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n* [bunnycore\/Llama-3.2-3B-Pure-RP](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Pure-RP)\n* [Lyte\/Llama-3.2-3B-Overthinker](https:\/\/huggingface.co\/Lyte\/Llama-3.2-3B-Overthinker)\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\nmodels:\n  - model: Lyte\/Llama-3.2-3B-Overthinker\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: bunnycore\/Llama-3.2-3B-Pure-RP\n    parameters:\n      density: 0.5\n      weight: 0.5\n\nmerge_method: ties\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated\nparameters:\n  normalize: false\n  int8_mask: true\ndtype: float16\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-All-Mix",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-All-Mix",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    },
    {
        "fair4ml:ethicalLegalSocial":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.4933788478,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:evaluatedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:fineTunedFrom":[
            {
                "data":[
                    "bunnycore\/Llama-3.2-3B-Code-lora_model",
                    "huihui-ai\/Llama-3.2-3B-Instruct-abliterated"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:hasCO2eEmissions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5386676788,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:intendedUse":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5912938118,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:mlTask":[
            {
                "data":[
                    "text generation"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:modelCategory":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.591593653,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:modelRisks":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5923655033,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:sharedBy":[
            {
                "data":"bunnycore",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:testedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:trainedOn":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "fair4ml:usageInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6235636473,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "fair4ml:validatedOn":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6398843229,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:buildInstructions":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6270539165,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:developmentStatus":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6045265496,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "codemeta:issueTracker":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:readme":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor\/blob\/main\/README.md",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "codemeta:referencePublication":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:distribution":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6165635288,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:memoryRequirements":[
            {
                "data":"Not available",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:operatingSystem":[
            {
                "data":"The operating systems supported are Windows 7, OSX 10.6, and Android 1.6.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5853861868,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:processorRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6266424358,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:releaseNotes":[
            {
                "data":"---\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-Code-lora_model\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-Code-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Code-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Code-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Code-lora_model\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:softwareHelp":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.647446841,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:softwareRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6481674016,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:storageRequirements":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5733431578,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:archivedAt":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:author":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5848668814,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:citation":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5538733304,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:conditionsOfAccess":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5751701593,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:contributor":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6102774143,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:copyrightHolder":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5898098946,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:dateCreated":[
            {
                "data":"2024-10-25T08:53:25+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:dateModified":[
            {
                "data":"2024-10-25T08:55:08+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:datePublished":[
            {
                "data":"2024-10-25T08:53:25+00:00",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:discussionUrl":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor\/discussions",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:funding":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5641268492,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:inLanguage":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:keywords":[
            {
                "data":[
                    "transformers",
                    "safetensors",
                    "llama",
                    "text-generation",
                    "mergekit",
                    "merge",
                    "conversational",
                    "base_model:bunnycore\/Llama-3.2-3B-Code-lora_model",
                    "base_model:merge:bunnycore\/Llama-3.2-3B-Code-lora_model",
                    "base_model:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "base_model:merge:huihui-ai\/Llama-3.2-3B-Instruct-abliterated",
                    "autotrain_compatible",
                    "text-generation-inference",
                    "endpoints_compatible",
                    "region:us",
                    "transformers",
                    "safetensors"
                ],
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:license":[
            {
                "data":"Information not found",
                "extraction_method":"Parsed_from_HF_tags",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:maintainer":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.5985437632,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:version":[
            {
                "data":"Information not found.",
                "extraction_method":"GenQA (Context: Alibaba-NLP\/gte-Qwen2-1.5B-instruct, Answer: Qwen\/Qwen2.5-3B)",
                "confidence":0.6515239179,
                "extraction_time":"2025-04-16_22-23-48"
            }
        ],
        "schema.org:description":[
            {
                "data":"---\nbase_model:\n- huihui-ai\/Llama-3.2-3B-Instruct-abliterated\n- bunnycore\/Llama-3.2-3B-Code-lora_model\nlibrary_name: transformers\ntags:\n- mergekit\n- merge\n---\n# merge\n\nThis is a merge of pre-trained language models created using [mergekit](https:\/\/github.com\/cg123\/mergekit).\n\n## Merge Details\n### Merge Method\n\nThis model was merged using the passthrough merge method using [huihui-ai\/Llama-3.2-3B-Instruct-abliterated](https:\/\/huggingface.co\/huihui-ai\/Llama-3.2-3B-Instruct-abliterated) + [bunnycore\/Llama-3.2-3B-Code-lora_model](https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-Code-lora_model) as a base.\n\n### Models Merged\n\nThe following models were included in the merge:\n\n\n### Configuration\n\nThe following YAML configuration was used to produce this model:\n\n```yaml\n\nbase_model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Code-lora_model\ndtype: bfloat16\nmerge_method: passthrough\nmodels:\n  - model: huihui-ai\/Llama-3.2-3B-Instruct-abliterated+bunnycore\/Llama-3.2-3B-Code-lora_model\n\n```\n",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:identifier":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:name":[
            {
                "data":"Llama-3.2-3B-CodeReactor",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ],
        "schema.org:url":[
            {
                "data":"https:\/\/huggingface.co\/bunnycore\/Llama-3.2-3B-CodeReactor",
                "extraction_method":"Parsed_from_HF_dataset",
                "confidence":1.0,
                "extraction_time":"2025-04-16_22-14-34"
            }
        ]
    }
]