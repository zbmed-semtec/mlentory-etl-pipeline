version: "3.8" 

services:
  
  # Airflow Scheduler
  scheduler:
    container_name: airflow_scheduler
    profiles:
      # - gpu
      - no_gpu
      - openml_no_gpu
      - airflow_test
    image: apache/airflow:2.10.1  # Use the latest stable Airflow image
    command: >
      bash -c "airflow db init &&
               airflow db check &&
               airflow users create -r Admin -u admin -e admin@admin.com -f admin -l admin -p admin &&
               airflow scheduler"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ''  # Replace with a generated Fernet key
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@airflow_postgres:5442/airflow'
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True'
    depends_on:
      - airflow_postgres
    volumes:
      - ./scheduler/dags:/opt/airflow/dags  # Folder for your DAGs
      - ./scheduler/logs:/opt/airflow/logs  # Folder for Airflow logs
      - ./scheduler/plugins:/opt/airflow/plugins  # Optional plugins folder
      - ./scheduler/scripts:/opt/airflow/scripts
      - //var/run/docker.sock:/var/run/docker.sock 
    ports:
      - "8794:8793"
    networks:
      - mlentory_network
    restart: always

  # Airflow Webserver (for UI)
  webserver:
    container_name: airflow_webserver
    profiles:
      # - gpu
      - no_gpu
      - openml_no_gpu
      - airflow_test
    image: apache/airflow:2.10.1
    command: "webserver"
    # entrypoint: ./scripts/airflow-entrypoint.sh
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ''  # Same key as scheduler
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@airflow_postgres:5442/airflow'
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True'
    depends_on:
      - scheduler
      - airflow_postgres
    volumes:
      - ./scheduler/dags:/opt/airflow/dags
      - ./scheduler/logs:/opt/airflow/logs
      - ./scheduler/plugins:/opt/airflow/plugins
      - ./scheduler/scripts:/opt/airflow/scripts
      - //var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    networks:
      - mlentory_network
    restart: always
  
  ### Airflow SQL database
  airflow_postgres:
    profiles:
      # - gpu
      - no_gpu
      - openml_no_gpu
      - airflow_test
    image: postgres:13-alpine
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - '5442:5442'
    command: -c config_file=/etc/postgresql/postgresql.conf
    volumes:
      - ./scheduler/scripts/postgresql.conf:/etc/postgresql/postgresql.conf
    #   - ../data/postgres_data:/var/lib/postgresql/data
    networks:
      - mlentory_network

  # Extractor service
  hf_etl_with_gpu:
    profiles: [gpu]
    container_name: hf_gpu
    build: 
      context: ../
      dockerfile: deployment/Dockerfile.gpu
      target: gpu_hf
    volumes:
      - ../data/configuration/hf:/app/configuration/hf
      - ../data/virtuoso_data/kg_files:/kg_files
      - ../code/extractors:/app/extractors
      - ../code/transform:/app/transform
      - ../code/load:/app/load
      - ./hf_etl:/app/hf_etl
      # - ./hf_etl/outputs/files:/app/outputs/files
      # Necessary to run docker commands inside other containers
      - /var/run/docker.sock:/var/run/docker.sock 
    environment:
      - HF_TOKEN=${HF_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    dns:
      - 1.1.1.1
    networks:
      - mlentory_network
    command: ["tail", "-f", "/dev/null"]

  hf_etl_with_no_gpu:
    profiles: [no_gpu]
    container_name: hf_no_gpu
    build: 
      context: ../
      dockerfile: deployment/Dockerfile.no_gpu
      target: no_gpu_hf
    volumes:
      - ../data/configuration/hf:/app/configuration/hf
      - ../data/transform_queue:/transform_queue
      - ../data/datasets:/datasets
      - ../code/extractors:/app/extractors  # Mount the entire package
      - ../code/transform:/app/transform
      - ../code/load:/app/load
      - ./hf_etl:/app/hf_etl
    dns:
      - 1.1.1.1
    command: ["tail", "-f", "/dev/null"]
    networks:
      - mlentory_network

  openml_etl_with_no_gpu:
    profiles: [no_gpu,openml_no_gpu]
    container_name: openml_no_gpu
    build: 
      context: ../
      dockerfile: deployment/Dockerfile.openml_no_gpu
      target: no_gpu_openml
    volumes:
      - ../data/configuration/openml:/app/configuration/openml
      - ../code/extractors:/app/extractors 
      - ../code/transform:/app/transform
      - ../code/load:/app/load
      - ./openml_etl:/app/openml_etl
      - /var/run/docker.sock:/var/run/docker.sock 
    dns:
      - 1.1.1.1
    command: ["tail", "-f", "/dev/null"]
    networks:
      - mlentory_network

  ### Module in charge of transforming the data to the RDA FAIR4ML schema
  # transform:
  #   profiles:
  #     - gpu
  #     - no_gpu
  #   container_name: transform
  #   build: 
  #     context: .
  #     dockerfile: ./transform/Dockerfile  # Path to extractor Dockerfile
  #   # Define volumes to mount extractor scripts or dependencies
  #   volumes:
  #     - ./transform:/app
  #     - ../data/transform_queue:/transform_queue
  #     - ../data/load_queue:/load_queue
  #     - ../data/datasets:/datasets
  #     - ./config_data:/config_data
  #   dns:
  #     - 1.1.1.1

  #   # Uncomment this line to test the script
  #   command: ["tail", "-f", "/dev/null"]
  #   # command: ["python3", "main.py"]
  #   networks:
  #     - mlentory_network
  #   # Uncomment for production
  #   #command: ["python3", "ExtractFromDataset.py"]

  ### Module in charge of loading the data into the database
  # load:    
  #   depends_on:
  #     - transform
  #     - postgres
  #     - virtuoso
  #     - elastic
  #   profiles:
  #     - gpu
  #     - no_gpu
  #   container_name: load
  #   build: 
  #     context: .
  #     dockerfile: ./load/Dockerfile  # Path to extractor Dockerfile
  #   # Define volumes to mount extractor scripts or dependencies
  #   volumes:
  #     - ./load:/app
  #     - ../data/load_queue:/load_queue
  #     - ../data/virtuoso_data/kg_files:/kg_files
  #     - ./config_data:/config_data
  #     # Necessary to run docker commands inside other containers
  #     - /var/run/docker.sock:/var/run/docker.sock 
  #   dns:
  #     - 1.1.1.1

  #   # Uncomment this line to test the script
  #   command: ["tail", "-f", "/dev/null"]
  #   # command: ["python3", "main.py"]
  #   networks:
  #     - mlentory_network

  ### SQL for storage database
  postgres:
    profiles:
        - gpu
        - no_gpu
        - openml_no_gpu
    image: postgres:14.1-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: history_DB
    ports:
      - '5432:5432'
    volumes:
      - ../data/configuration/hf/load/sql_files/init.sql:/docker-entrypoint-initdb.d/init.sql
      - ../data/postgres_data:/var/lib/postgresql/data
    networks:
      - mlentory_network

  ### Database visualization tool
  pgadmin:
    profiles:
      - gpu
      - no_gpu
      - openml_no_gpu
    container_name: pgadmin
    image: dpage/pgadmin4:latest
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: 'False'
    ports:
      - "5050:80"
    volumes:
      - ../data/pgadmin_data:/var/lib/pgadmin
    depends_on:
      - postgres
      # - airflow_postgres
    networks:
      - mlentory_network
    restart: always

  ### RDF database
  virtuoso:
    profiles:
      - gpu
      - no_gpu
      - openml_no_gpu
    image: openlink/virtuoso-opensource-7:latest
    hostname: virtuoso
    container_name: virtuoso_db
    ports:
      - "1111:1111"
      - "8890:8890"
    environment:
      DBA_PASSWORD: my_strong_password
    volumes:
      - ../data/virtuoso_data:/opt/virtuoso-opensource/database
    networks:
      - mlentory_network

  ### Component to index data based on specific paramaters in Elasticsearch
  elastic:
    profiles:
      - gpu
      - no_gpu
      - openml_no_gpu
    image: docker.elastic.co/elasticsearch/elasticsearch:8.18.2
    container_name: elastic_db
    hostname: elastic
    # privileged: true
    environment:
      - node.name=elastic
      - "transport.host=localhost"
      - "xpack.security.enabled=false"  # Disable security features for development, needs to be enabled for production
      - discovery.type=single-node  #  Use single-node discovery for development
      # - cluster.initial_master_nodes=elastic   # Use this for production
      - cluster.name=es-docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
      # - "bootstrap.system_call_filter=false"   # Is not required for version 8.0 and above
      # - node.max_local_storage_nodes=2
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ../data/elasticsearch_data:/var/lib/elasticsearch/data
    ports:
      - 9200:9200
      - 9300:9300
    networks:
      - mlentory_network

networks:
  mlentory_network:
    external: true
