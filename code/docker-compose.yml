version: "3.8" 

services:
  
  # Airflow Scheduler
  scheduler:
    container_name: airflow_scheduler
    profiles:
      - gpu
      - no_gpu
      - airflow_test
    image: apache/airflow:2.10.1  # Use the latest stable Airflow image
    command: >
      bash -c "airflow db init &&
               airflow db check &&
               airflow users create -r Admin -u admin -e admin@admin.com -f admin -l admin -p admin &&
               airflow scheduler"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ''  # Replace with a generated Fernet key
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@airflow_postgres:5442/airflow'
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True'
    depends_on:
      - airflow_postgres
    volumes:
      - ./scheduler/dags:/opt/airflow/dags  # Folder for your DAGs
      - ./scheduler/logs:/opt/airflow/logs  # Folder for Airflow logs
      - ./scheduler/plugins:/opt/airflow/plugins  # Optional plugins folder
      - ./scheduler/scripts:/opt/airflow/scripts
      - //var/run/docker.sock:/var/run/docker.sock 
    ports:
      - "8794:8793"
    networks:
      - mlentory_network
    restart: always

  # Airflow Webserver (for UI)
  webserver:
    container_name: airflow_webserver
    profiles:
      - gpu
      - no_gpu
      - airflow_test
    image: apache/airflow:2.10.1
    command: "webserver"
    # entrypoint: ./scripts/airflow-entrypoint.sh
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ''  # Same key as scheduler
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@airflow_postgres:5442/airflow'
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True'
    depends_on:
      - scheduler
      - airflow_postgres
    volumes:
      - ./scheduler/dags:/opt/airflow/dags
      - ./scheduler/logs:/opt/airflow/logs
      - ./scheduler/plugins:/opt/airflow/plugins
      - ./scheduler/scripts:/opt/airflow/scripts
      - //var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    networks:
      - mlentory_network
    restart: always
  
  ### Airflow SQL database
  airflow_postgres:
    profiles:
      - gpu
      - no_gpu
      - airflow_test
    image: postgres:13-alpine
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - '5442:5442'
    command: -c config_file=/etc/postgresql/postgresql.conf
    volumes:
      - ./scheduler/scripts/postgresql.conf:/etc/postgresql/postgresql.conf
    #   - ../data/postgres_data:/var/lib/postgresql/data
    networks:
      - mlentory_network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    profiles:
      - airflow_test
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    profiles:
      - airflow_test
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      # KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    # command: "bin/kafka-topics.sh --create --topic hf_events --bootstrap-server localhost:9092"
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"
    

  init_kafka:
    profiles:
      - airflow_test
    image: confluentinc/cp-kafka:7.3.2
    container_name: init_kafka
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    volumes:
      - ./extractors/hf_extractor:/app
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server kafka:9092 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic hf_topic --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka:9092 --list

      tail -f /dev/null
      "
  # Extractor service
  hf_extractor_with_gpu:
    profiles:
      - gpu
      - airflow_test
    depends_on:
      - kafka
    container_name: hf_gpu
    build: 
      context: .
      dockerfile: ./extractors/hf_extractor/Dockerfile.gpu  # Path to extractor Dockerfile
    # Define volumes to mount extractor scripts or dependencies
    volumes:
      - ./extractors/hf_extractor:/app
      - ../data/transform_queue:/transform_queue
      - ../data/datasets:/datasets
      - ./config_data:/config_data
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    dns:
      - 1.1.1.1
    # depends_on:
    #   - mysql
    # Uncomment this line to run the script
    # command: ["python3", "ExtractFromDataset.py"]
    # Uncomment this line to develop on the container
    command: ["tail", "-f", "/dev/null"]
    # networks:
    #   - mlentory_network
  
  hf_extractor_with_no_gpu:
    profiles: [no_gpu]
    container_name: hf_no_gpu
    build: 
      context: .
      dockerfile: ./extractors/hf_extractor/Dockerfile.no_gpu  # Path to extractor Dockerfile
    # Define volumes to mount extractor scripts or dependencies
    volumes:
      - ./extractors/hf_extractor:/app
      - ../data/transform_queue:/transform_queue
      - ../data/datasets:/datasets
      - ./config_data:/config_data
    dns:
      - 1.1.1.1
    # Uncomment this line to test the script
    # command: ["python3", "main.py"]
    # Uncomment this line to develop on the container
    command: ["tail", "-f", "/dev/null"]
    networks:
      - mlentory_network

  ### Module in charge of transforming the data to the RDA FAIR4ML schema
  transform:
    profiles:
      - gpu
      - no_gpu
    container_name: transform
    build: 
      context: .
      dockerfile: ./transform/Dockerfile  # Path to extractor Dockerfile
    # Define volumes to mount extractor scripts or dependencies
    volumes:
      - ./transform:/app
      - ../data/transform_queue:/transform_queue
      - ../data/load_queue:/load_queue
      - ../data/datasets:/datasets
      - ./config_data:/config_data
    dns:
      - 1.1.1.1

    # Uncomment this line to test the script
    command: ["tail", "-f", "/dev/null"]
    # command: ["python3", "main.py"]
    networks:
      - mlentory_network
    # Uncomment for production
    #command: ["python3", "ExtractFromDataset.py"]

  ### Module in charge of loading the data into the database
  load:    
    # depends_on:
      # - transform
      # - postgres
      # - virtuoso
      # - elastic
    profiles:
      - gpu
      - no_gpu
      - airflow_test
    container_name: load
    build: 
      context: .
      dockerfile: ./load/Dockerfile  # Path to extractor Dockerfile
    # Define volumes to mount extractor scripts or dependencies
    volumes:
      - ./load:/app
      - ../data/load_queue:/load_queue
      - ../data/virtuoso_data/kg_files:/kg_files
      - ./config_data:/config_data
      # Necessary to run docker commands inside other containers
      - /var/run/docker.sock:/var/run/docker.sock 
    dns:
      - 1.1.1.1

    # Uncomment this line to test the script
    command: ["tail", "-f", "/dev/null"]
    # command: ["python3", "main.py"]
    networks:
      - mlentory_network

  ### SQL for storage database
  postgres:
    profiles:
        - gpu
        - no_gpu
    image: postgres:14.1-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: history_DB
    ports:
      - '5432:5432'
    volumes:
      - ./load/sql_files/init.sql:/docker-entrypoint-initdb.d/init.sql
      - ../data/postgres_data:/var/lib/postgresql/data
    networks:
      - mlentory_network

  # anzograph:
  #   image: cambridgesemantics/anzograph:latest
  #   ports:
  #     - "8085:8080"
  #     - "8443:8443"
  #   volumes:
  #     - /DB/anzograph-data:/opt/anzograph-data

  ### RDF database
  virtuoso:
    profiles:
      - gpu
      - no_gpu
    image: openlink/virtuoso-opensource-7:latest
    hostname: virtuoso
    container_name: virtuoso
    ports:
      - "1111:1111"
      - "8890:8890"
    environment:
      DBA_PASSWORD: my_strong_password
    volumes:
      - ../data/virtuoso_data:/opt/virtuoso-opensource/database
    networks:
      - mlentory_network

  ### Component to index data based on specific paramaters in Elasticsearch
  elastic:
    profiles:
      - gpu
      - no_gpu
    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.0
    container_name: elastic
    hostname: elastic
    # privileged: true
    environment:
      - node.name=elastic
      - "transport.host=localhost"
      - "bootstrap.system_call_filter=false"
      # - discovery.type=single-node
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=elastic
      - bootstrap.memory_lock=true
      # - node.max_local_storage_nodes=2
      - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ../data/elasticsearch_data:/var/lib/elasticsearch/data
    ports:
      - 9200:9200
      - 9300:9300
    networks:
      - mlentory_network

# networks:
#   my-macvlan-net:
#     external: true
#     name: my-macvlan-net
#   host:
#     external: true

networks:
  mlentory_network:
    external: true

