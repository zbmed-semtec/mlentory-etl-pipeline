	fair4ml:ethicalLegalSocial	fair4ml:evaluatedOn	fair4ml:fineTunedFrom	fair4ml:hasCO2eEmissions	fair4ml:intendedUse	fair4ml:mlTask	fair4ml:modelCategory	fair4ml:modelRisks	fair4ml:sharedBy	fair4ml:testedOn	fair4ml:trainedOn	fair4ml:usageInstructions	fair4ml:validatedOn	distribution	memoryRequirements	operatingSystem	processorRequirements	releaseNotes	softwareHelp	softwareRequirements	storageRequirements	codemeta:buildInstructions	codemeta:developmentStatus	codemeta:issueTracker	codemeta:readme	codemeta:referencePublication	archivedAt	author	citation	conditionsOfAccess	contributor	copyrightHolder	dateCreated	dateModified	datePublished	discussionUrl	funding	inLanguage	isAccessibleForFree	keywords	license	maintainer	version	description	identifier	name	url
0	[{'data': ['bsd-3-clause'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'AudioSet classes', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.00013806817878503352}]	[{'data': 'Audio Spectrogram Transformer', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.00011329610424581915}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'classifying audio into one of the AudioSet classes', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.2491736114025116}]	[{'data': ['audio classification'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'several audio classification benchmarks', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.0608644060994266e-06}]	[{'data': 'MIT', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'classifying audio into one of the AudioSet classes', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0030048019252717495}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '0.693 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'audio', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.6751174891638243e-09}]	[{'data': '---\nlicense: bsd-3-clause\ntags:\n- audio-classification\n---\n\n# Audio Spectrogram Transformer (fine-tuned on AudioSet) \n\nAudio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [this repository](https://github.com/YuanGongND/ast). \n\nDisclaimer: The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Audio Spectrogram Transformer is equivalent to [ViT](https://huggingface.co/docs/transformers/model_doc/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.\n\n## Usage\n\nYou can use the raw model for classifying audio into one of the AudioSet classes. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		"[[{'data': ['transformers', 'pytorch', 'safetensors'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '0.693 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': ['2104.01778'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Gong et al.', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.19026751816272736}]					[{'data': '2022-11-14 18:41:48+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2023-09-06 14:49:15+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-11-14 18:41:48+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'The team releasing Audio Spectrogram Transformer did not write a model card', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 2.589032419564319e-07}]				[{'data': ['bsd-3-clause'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'MIT', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'Audio Spectrogram Transformer', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 3.1166209168986825e-07}]		"[""{'data': 'https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'MIT/ast-finetuned-audioset-10-10-0.4593', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
1	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'pre-existing image datasets', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.04492053762078285}]	[{'data': 'ViT-L/14 Transformer architecture', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.3469974100589752}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'to evaluate performance of the model across people and surface potential risks', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.27878957986831665}]	[{'data': ['zero shot image classification'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'surface potential risks', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.09648594260215759}]	[{'data': 'openai', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'The model card is taken and modified from the official CLIP repository', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.025931837037205696}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/openai/clip-vit-large-patch14', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '6.847 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'to evaluate performance of the model across people and surface potential risks', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.011482571251690388}]	"[{'data': '---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(""openai/clip-vit-large-patch14"")\nprocessor = CLIPProcessor.from_pretrained(""openai/clip-vit-large-patch14"")\n\nurl = ""http://images.cocodataset.org/val2017/000000039769.jpg""\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[""a photo of a cat"", ""a photo of a dog""], images=image, return_tensors=""pt"", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['transformers', 'pytorch', 'jax', 'safetensors'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '6.847 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/openai/clip-vit-large-patch14/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/openai/clip-vit-large-patch14/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': ['2103.00020', '1908.04913'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'researchers at OpenAI', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0020058858208358288}]					[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2023-09-15 15:49:35+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/openai/clip-vit-large-patch14/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'The model is intended as a research output for research communities', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 3.6503258797893068e-06}]				[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'openai', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'gle/Uv7afRH5dvY34ZEs9', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.6580563169554807e-05}]		"[""{'data': 'https://huggingface.co/openai/clip-vit-large-patch14', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'openai/clip-vit-large-patch14', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/openai/clip-vit-large-patch14', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
2	[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'The model can be used directly (without a language model', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.010890268720686436}]	[{'data': 'XLSR-53', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0021992120891809464}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'speech recognition', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.16052381694316864}]	[{'data': ['automatic speech recognition'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'english --dataset mozilla-foundation', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 3.051673229492735e-05}]	[{'data': 'jonatasgrosman', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['common_voice', 'mozilla-foundation/common_voice_6_0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['common_voice', 'mozilla-foundation/common_voice_6_0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'directly', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.2019733041524887}]	[{'data': ['common_voice', 'mozilla-foundation/common_voice_6_0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '4.656 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Wav2Vec2Processor', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.5176950693130493}]	"[{'data': '---\nlanguage: en\ndatasets:\n- common_voice\n- mozilla-foundation/common_voice_6_0\nmetrics:\n- wer\n- cer\ntags:\n- audio\n- automatic-speech-recognition\n- en\n- hf-asr-leaderboard\n- mozilla-foundation/common_voice_6_0\n- robust-speech-event\n- speech\n- xlsr-fine-tuning-week\nlicense: apache-2.0\nmodel-index:\n- name: XLSR Wav2Vec2 English by Jonatas Grosman\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice en\n      type: common_voice\n      args: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 19.06\n    - name: Test CER\n      type: cer\n      value: 7.69\n    - name: Test WER (+LM)\n      type: wer\n      value: 14.81\n    - name: Test CER (+LM)\n      type: cer\n      value: 6.84\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Robust Speech Event - Dev Data\n      type: speech-recognition-community-v2/dev_data\n      args: en\n    metrics:\n    - name: Dev WER\n      type: wer\n      value: 27.72\n    - name: Dev CER\n      type: cer\n      value: 11.65\n    - name: Dev WER (+LM)\n      type: wer\n      value: 20.85\n    - name: Dev CER (+LM)\n      type: cer\n      value: 11.01\n---\n\n# Fine-tuned XLSR-53 large model for speech recognition in English\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on English using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(""jonatasgrosman/wav2vec2-large-xlsr-53-english"")\naudio_paths = [""/path/to/file.mp3"", ""/path/to/another_file.wav""]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = ""en""\nMODEL_ID = ""jonatasgrosman/wav2vec2-large-xlsr-53-english""\nSAMPLES = 10\n\ntest_dataset = load_dataset(""common_voice"", LANG_ID, split=f""test[:{SAMPLES}]"")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[""path""], sr=16_000)\n    batch[""speech""] = speech_array\n    batch[""sentence""] = batch[""sentence""].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[""speech""], sampling_rate=16_000, return_tensors=""pt"", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(""-"" * 100)\n    print(""Reference:"", test_dataset[i][""sentence""])\n    print(""Prediction:"", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| ""SHE\'LL BE ALL RIGHT."" | SHE\'LL BE ALL RIGHT |\n| SIX | SIX |\n| ""ALL\'S WELL THAT ENDS WELL."" | ALL AS WELL THAT ENDS WELL |\n| DO YOU MEAN IT? | DO YOU MEAN IT |\n| THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE, BUT STILL CAUSES REGRESSIONS. | THE NEW PATCH IS LESS INVASIVE THAN THE OLD ONE BUT STILL CAUSES REGRESSION |\n| HOW IS MOZILLA GOING TO HANDLE AMBIGUITIES LIKE QUEUE AND CUE? | HOW IS MOSLILLAR GOING TO HANDLE ANDBEWOOTH HIS LIKE Q AND Q |\n| ""I GUESS YOU MUST THINK I\'M KINDA BATTY."" | RUSTIAN WASTIN PAN ONTE BATTLY |\n| NO ONE NEAR THE REMOTE MACHINE YOU COULD RING? | NO ONE NEAR THE REMOTE MACHINE YOU COULD RING |\n| SAUCE FOR THE GOOSE IS SAUCE FOR THE GANDER. | SAUCE FOR THE GUICE IS SAUCE FOR THE GONDER |\n| GROVES STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD. | GRAFS STARTED WRITING SONGS WHEN SHE WAS FOUR YEARS OLD |\n\n## Evaluation\n\n1. To evaluate on `mozilla-foundation/common_voice_6_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-english --dataset mozilla-foundation/common_voice_6_0 --config en --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-english --dataset speech-recognition-community-v2/dev_data --config en --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-english,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {E}nglish},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english}},\n  year={2021}\n}\n```', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['transformers', 'pytorch', 'jax', 'safetensors'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '4.656 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Grosman, Jonatas', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.7634411454200745}]					[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2023-03-25 10:56:55+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'fine-tuned', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.4844864381302614e-05}]				[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'jonatasgrosman', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2021', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.5561656255158596e-05}]		"[""{'data': 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'jonatasgrosman/wav2vec2-large-xlsr-53-english', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
3	[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '[CLS]', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.2166174054145813}]	[{'data': '[CLS]', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.22914648056030273}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'to\nbe fine-tuned on a downstream task', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.26473501324653625}]	[{'data': ['fill mask'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': '[CLS]', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.20580631494522095}]	[{'data': 'google-bert', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['bookcorpus', 'wikipedia'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['bookcorpus', 'wikipedia'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '[CLS]', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.20483922958374023}]	[{'data': ['bookcorpus', 'wikipedia'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/google-bert/bert-base-uncased', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '3.454 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': '[CLS]', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.22136282920837402}]	"[{'data': '---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it\'s mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(\'fill-mask\', model=\'bert-base-uncased\')\n>>> unmasker(""Hello I\'m a [MASK] model."")\n\n[{\'sequence\': ""[CLS] hello i\'m a fashion model. [SEP]"",\n  \'score\': 0.1073106899857521,\n  \'token\': 4827,\n  \'token_str\': \'fashion\'},\n {\'sequence\': ""[CLS] hello i\'m a role model. [SEP]"",\n  \'score\': 0.08774490654468536,\n  \'token\': 2535,\n  \'token_str\': \'role\'},\n {\'sequence\': ""[CLS] hello i\'m a new model. [SEP]"",\n  \'score\': 0.05338378623127937,\n  \'token\': 2047,\n  \'token_str\': \'new\'},\n {\'sequence\': ""[CLS] hello i\'m a super model. [SEP]"",\n  \'score\': 0.04667217284440994,\n  \'token\': 3565,\n  \'token_str\': \'super\'},\n {\'sequence\': ""[CLS] hello i\'m a fine model. [SEP]"",\n  \'score\': 0.027095865458250046,\n  \'token\': 2986,\n  \'token_str\': \'fine\'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\nmodel = BertModel.from_pretrained(""bert-base-uncased"")\ntext = ""Replace me by any text you\'d like.""\nencoded_input = tokenizer(text, return_tensors=\'pt\')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\nmodel = TFBertModel.from_pretrained(""bert-base-uncased"")\ntext = ""Replace me by any text you\'d like.""\nencoded_input = tokenizer(text, return_tensors=\'tf\')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(\'fill-mask\', model=\'bert-base-uncased\')\n>>> unmasker(""The man worked as a [MASK]."")\n\n[{\'sequence\': \'[CLS] the man worked as a carpenter. [SEP]\',\n  \'score\': 0.09747550636529922,\n  \'token\': 10533,\n  \'token_str\': \'carpenter\'},\n {\'sequence\': \'[CLS] the man worked as a waiter. [SEP]\',\n  \'score\': 0.0523831807076931,\n  \'token\': 15610,\n  \'token_str\': \'waiter\'},\n {\'sequence\': \'[CLS] the man worked as a barber. [SEP]\',\n  \'score\': 0.04962705448269844,\n  \'token\': 13362,\n  \'token_str\': \'barber\'},\n {\'sequence\': \'[CLS] the man worked as a mechanic. [SEP]\',\n  \'score\': 0.03788609802722931,\n  \'token\': 15893,\n  \'token_str\': \'mechanic\'},\n {\'sequence\': \'[CLS] the man worked as a salesman. [SEP]\',\n  \'score\': 0.037680890411138535,\n  \'token\': 18968,\n  \'token_str\': \'salesman\'}]\n\n>>> unmasker(""The woman worked as a [MASK]."")\n\n[{\'sequence\': \'[CLS] the woman worked as a nurse. [SEP]\',\n  \'score\': 0.21981462836265564,\n  \'token\': 6821,\n  \'token_str\': \'nurse\'},\n {\'sequence\': \'[CLS] the woman worked as a waitress. [SEP]\',\n  \'score\': 0.1597415804862976,\n  \'token\': 13877,\n  \'token_str\': \'waitress\'},\n {\'sequence\': \'[CLS] the woman worked as a maid. [SEP]\',\n  \'score\': 0.1154729500412941,\n  \'token\': 10850,\n  \'token_str\': \'maid\'},\n {\'sequence\': \'[CLS] the woman worked as a prostitute. [SEP]\',\n  \'score\': 0.037968918681144714,\n  \'token\': 19215,\n  \'token_str\': \'prostitute\'},\n {\'sequence\': \'[CLS] the woman worked as a cook. [SEP]\',\n  \'score\': 0.03042375110089779,\n  \'token\': 5660,\n  \'token_str\': \'cook\'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it\'s another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n""sentences"" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=""https://huggingface.co/exbert/?model=bert-base-uncased"">\n\t<img width=""300px"" src=""https://cdn-media.huggingface.co/exbert/button.png"">\n</a>\n', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['transformers', 'pytorch', 'jax', 'rust', 'onnx', 'safetensors'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '3.454 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/google-bert/bert-base-uncased/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/google-bert/bert-base-uncased/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': ['1810.04805'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.3222407102584839}]					[{'data': '2022-03-02 23:29:04+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2024-02-19 11:06:12+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:04+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/google-bert/bert-base-uncased/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '[CLS]', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.1951427459716797}]				[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'google-bert', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '[CLS]', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.22659842669963837}]		"[""{'data': 'https://huggingface.co/google-bert/bert-base-uncased', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'google-bert/bert-base-uncased', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/google-bert/bert-base-uncased', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
4	[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'ARZTB', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.10085676610469818}]	[{'data': 'CAMeLBERT-Mix POS-EGY', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0018236819887533784}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'as part of the transformers pipeline', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.34849315881729126}]	[{'data': ['token classification'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'variant proximity of pre-training data to fine-tuning data is more important', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.053566377609968185}]	[{'data': 'CAMeL-Lab', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'scaled-down set of the MSA variant', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.09544355422258377}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '1.742 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'transformers pipeline', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.6239614524238277e-06}]	"[{'data': '---\nlanguage: \n- ar\nlicense: apache-2.0\nwidget:\n - text: \'عامل ايه ؟\'\n---\n# CAMeLBERT-Mix POS-EGY Model\n## Model description\n**CAMeLBERT-Mix POS-EGY Model** is a Egyptian Arabic POS tagging model that was built by fine-tuning the [CAMeLBERT-Mix](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix/) model.\nFor the fine-tuning, we used the ARZTB dataset .\nOur fine-tuning procedure and the hyperparameters we used can be found in our paper *""[The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://arxiv.org/abs/2103.06678).""* Our fine-tuning code can be found [here](https://github.com/CAMeL-Lab/CAMeLBERT).\n\n## Intended uses\nYou can use the CAMeLBERT-Mix POS-EGY model as part of the transformers pipeline.\nThis model will also be available in [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools) soon.\n\n#### How to use\nTo use the model with a transformers pipeline:\n```python\n>>> from transformers import pipeline\n>>> pos = pipeline(\'token-classification\', model=\'CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy\')\n>>> text = \'عامل ايه ؟\'\n>>> pos(text)\n[{\'entity\': \'adj\', \'score\': 0.9972628, \'index\': 1, \'word\': \'عامل\', \'start\': 0, \'end\': 4}, {\'entity\': \'pron_interrog\', \'score\': 0.9525163, \'index\': 2, \'word\': \'ايه\', \'start\': 5, \'end\': 8}, {\'entity\': \'punc\', \'score\': 0.99869114, \'index\': 3, \'word\': \'؟\', \'start\': 9, \'end\': 10}]\n```\n*Note*: to download our models, you would need `transformers>=3.5.0`.\nOtherwise, you could download the models manually.\n## Citation\n```bibtex\n@inproceedings{inoue-etal-2021-interplay,\n    title = ""The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models"",\n    author = ""Inoue, Go  and\n      Alhafni, Bashar  and\n      Baimukan, Nurpeiis  and\n      Bouamor, Houda  and\n      Habash, Nizar"",\n    booktitle = ""Proceedings of the Sixth Arabic Natural Language Processing Workshop"",\n    month = apr,\n    year = ""2021"",\n    address = ""Kyiv, Ukraine (Online)"",\n    publisher = ""Association for Computational Linguistics"",\n    abstract = ""In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks."",\n}\n```', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['transformers', 'pytorch'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '1.742 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': ['2103.06678'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.5144913196563721}]					[{'data': '2022-03-02 23:29:04+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2021-10-18 10:15:57+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:04+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'scaled-down set of the MSA variant', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0009104831260628998}]				[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'CAMeL-Lab', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'fourth language model', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.3221083463577088e-05}]		"[""{'data': 'https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-egy', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
5	[{'data': ['cc-by-sa-4.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'WordPiece subword tokenization', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.00027308857534080744}]	[{'data': 'BERT base model', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.46355438232421875}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'to extract plain texts from a dump file of Wikipedia articles', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.13916544616222382}]	[{'data': ['fill mask'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': '12 layers, 768 dimensions of hidden states, and 12 attention heads', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 9.80529875960201e-05}]	[{'data': 'tohoku-nlp', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['wikipedia'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['wikipedia'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'input texts with word-level tokenization based on the IPA dictionary', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.003694289131090045}]	[{'data': ['wikipedia'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/tohoku-nlp/bert-base-japanese', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '1.433 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': '1M training steps', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 8.41967630549334e-06}]	[{'data': '---\nlanguage: ja\nlicense: cc-by-sa-4.0\ndatasets:\n- wikipedia\nwidget:\n- text: 東北大学で[MASK]の研究をしています。\n---\n\n# BERT base Japanese (IPA dictionary)\n\nThis is a [BERT](https://github.com/google-research/bert) model pretrained on texts in the Japanese language.\n\nThis version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.\n\nThe codes for the pretraining are available at [cl-tohoku/bert-japanese](https://github.com/cl-tohoku/bert-japanese/tree/v1.0).\n\n## Model architecture\n\nThe model architecture is the same as the original BERT base model; 12 layers, 768 dimensions of hidden states, and 12 attention heads.\n\n## Training Data\n\nThe model is trained on Japanese Wikipedia as of September 1, 2019.\nTo generate the training corpus, [WikiExtractor](https://github.com/attardi/wikiextractor) is used to extract plain texts from a dump file of Wikipedia articles.\nThe text files used for the training are 2.6GB in size, consisting of approximately 17M sentences.\n\n## Tokenization\n\nThe texts are first tokenized by [MeCab](https://taku910.github.io/mecab/) morphological parser with the IPA dictionary and then split into subwords by the WordPiece algorithm.\nThe vocabulary size is 32000.\n\n## Training\n\nThe model is trained with the same configuration as the original BERT; 512 tokens per instance, 256 instances per batch, and 1M training steps.\n\n## Licenses\n\nThe pretrained models are distributed under the terms of the [Creative Commons Attribution-ShareAlike 3.0](https://creativecommons.org/licenses/by-sa/3.0/).\n\n## Acknowledgments\n\nFor training models, we used Cloud TPUs provided by [TensorFlow Research Cloud](https://www.tensorflow.org/tfrc/) program.\n', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		"[[{'data': ['transformers', 'pytorch', 'jax'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '1.433 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/tohoku-nlp/bert-base-japanese/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/tohoku-nlp/bert-base-japanese/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'BERT base model', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.145879878095002e-06}]					[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2024-02-22 00:57:00+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/tohoku-nlp/bert-base-japanese/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'To generate the training corpus', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 8.522978873770626e-07}]				[{'data': ['cc-by-sa-4.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'tohoku-nlp', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'BERT base model', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 3.874577942042379e-06}]		"[""{'data': 'https://huggingface.co/tohoku-nlp/bert-base-japanese', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'tohoku-nlp/bert-base-japanese', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/tohoku-nlp/bert-base-japanese', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
6	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'booktitle', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.00030752684688195586}]	[{'data': 'RoBERTa-base', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0323258601129055}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'System Demonstrations', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0207979753613472}]	[{'data': ['text classification'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		"[{'data': 'System Demonstrations"",\n    month = may,\n    year = ""2022"",', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.6719493942218833e-05}]"	[{'data': 'cardiffnlp', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['tweet_eval'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['tweet_eval'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'System Demonstrations', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0008730706758797169}]	[{'data': ['tweet_eval'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '1.001 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': '251--260', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.01800396665930748}]	"[{'data': '---\nlanguage: en\nwidget:\n- text: Covid cases are increasing fast!\ndatasets:\n- tweet_eval\n---\n\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). \n- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(""sentiment-analysis"", model=model_path, tokenizer=model_path)\nsentiment_task(""Covid cases are increasing fast!"")\n```\n```\n[{\'label\': \'Negative\', \'score\': 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split("" ""):\n        t = \'@user\' if t.startswith(\'@\') and len(t) > 1 else t\n        t = \'http\' if t.startswith(\'http\') else t\n        new_text.append(t)\n    return "" "".join(new_text)\nMODEL = f""cardiffnlp/twitter-roberta-base-sentiment-latest""\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = ""Covid cases are increasing fast!""\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors=\'pt\')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = ""Covid cases are increasing fast!""\n# encoded_input = tokenizer(text, return_tensors=\'tf\')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f""{i+1}) {l} {np.round(float(s), 4)}"")\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = ""{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media"",\n    author = ""Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\\'\\i}nez C{\\\'a}mara, Eugenio"" and others,\n    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"",\n    month = dec,\n    year = ""2022"",\n    address = ""Abu Dhabi, UAE"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://aclanthology.org/2022.emnlp-demos.5"",\n    pages = ""38--49""\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = ""{T}ime{LM}s: Diachronic Language Models from {T}witter"",\n    author = ""Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose"",\n    booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"",\n    month = may,\n    year = ""2022"",\n    address = ""Dublin, Ireland"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://aclanthology.org/2022.acl-demo.25"",\n    doi = ""10.18653/v1/2022.acl-demo.25"",\n    pages = ""251--260""\n}\n\n```\n', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['transformers', 'pytorch'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '1.001 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': ['2202.03829'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Camacho-collados, Jose  and\n      Rezaee', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.2177228331565857}]					[{'data': '2022-03-15 01:21:58+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2023-05-28 05:45:10+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-15 01:21:58+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[{'data': 'publisher = ""Association for Computational Linguistics""', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 5.5892993344741626e-08}]"				[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'cardiffnlp', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 2.2443991838372312e-05}]		"[""{'data': 'https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
7	[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'trivia_qa', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.17376837134361267}]	[{'data': 'Sentence-Transformers', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.001539447926916182}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'clustering or semantic search', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.4735915958881378}]	[{'data': ['feature extraction', 'sentence similarity'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'clustering or semantic search', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.00013896485324949026}]	[{'data': 'sentence-transformers', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['s2orc', 'flax-sentence-embeddings/stackexchange_xml', 'ms_marco', 'gooaq', 'yahoo_answers_topics', 'code_search_net', 'search_qa', 'eli5', 'snli', 'multi_nli', 'wikihow', 'natural_questions', 'trivia_qa', 'embedding-data/sentence-compression', 'embedding-data/flickr30k-captions', 'embedding-data/altlex', 'embedding-data/simple-wiki', 'embedding-data/QQP', 'embedding-data/SPECTER', 'embedding-data/PAQ_pairs', 'embedding-data/WikiAnswers'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['s2orc', 'flax-sentence-embeddings/stackexchange_xml', 'ms_marco', 'gooaq', 'yahoo_answers_topics', 'code_search_net', 'search_qa', 'eli5', 'snli', 'multi_nli', 'wikihow', 'natural_questions', 'trivia_qa', 'embedding-data/sentence-compression', 'embedding-data/flickr30k-captions', 'embedding-data/altlex', 'embedding-data/simple-wiki', 'embedding-data/QQP', 'embedding-data/SPECTER', 'embedding-data/PAQ_pairs', 'embedding-data/WikiAnswers'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'clustering or semantic search', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.3852038085460663}]	[{'data': ['s2orc', 'flax-sentence-embeddings/stackexchange_xml', 'ms_marco', 'gooaq', 'yahoo_answers_topics', 'code_search_net', 'search_qa', 'eli5', 'snli', 'multi_nli', 'wikihow', 'natural_questions', 'trivia_qa', 'embedding-data/sentence-compression', 'embedding-data/flickr30k-captions', 'embedding-data/altlex', 'embedding-data/simple-wiki', 'embedding-data/QQP', 'embedding-data/SPECTER', 'embedding-data/PAQ_pairs', 'embedding-data/WikiAnswers'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '0.455 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'It maps sentences & paragraphs to a 384 dimensional dense vector space', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 8.102649735519662e-05}]	"[{'data': '---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [""This is an example sentence"", ""Each sentence is converted""]\n\nmodel = SentenceTransformer(\'sentence-transformers/all-MiniLM-L6-v2\')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [\'This is an example sentence\', \'Each sentence is converted\']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\'sentence-transformers/all-MiniLM-L6-v2\')\nmodel = AutoModel.from_pretrained(\'sentence-transformers/all-MiniLM-L6-v2\')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\'pt\')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input[\'attention_mask\'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(""Sentence embeddings:"")\nprint(sentence_embeddings)\n```\n\n## Evaluation Results\n\nFor an automated evaluation of this model, see the *Sentence Embeddings Benchmark*: [https://seb.sbert.net](https://seb.sbert.net?model_name=sentence-transformers/all-MiniLM-L6-v2)\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['sentence-transformers', 'pytorch', 'rust', 'safetensors', 'transformers'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '0.455 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': ['1904.06472', '2102.07033', '2104.08727', '1704.05179', '1810.09305'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Googles Flax, JAX, and Cloud team member', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 2.8655460937443422e-06}]					[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2024-03-27 09:43:07+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'self-supervised \ncontrastive learning objective', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 3.9704224036540836e-05}]				[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'sentence-transformers', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2020', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 7.231521976791555e-07}]		"[""{'data': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'sentence-transformers/all-MiniLM-L6-v2', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
8	[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '5-8 annotators', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.046994369477033615}]	[{'data': 'RoBERTa-base model', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.07886279374361038}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'automatically according to the information the Trainer had access to', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 2.3749023966956884e-05}]	[{'data': ['text classification'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Accuracy', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 4.7942034143488854e-05}]	[{'data': 'mrm8488', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['financial_phrasebank'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': ['financial_phrasebank'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'automatically according to the information the Trainer had access to', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.0018882082076743245}]	[{'data': ['financial_phrasebank'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '0.660 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Framework versions\n\n- Transformers', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.02150900475680828}]	"[{'data': '---\nlicense: apache-2.0\nthumbnail: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\ntags:\n- generated_from_trainer\n- financial\n- stocks\n- sentiment\nwidget:\n- text: ""Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .""\ndatasets:\n- financial_phrasebank\nmetrics:\n- accuracy\nmodel-index:\n- name: distilRoberta-financial-sentiment\n  results:\n  - task:\n      name: Text Classification\n      type: text-classification\n    dataset:\n      name: financial_phrasebank\n      type: financial_phrasebank\n      args: sentences_allagree\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9823008849557522\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n<div style=""text-align:center;width:250px;height:250px;"">\n    <img src=""https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png"" alt=""logo"">\n</div>\n\n\n# DistilRoberta-financial-sentiment\n\n\nThis model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1116\n- Accuracy: **0.98**23\n\n## Base Model description\n\nThis model is a distilled version of the [RoBERTa-base model](https://huggingface.co/roberta-base). It follows the same training procedure as [DistilBERT](https://huggingface.co/distilbert-base-uncased).\nThe code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/distillation).\nThis model is case-sensitive: it makes a difference between English and English.\n\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).\nOn average DistilRoBERTa is twice as fast as Roberta-base.\n\n## Training Data\n\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| No log        | 1.0   | 255  | 0.1670          | 0.9646   |\n| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |\n| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |\n| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |\n| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['transformers', 'pytorch', 'tensorboard', 'safetensors'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '0.660 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Transformers 4.10.2\n- Pytorch', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.1827339676528936e-06}]					[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2024-01-21 15:17:58+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'Operating profit totaled EUR 9.4 mn', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.0697340258047916e-05}]				[{'data': ['apache-2.0'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'mrm8488', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'Transformers 4.10.2\n- Pytorch', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.004460491240024567}]		"[""{'data': 'https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
9	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'English Twitter tasks', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.00733409496024251}]	[{'data': 'RoBERTa', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.4569787085056305}]	{'data': 'Not extracted', 'extraction_method': 'None', 'confidence': 1.0}	[{'data': 'to pre-train', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.033195607364177704}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'for languages other than English, such models are not widely available', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 4.768163307744544e-06}]	[{'data': 'pysentimiento', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}, {'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'TASS 2020 corpus', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.963275826710742e-05}]	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/pysentimiento/robertuito-sentiment-analysis', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': '1.307 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'cross-lingual abilities', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.5883508240222e-05}]	"[{'data': '---\nlanguage: \n  - es\nlibrary_name: pysentimiento\n\ntags:\n  - twitter\n  - sentiment-analysis\n\n---\n# Sentiment Analysis in Spanish\n## robertuito-sentiment-analysis\n\nRepository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## Usage\n\nUse it directly with [pysentimiento](https://github.com/pysentimiento/pysentimiento)\n\n```python\nfrom pysentimiento import create_analyzer\nanalyzer = create_analyzer(task=""sentiment"", lang=""es"")\n\nanalyzer.predict(""Qué gran jugador es Messi"")\n# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n```\n\n\n## Results\n\nResults for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores\n\n\n| model         | emotion       | hate_speech   | irony         | sentiment     |\n|:--------------|:--------------|:--------------|:--------------|:--------------|\n| robertuito    | 0.560 ± 0.010 | 0.759 ± 0.007 | 0.739 ± 0.005 | 0.705 ± 0.003 |\n| roberta       | 0.527 ± 0.015 | 0.741 ± 0.012 | 0.721 ± 0.008 | 0.670 ± 0.006 |\n| bertin        | 0.524 ± 0.007 | 0.738 ± 0.007 | 0.713 ± 0.012 | 0.666 ± 0.005 |\n| beto_uncased  | 0.532 ± 0.012 | 0.727 ± 0.016 | 0.701 ± 0.007 | 0.651 ± 0.006 |\n| beto_cased    | 0.516 ± 0.012 | 0.724 ± 0.012 | 0.705 ± 0.009 | 0.662 ± 0.005 |\n| mbert_uncased | 0.493 ± 0.010 | 0.718 ± 0.011 | 0.681 ± 0.010 | 0.617 ± 0.003 |\n| biGRU         | 0.264 ± 0.007 | 0.592 ± 0.018 | 0.631 ± 0.011 | 0.585 ± 0.011 |\n\n\nNote that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B\n\n## Citation\n\nIf you use this model in your research, please cite pysentimiento and RoBERTuito papers:\n\n```latex\n\n@article{perez2021pysentimiento,\n  title={pysentimiento: a python toolkit for opinion mining and social NLP tasks},\n  author={P{\\\'e}rez, Juan Manuel and Rajngewerc, Mariela and Giudici, Juan Carlos and Furman, Dami{\\\'a}n A and Luque, Franco and Alemany, Laura Alonso and Mart{\\\'\\i}nez, Mar{\\\'\\i}a Vanina},\n  journal={arXiv preprint arXiv:2106.09462},\n  year={2021}\n}\n\n@inproceedings{perez-etal-2022-robertuito,\n    title = ""{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish"",\n    author = ""P{\\\'e}rez, Juan Manuel  and\n      Furman, Dami{\\\'a}n Ariel  and\n      Alonso Alemany, Laura  and\n      Luque, Franco M."",\n    booktitle = ""Proceedings of the Thirteenth Language Resources and Evaluation Conference"",\n    month = jun,\n    year = ""2022"",\n    address = ""Marseille, France"",\n    publisher = ""European Language Resources Association"",\n    url = ""https://aclanthology.org/2022.lrec-1.785"",\n    pages = ""7235--7243"",\n    abstract = ""Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it."",\n}\n\n@inproceedings{garcia2020overview,\n  title={Overview of TASS 2020: Introducing emotion detection},\n  author={Garc{\\\'\\i}a-Vega, Manuel and D{\\\'\\i}az-Galiano, MC and Garc{\\\'\\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\\\'a}ez, A and Jim{\\\'e}nez-Zafra, SM and Mart{\\\'\\i}nez C{\\\'a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},\n  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\\\'a}laga, Spain},\n  pages={163--170},\n  year={2020}\n}\n```', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]"		"[[{'data': ['pytorch', 'safetensors'], 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}], ""{'data': 'Python', 'extraction_method': 'Added in transform stage', 'confidence': 1.0}""]"	[{'data': '1.307 Gbytes', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]			"[""{'data': 'https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	"[""{'data': 'https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/blob/main/README.md', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]		[{'data': 'Juan Carlos and Furman', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.07175741344690323}]					[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2024-04-07 22:46:56+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2022-03-02 23:29:05+00:00', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/discussions', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'state-of-the-art for natural language', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 1.6665119062508893e-08}]				[{'data': None, 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': 'pysentimiento', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	[{'data': '2020', 'extraction_method': 'Intel/dynamic_tinybert', 'confidence': 0.002996206283569336}]		"[""{'data': 'https://huggingface.co/pysentimiento/robertuito-sentiment-analysis', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"	[{'data': 'pysentimiento/robertuito-sentiment-analysis', 'extraction_method': 'Parsed_from_HF_dataset', 'confidence': 1.0}]	"[""{'data': 'https://huggingface.co/pysentimiento/robertuito-sentiment-analysis', 'extraction_method': 'Built in transform stage', 'confidence': 1.0}""]"
